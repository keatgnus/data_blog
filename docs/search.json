[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nData_Mining CH3\n\n\n\n\n\n\n\ncode\n\n\ndata_mining\n\n\njupyter\n\n\n\n\n\n\n\n\n\n\n\nApr 27, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nSpatial_Info_Analysis CH2\n\n\n\n\n\n\n\ncode\n\n\nSpatial_Info_Analysis\n\n\nrstudio\n\n\n\n\n\n\n\n\n\n\n\nApr 22, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nSpatial_Info_Analysis CH1\n\n\n\n\n\n\n\ncode\n\n\nSpatial_Info_Analysis\n\n\nrstudio\n\n\n\n\n\n\n\n\n\n\n\nApr 20, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nOpendata_Analysis CH3\n\n\n\n\n\n\n\ncode\n\n\nopendata_analysis\n\n\nrstudio\n\n\n\n\n\n\n\n\n\n\n\nApr 16, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nOpendata_Analysis CH4\n\n\n\n\n\n\n\ncode\n\n\nopendata_analysis\n\n\nrstudio\n\n\n\n\n\n\n\n\n\n\n\nApr 16, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nOpendata_Analysis CH5\n\n\n\n\n\n\n\ncode\n\n\nopendata_analysis\n\n\nrstudio\n\n\n\n\n\n\n\n\n\n\n\nApr 16, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nOpendata_Analysis CH2\n\n\n\n\n\n\n\ncode\n\n\nopendata_analysis\n\n\nrstudio\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nOpendata_Analysis CH1\n\n\n\n\n\n\n\ncode\n\n\nopendata_analysis\n\n\nrstudio\n\n\n\n\n\n\n\n\n\n\n\nApr 14, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nData_Visualization CH7\n\n\n\n\n\n\n\ncode\n\n\ndata_visualization\n\n\nrstudio\n\n\n\n\n\n\n\n\n\n\n\nApr 13, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nData_Visualization CH6\n\n\n\n\n\n\n\ncode\n\n\ndata_visualization\n\n\nrstudio\n\n\n\n\n\n\n\n\n\n\n\nApr 12, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nData_Visualization CH5\n\n\n\n\n\n\n\ncode\n\n\ndata_visualization\n\n\nrstudio\n\n\n\n\n\n\n\n\n\n\n\nApr 6, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nData_Visualization CH4\n\n\n\n\n\n\n\ncode\n\n\ndata_visualization\n\n\nrstudio\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nData_Mining CH1\n\n\n\n\n\n\n\ncode\n\n\ndata_mining\n\n\njupyter\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nData_Mining CH2\n\n\n\n\n\n\n\ncode\n\n\ndata_mining\n\n\njupyter\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nData_Visualization CH3\n\n\n\n\n\n\n\ncode\n\n\ndata_visualization\n\n\nrstudio\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nData_Visualization CH2\n\n\n\n\n\n\n\ncode\n\n\ndata_visualization\n\n\nrstudio\n\n\n\n\n\n\n\n\n\n\n\nMar 30, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nData_Visualization CH1\n\n\n\n\n\n\n\ncode\n\n\ndata_visualization\n\n\nrstudio\n\n\n\n\n\n\n\n\n\n\n\nMar 29, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nadvanced_python CH1\n\n\n\n\n\n\n\ncode\n\n\nadvanced_python\n\n\njupyter\n\n\n\n\n\n\n\n\n\n\n\nMar 28, 2023\n\n\nSeongtaek\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Advanced_Python_ch2/파이썬 심화 챕터2 공부.html",
    "href": "posts/Advanced_Python_ch2/파이썬 심화 챕터2 공부.html",
    "title": "advanced_python CH1",
    "section": "",
    "text": "Jupyter에서 실행하기\nBasic Grammar"
  },
  {
    "objectID": "posts/Advanced_Python_ch2/파이썬 심화 챕터2 공부.html#python-ch1.-기본-문법",
    "href": "posts/Advanced_Python_ch2/파이썬 심화 챕터2 공부.html#python-ch1.-기본-문법",
    "title": "advanced_python CH1",
    "section": "1 python CH1. 기본 문법",
    "text": "1 python CH1. 기본 문법\n\n1.1 수치 할당\nimport numpy as np\na = 5; b = 6; c = 7\nc\n7\n\n\n1.2 변수, 논리연산자\n# append() : 값 추가\na = [1,2,3]\nb=a\na.append(4)\nb\n[1, 2, 3, 4]\n\n\n1.3 타입 확인\na = 5; b = 'foo'\ntype(a), type(b)\n(int, str)\n# format() : print{} 안에 내용 추가 (순서대로 0,1,2..)\n# float : 소수\na = 4.5; b = 2\nprint('a is {0}, b is {1}'.format(type(a), type(b)))\na/b\na is <class 'float'>, b is <class 'int'>\n2.25\n# isinstance() : 타입 맞는지 확인\na = 5\nisinstance(a, int)\nTrue\n# 여러 타입 중 하나라도 맞으면 True\na = 5; b = 4.5\nisinstance(a, (int, float, str)), isinstance(b, (int, float))\n(True, True)\n\n\n1.4 속성, 메서드\na = 'foo'\n# upper() : 소문자 → 대문자\na.upper()\n'FOO'\n# getattr(객체, 속성) : 객체의 속성에 접근해줌\ngetattr(a, 'split')\n<function str.split(sep=None, maxsplit=-1)>\n\n\n1.5 Duck typing\n# 덕 타이핑: \"특정 기능을 지원하는가만 중요하다\"는 의미를 전달할 때 사용하는 표현\n# = 이터러블 객체\ndef isiterable(obj):\n    try:\n        iter(obj)\n        return True\n    except TypeError: # 이터러블 하지 않은 값\n        return False\nisiterable('a string')\nTrue\nisiterable([1,2,3])\nTrue\n# 정수는 이터러블 하지 않음\nisiterable(5) \nFalse\n\n\n1.6 Import\n\n1.6.0.0.1 some_module.py\n\n\n1.6.0.0.2 Pi = 3.14159\n\n\n1.6.0.0.3 def f(x): return x + 2\n\n\n1.6.0.0.4 def g(a,b):return a+b\n# some_module : 임의적으로 만든 py 파일\nimport some_module\nresult = some_module.f(5)\nresult\n7\npi = some_module.PI\npi\n3.14159\n# 모듈 중 몇개만 import\nfrom some_module import f,g,PI\nresult = g(5,PI)\nresult\n8.14159\n# as 로 이름 간략하게 바꿔서도 사용 가능\nimport some_module as sm\nfrom some_module import PI as pi, g as gf\nr1 = sm.f(pi)\nr2 = gf(6, pi)\nr1, r2\n(5.14159, 9.14159)\n\n\n\n1.7 이항 연산, 비교\n5 - 7, 12 + 21.5, 5 <= 2\n(-2, 33.5, False)\na = [1,2,3]\nb = a\nc = list(a)\na is b, a is not c\n(True, True)\na == c\nTrue\na = None\na is None\nTrue\na_list = ['foo', 2, [4,5]]\na_list[2] = (3,4)\na_list\n['foo', 2, (3, 4)]\n# 잘못된 예시 (수치형을 범주형으로 바꾸려한 경우)\na_tuple = (3, 5, (4,5))\na_tuple[1] = 'four'\n---------------------------------------------------------------------------\n\nTypeError                                 Traceback (most recent call last)\n\nCell In[28], line 3\n      1 # 잘못된 예시 (수치형을 범주형으로 바꾸려한 경우)\n      2 a_tuple = (3, 5, (4,5))\n----> 3 a_tuple[1] = 'four'\n\n\nTypeError: 'tuple' object does not support item assignment\nival = 17239871\nival ** 6\n26254519291092456596965462913230729701102721\nfval = 7.243\nfval2 = 6.78e-5\n# // : 몫 (% : 나머지)\n3/2, type(3/2), 3//2, type(3//2)\n(1.5, float, 1, int)\na = 'one way of writing a string'\nb = 'another way'\na,b\n('one way of writing a string', 'another way')\n# 따옴표 3개 : 여러 줄을 한 줄로 출력\nc = '''\nThis is a longer string that\nspans multiple lines\n'''\nc\n'\\nThis is a longer string that\\nspans multiple lines\\n'\n# 띄어진 줄 개수\nc.count('\\n')\n3\n# 잘못된 예시 (문자 바꾸려한 경우)\na = 'this is a string'\na[10] = 'f'\n---------------------------------------------------------------------------\n\nTypeError                                 Traceback (most recent call last)\n\nCell In[35], line 3\n      1 # 잘못된 예시 (문자 바꾸려한 경우)\n      2 a = 'this is a string'\n----> 3 a[10] = 'f'\n\n\nTypeError: 'str' object does not support item assignment\n# replace 이용해 변경\nb = a.replace('string', 'longer string')\na,b\n('this is a string', 'this is a longer string')\n# 수치형 → 범주형 변경\na = 5.6\ns = str(a)\nprint(s),\ntype(s)\n5.6\nstr\n# list 이용해 문자 하나씩 출력\ns = 'python'\nlist(s)\n['p', 'y', 't', 'h', 'o', 'n']\n# 3전 까지 출력, 3부터 출력\ns[:3], s[3:]\n('pyt', 'hon')\n# \\n로 줄 띄우기\nprint('12\\n34')\n12\n34\n# 이상한 예시\ns = '12\\\\34'\nprint(s)\n12\\34\n# 범주형도 더해진다\na = 'this is the first half'\nb = 'and this is the second half'\na + b\n'this is the first halfand this is the second half'\n# {0:.2f} : 소수점 둘째 자리로 반올림\n# {1:s} : 두번째 인수의 형식을 범주형으로 저장\n# {2:d} : 세번째 인수의 형식을 정확한 정수로 저장\ntemplate = '{0:.2f} {1:s} are worth US${2:d}'\ntemplate\n'{0:.2f} {1:s} are worth US${2:d}'\ntemplate.format(4.5560, 'Argentine Pesos', 1)\n'4.56 Argentine Pesos are worth US$1'\ntemplate.format(1263.23, 'won', 1)\n'1263.23 won are worth US$1'\n\n\n1.8 Booleans\n\n1.8.0.0.1 불리언 타입 - True, False 두가지 값\nTrue and True\nTrue\nFalse or True\nTrue\n\n\n\n1.9 Type casting\n\n1.9.0.0.1 원하는 타입으로 해석\ns = '3.14159'\nfval = float(s)\ntype(fval)\nfloat\nint(fval)\n3\nbool(fval)\nTrue\n# bool : 비어있는 값, 0 → False\nbool(0)\nFalse\na = None\na is None\nTrue\ndef add_and_maybe_multiply(a, b, c=None):\n    result = a + b\n    if c is not None:\n        result = result * c\n    return result\nadd_and_maybe_multiply(5,3)\n8\nadd_and_maybe_multiply(5,3,10)\n80\ntype(None)\nNoneType\n\n\n\n1.10 날짜, 시간 값\nfrom datetime import datetime, date, time\ndt = datetime(2011, 10, 29, 20, 30, 21)\ndt\n# 년, 월, 일, 시, 분, 초\ndatetime.datetime(2011, 10, 29, 20, 30, 21)\ndt.day\n29\ndt.minute\n30\n# 년, 월, 일\ndt.date()\ndatetime.date(2011, 10, 29)\n# 시, 분, 초\ndt.time()\ndatetime.time(20, 30, 21)\n# strftime : 날짜 문자열을 날짜 객체로 만들 때\ndt.strftime('%m/%d/%Y %H:%M'), dt.strftime('%Y/%m/%d %H:%M')\n('10/29/2011 20:30', '2011/10/29 20:30')\n# strptime : 날짜 객체열을 날짜 문자열로 출력 할 때 \ndatetime.strptime('20091031', '%Y%m%d')\ndatetime.datetime(2009, 10, 31, 0, 0)\ndt.replace(minute=0, second=0)\ndatetime.datetime(2011, 10, 29, 20, 0)\n# 날짜 끼리 연산 가능\ndt2 = datetime(2011, 11, 15, 22, 30)\ndelta = dt2 - dt\ndelta\ndatetime.timedelta(days=17, seconds=7179)\ntype(delta)\ndatetime.timedelta\ndt\ndt + delta\ndatetime.datetime(2011, 11, 15, 22, 30)\n\n\n1.11 Control Flow\nx = -5\n\nif x < 0:\n    print('It is negative')\nIt is negative\nx = 8\nif x < 0:\n    print('It is negative')\nelif x == 0:\n    print('Equal to zero')\nelif 0 < x < 5:\n    print('positive but smaller than 5')\nelse:\n    print('positive and larger than or equal to 5')\npositive and larger than or equal to 5\na = 5; b = 7; c = 8; d = 4\nif a < b or c > d:\n    print('Made it')\nMade it\n4>3>2>1, 3>5 or 2>1, 3>5>2>1\n(True, True, False)\n\n\n1.12 for loops 루프문\n# 잘못된 예시 (+= 는 int, NoneType 지원 안함)\nsequence = [1,2,None,4,None,5]\ntotal = 0\n\nfor value in sequence:\n    total += value\n    # total = total + value\n---------------------------------------------------------------------------\n\nTypeError                                 Traceback (most recent call last)\n\nCell In[72], line 6\n      3 total = 0\n      5 for value in sequence:\n----> 6     total += value\n\n\nTypeError: unsupported operand type(s) for +=: 'int' and 'NoneType'\nsequence = [1,2,None,4,None,5]\ntotal = 10\n\nfor value in sequence:\n    if value is None:\n        continue\n    total += value\ntotal\n22\nsequence = [1,2,0,4,6,5,2,1]\ntotal_until_5 = 0\nfor value in sequence:\n    if value == 5:\n        break\n    total_until_5 += value\ntotal_until_5\n13\nlist(range(4))\n[0, 1, 2, 3]\n# j가 i 보다 커지기 전까지 조합\nfor i in range(4):\n    for j in range(4):\n        if j > i:\n            break\n        print((j,i))\n(0, 0)\n(0, 1)\n(1, 1)\n(0, 2)\n(1, 2)\n(2, 2)\n(0, 3)\n(1, 3)\n(2, 3)\n(3, 3)\n# 전체 조합 출력\nfor i in range(4):\n    for j in range(4):\n        print((i,j))\n(0, 0)\n(0, 1)\n(0, 2)\n(0, 3)\n(1, 0)\n(1, 1)\n(1, 2)\n(1, 3)\n(2, 0)\n(2, 1)\n(2, 2)\n(2, 3)\n(3, 0)\n(3, 1)\n(3, 2)\n(3, 3)\nfor a, b, c in [[1,2,3],[4,5,6],[7,8,9]]:\n    print(a,b,c)\n1 2 3\n4 5 6\n7 8 9\nfor a in [[1,2,3],[4,5,6],[7,8,9]]:\n    print(a)\n[1, 2, 3]\n[4, 5, 6]\n[7, 8, 9]\n\n\n1.13 while loops\nx = 256\ntotal = 0\n\nwhile x > 0:\n    if total >500:\n        break\n    total += x\n    x = x // 2\n    print(total,x)\ntotal, x\n256 128\n384 64\n448 32\n480 16\n496 8\n504 4\n\n\n\n\n\n(504, 4)\ntotal,x\n(504, 4)\n\n\n1.14 pass\nx = 7\n\nif x < 0:\n    print('negative!')\nelif x == 0:\n    pass\nelse:\n    print('positive!')\npositive!\n\n\n1.15 range\nrange(10)\nrange(0, 10)\nlist(range(10))\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nlist(range(0, 20, 2))\n[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\nlist(range(5, 0, -1))\n[5, 4, 3, 2, 1]\nsum = 0\nfor i in range(100000):\n    if i % 3 == 0 or i % 5 == 0:\n        sum += i\n\nsum\n2333316668\nx = 5\nb = 'Non-negative' if x >= 0 else 'Negative'\nb\n'Non-negative'\nx = 5\n\na = 100 if x>=0 else -100\na\n100"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html",
    "title": "Data_Mining CH1",
    "section": "",
    "text": "Numpy Basic"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#numpy-기본",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#numpy-기본",
    "title": "Data_Mining CH1",
    "section": "0.1 Numpy 기본",
    "text": "0.1 Numpy 기본\n도구 - 넘파이(NumPy)\n\n넘파이(NumPy)는 파이썬의 과학 컴퓨팅을 위한 기본 라이브러리입니다.\n넘파이의 핵심은 강력한 N-차원 배열 객체입니다.\n선형 대수, 푸리에(Fourier) 변환, 유사 난수 생성과 같은 유용한 함수들도 제공합니다.\n\n\n\n\nJupyter에서 실행하기\n\n\n\n\n\n구글 코랩에서 실행하기"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#배열-생성",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#배열-생성",
    "title": "Data_Mining CH1",
    "section": "0.2 배열 생성",
    "text": "0.2 배열 생성\nnumpy를 임포트해 보죠. 대부분의 사람들이 np로 알리아싱하여 임포트합니다:\nimport numpy as np\n\n0.2.1 np.zeros\nzeros 함수는 0으로 채워진 배열을 만듭니다:\nnp.zeros(5)\narray([0., 0., 0., 0., 0.])\n2D 배열(즉, 행렬)을 만들려면 원하는 행과 열의 크기를 튜플로 전달합니다. 예를 들어 다음은 \\(3 \\times 4\\) 크기의 행렬입니다:\nnp.zeros((3,4))\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#np.zeros",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#np.zeros",
    "title": "Data_Mining CH1",
    "section": "3 np.zeros",
    "text": "3 np.zeros\nzeros 함수는 0으로 채워진 배열을 만듭니다:\nnp.zeros(5)\narray([0., 0., 0., 0., 0.])\n2D 배열(즉, 행렬)을 만들려면 원하는 행과 열의 크기를 튜플로 전달합니다. 예를 들어 다음은 \\(3 \\times 4\\) 크기의 행렬입니다:\nnp.zeros((3,4))\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#용어",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#용어",
    "title": "Data_Mining CH1",
    "section": "0.3 용어",
    "text": "0.3 용어\n\n넘파이에서 각 차원을 축(axis) 이라고 합니다\n축의 개수를 랭크(rank) 라고 합니다.\n\n예를 들어, 위의 \\(3 \\times 4\\) 행렬은 랭크 2인 배열입니다(즉 2차원입니다).\n첫 번째 축의 길이는 3이고 두 번째 축의 길이는 4입니다.\n\n배열의 축 길이를 배열의 크기(shape)라고 합니다.\n\n예를 들어, 위 행렬의 크기는 (3, 4)입니다.\n랭크는 크기의 길이와 같습니다.\n\n배열의 사이즈(size)는 전체 원소의 개수입니다. 축의 길이를 모두 곱해서 구할 수 있습니다(가령, \\(3 \\times 4=12\\)).\n\na = np.zeros((3,4))\na\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])\na.shape\n(3, 4)\na.ndim  # len(a.shape)와 같습니다\n2\na.size\n12"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#n-차원-배열",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#n-차원-배열",
    "title": "Data_Mining CH1",
    "section": "0.4 N-차원 배열",
    "text": "0.4 N-차원 배열\n임의의 랭크 수를 가진 N-차원 배열을 만들 수 있습니다. 예를 들어, 다음은 크기가 (2,3,4)인 3D 배열(랭크=3)입니다:\nnp.zeros((2,3,4))\narray([[[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]],\n\n       [[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#배열-타입",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#배열-타입",
    "title": "Data_Mining CH1",
    "section": "0.5 배열 타입",
    "text": "0.5 배열 타입\n넘파이 배열의 타입은 ndarray입니다:\ntype(np.zeros((3,4)))\nnumpy.ndarray\n\n0.5.1 np.ones\nndarray를 만들 수 있는 넘파이 함수가 많습니다.\n다음은 1로 채워진 \\(3 \\times 4\\) 크기의 행렬입니다:\nnp.ones((3,4))\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.]])\n\n\n0.5.2 np.full\n주어진 값으로 지정된 크기의 배열을 초기화합니다. 다음은 π로 채워진 \\(3 \\times 4\\) 크기의 행렬입니다.\nnp.full((3,4), np.pi)\narray([[3.14159265, 3.14159265, 3.14159265, 3.14159265],\n       [3.14159265, 3.14159265, 3.14159265, 3.14159265],\n       [3.14159265, 3.14159265, 3.14159265, 3.14159265]])\n\n\n0.5.3 np.empty\n초기화되지 않은 \\(2 \\times 3\\) 크기의 배열을 만듭니다(배열의 내용은 예측이 불가능하며 메모리 상황에 따라 달라집니다):\nnp.empty((2,3))\narray([[0., 0., 0.],\n       [0., 0., 0.]])\n\n\n0.5.4 np.array\narray 함수는 파이썬 리스트를 사용하여 ndarray를 초기화합니다:\nnp.array([[1,2,3,4], [10, 20, 30, 40],[3,4,5,6]])\narray([[ 1,  2,  3,  4],\n       [10, 20, 30, 40],\n       [ 3,  4,  5,  6]])\n\n\n0.5.5 np.arange\n파이썬의 기본 range 함수와 비슷한 넘파이 arange 함수를 사용하여 ndarray를 만들 수 있습니다:\nnp.arange(1, 5)\narray([1, 2, 3, 4])\n부동 소수도 가능합니다:\nnp.arange(1.0, 5.0)\narray([1., 2., 3., 4.])\n파이썬의 기본 range 함수처럼 건너 뛰는 정도를 지정할 수 있습니다:\nnp.arange(1, 5, 0.5)\narray([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5])\n부동 소수를 사용하면 원소의 개수가 일정하지 않을 수 있습니다. 예를 들면 다음과 같습니다:\nprint(np.arange(0, 5/3, 1/3)) # 부동 소수 오차 때문에, 최댓값은 4/3 또는 5/3이 됩니다.\nprint(np.arange(0, 5/3, 0.333333333))\nprint(np.arange(0, 5/3, 0.333333334))\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]\n[0.         0.33333333 0.66666667 1.         1.33333334]\n\n\n0.5.6 np.linspace\n이런 이유로 부동 소수를 사용할 땐 arange 대신에 linspace 함수를 사용하는 것이 좋습니다. linspace 함수는 지정된 개수만큼 두 값 사이를 나눈 배열을 반환합니다(arange와는 다르게 최댓값이 포함됩니다):\nprint(np.linspace(0, 5/3, 6)) #n=6\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]\n\n\n0.5.7 np.rand & np.randn\n넘파이의 random 모듈에는 ndarray를 랜덤한 값으로 초기화할 수 있는 함수들이 많이 있습니다. 예를 들어, 다음은 (균등 분포인) 0과 1사이의 랜덤한 부동 소수로 \\(3 \\times 4\\) 행렬을 초기화합니다:\nnp.random.rand(3,4)\narray([[0.97658453, 0.12023957, 0.1385255 , 0.27344451],\n       [0.7107592 , 0.51802762, 0.03772338, 0.76349622],\n       [0.22866009, 0.60059563, 0.20339496, 0.83995902]])\n다음은 평균이 0이고 분산이 1인 일변량 정규 분포(가우시안 분포)에서 샘플링한 랜덤한 부동 소수를 담은 \\(3 \\times 4\\) 행렬입니다:\nnp.random.randn(3,4)\narray([[ 0.44399064, -0.38009088,  1.49175547,  0.15028664],\n       [ 0.12956604, -1.54072513, -0.26739546,  0.15539485],\n       [-0.62462998,  0.53105371,  0.09237904, -1.127329  ]])\n이 분포의 모양을 알려면 맷플롯립을 사용해 그려보는 것이 좋습니다(더 자세한 것은 맷플롯립 튜토리얼을 참고하세요):\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.hist(np.random.rand(100000), density=True, bins=100, histtype=\"step\", color=\"blue\", label=\"rand\")\nplt.hist(np.random.randn(100000), density=True, bins=100, histtype=\"step\", color=\"red\", label=\"randn\")\nplt.axis([-2.5, 2.5, 0, 1.1])\nplt.legend(loc = \"upper left\")\nplt.title(\"Random distributions\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Density\")\nplt.show()\n\n\n\npng\n\n\n\n\n0.5.8 np.fromfunction\n함수를 사용하여 ndarray를 초기화할 수도 있습니다:\ndef my_function(z, y, x):\n    return x + 10 * y + 100 * z\n\nnp.fromfunction(my_function, (3, 2, 10))\narray([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.],\n        [ 10.,  11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.]],\n\n       [[100., 101., 102., 103., 104., 105., 106., 107., 108., 109.],\n        [110., 111., 112., 113., 114., 115., 116., 117., 118., 119.]],\n\n       [[200., 201., 202., 203., 204., 205., 206., 207., 208., 209.],\n        [210., 211., 212., 213., 214., 215., 216., 217., 218., 219.]]])\n넘파이는 먼저 크기가 (3, 2, 10)인 세 개의 ndarray(차원마다 하나씩)를 만듭니다. 각 배열은 축을 따라 좌표 값과 같은 값을 가집니다. 예를 들어, z 축에 있는 배열의 모든 원소는 z-축의 값과 같습니다:\n[[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n\n [[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]]\n\n [[ 2.  2.  2.  2.  2.  2.  2.  2.  2.  2.]\n  [ 2.  2.  2.  2.  2.  2.  2.  2.  2.  2.]]]\n위의 식 x + 10 * y + 100 * z에서 x, y, z는 사실 ndarray입니다(배열의 산술 연산에 대해서는 아래에서 설명합니다). 중요한 점은 함수 my_function이 원소마다 호출되는 것이 아니고 딱 한 번 호출된다는 점입니다. 그래서 매우 효율적으로 초기화할 수 있습니다."
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#np.ones",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#np.ones",
    "title": "Data_Mining CH1",
    "section": "7 np.ones",
    "text": "7 np.ones\nndarray를 만들 수 있는 넘파이 함수가 많습니다.\n다음은 1로 채워진 \\(3 \\times 4\\) 크기의 행렬입니다:\nnp.ones((3,4))\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#np.full",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#np.full",
    "title": "Data_Mining CH1",
    "section": "8 np.full",
    "text": "8 np.full\n주어진 값으로 지정된 크기의 배열을 초기화합니다. 다음은 π로 채워진 \\(3 \\times 4\\) 크기의 행렬입니다.\nnp.full((3,4), np.pi)\narray([[3.14159265, 3.14159265, 3.14159265, 3.14159265],\n       [3.14159265, 3.14159265, 3.14159265, 3.14159265],\n       [3.14159265, 3.14159265, 3.14159265, 3.14159265]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#np.empty",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#np.empty",
    "title": "Data_Mining CH1",
    "section": "9 np.empty",
    "text": "9 np.empty\n초기화되지 않은 \\(2 \\times 3\\) 크기의 배열을 만듭니다(배열의 내용은 예측이 불가능하며 메모리 상황에 따라 달라집니다):\nnp.empty((2,3))\narray([[0., 0., 0.],\n       [0., 0., 0.]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#np.array",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#np.array",
    "title": "Data_Mining CH1",
    "section": "10 np.array",
    "text": "10 np.array\narray 함수는 파이썬 리스트를 사용하여 ndarray를 초기화합니다:\nnp.array([[1,2,3,4], [10, 20, 30, 40]])\narray([[ 1,  2,  3,  4],\n       [10, 20, 30, 40]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#np.arange",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#np.arange",
    "title": "Data_Mining CH1",
    "section": "11 np.arange",
    "text": "11 np.arange\n파이썬의 기본 range 함수와 비슷한 넘파이 arange 함수를 사용하여 ndarray를 만들 수 있습니다:\nnp.arange(1, 5)\narray([1, 2, 3, 4])\n부동 소수도 가능합니다:\nnp.arange(1.0, 5.0)\narray([1., 2., 3., 4.])\n파이썬의 기본 range 함수처럼 건너 뛰는 정도를 지정할 수 있습니다:\nnp.arange(1, 5, 0.5)\narray([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5])\n부동 소수를 사용하면 원소의 개수가 일정하지 않을 수 있습니다. 예를 들면 다음과 같습니다:\nprint(np.arange(0, 5/3, 1/3)) # 부동 소수 오차 때문에, 최댓값은 4/3 또는 5/3이 됩니다.\nprint(np.arange(0, 5/3, 0.333333333))\nprint(np.arange(0, 5/3, 0.333333334))\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]\n[0.         0.33333333 0.66666667 1.         1.33333334]"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#np.linspace",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#np.linspace",
    "title": "Data_Mining CH1",
    "section": "12 np.linspace",
    "text": "12 np.linspace\n이런 이유로 부동 소수를 사용할 땐 arange 대신에 linspace 함수를 사용하는 것이 좋습니다. linspace 함수는 지정된 개수만큼 두 값 사이를 나눈 배열을 반환합니다(arange와는 다르게 최댓값이 포함됩니다):\nprint(np.linspace(0, 5/3, 6))\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#np.rand와-np.randn",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#np.rand와-np.randn",
    "title": "Data_Mining CH1",
    "section": "13 np.rand와 np.randn",
    "text": "13 np.rand와 np.randn\n넘파이의 random 모듈에는 ndarray를 랜덤한 값으로 초기화할 수 있는 함수들이 많이 있습니다. 예를 들어, 다음은 (균등 분포인) 0과 1사이의 랜덤한 부동 소수로 \\(3 \\times 4\\) 행렬을 초기화합니다:\nnp.random.rand(3,4)\narray([[0.06155426, 0.74605262, 0.15696099, 0.10848505],\n       [0.99477242, 0.50644411, 0.85697022, 0.7538202 ],\n       [0.69186472, 0.70790355, 0.1212439 , 0.40088931]])\n다음은 평균이 0이고 분산이 1인 일변량 정규 분포(가우시안 분포)에서 샘플링한 랜덤한 부동 소수를 담은 \\(3 \\times 4\\) 행렬입니다:\nnp.random.randn(3,4)\narray([[ 0.68549892, -0.35640678,  0.49284835, -1.11736284],\n       [ 1.50288866, -1.04041734,  1.1961761 ,  1.34060815],\n       [ 0.19706421,  0.13122654,  0.68539933, -0.13550458]])\n이 분포의 모양을 알려면 맷플롯립을 사용해 그려보는 것이 좋습니다(더 자세한 것은 맷플롯립 튜토리얼을 참고하세요):\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.hist(np.random.rand(100000), density=True, bins=100, histtype=\"step\", color=\"blue\", label=\"rand\")\nplt.hist(np.random.randn(100000), density=True, bins=100, histtype=\"step\", color=\"red\", label=\"randn\")\nplt.axis([-2.5, 2.5, 0, 1.1])\nplt.legend(loc = \"upper left\")\nplt.title(\"Random distributions\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Density\")\nplt.show()\n\n\n\npng"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#np.fromfunction",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#np.fromfunction",
    "title": "Data_Mining CH1",
    "section": "14 np.fromfunction",
    "text": "14 np.fromfunction\n함수를 사용하여 ndarray를 초기화할 수도 있습니다:\ndef my_function(z, y, x):\n    return x + 10 * y + 100 * z\n\nnp.fromfunction(my_function, (3, 2, 10))\narray([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.],\n        [ 10.,  11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.]],\n\n       [[100., 101., 102., 103., 104., 105., 106., 107., 108., 109.],\n        [110., 111., 112., 113., 114., 115., 116., 117., 118., 119.]],\n\n       [[200., 201., 202., 203., 204., 205., 206., 207., 208., 209.],\n        [210., 211., 212., 213., 214., 215., 216., 217., 218., 219.]]])\n넘파이는 먼저 크기가 (3, 2, 10)인 세 개의 ndarray(차원마다 하나씩)를 만듭니다. 각 배열은 축을 따라 좌표 값과 같은 값을 가집니다. 예를 들어, z 축에 있는 배열의 모든 원소는 z-축의 값과 같습니다:\n[[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n\n [[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]]\n\n [[ 2.  2.  2.  2.  2.  2.  2.  2.  2.  2.]\n  [ 2.  2.  2.  2.  2.  2.  2.  2.  2.  2.]]]\n위의 식 x + 10 * y + 100 * z에서 x, y, z는 사실 ndarray입니다(배열의 산술 연산에 대해서는 아래에서 설명합니다). 중요한 점은 함수 my_function이 원소마다 호출되는 것이 아니고 딱 한 번 호출된다는 점입니다. 그래서 매우 효율적으로 초기화할 수 있습니다."
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#배열-데이터",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#배열-데이터",
    "title": "Data_Mining CH1",
    "section": "0.6 배열 데이터",
    "text": "0.6 배열 데이터\n\n0.6.1 dtype\n넘파이의 ndarray는 모든 원소가 동일한 타입(보통 숫자)을 가지기 때문에 효율적입니다. dtype 속성으로 쉽게 데이터 타입을 확인할 수 있습니다:\nc = np.arange(1, 5)\nprint(c.dtype, c)\nint32 [1 2 3 4]\nc = np.arange(1.0, 5.0)\nprint(c.dtype, c)\nfloat64 [1. 2. 3. 4.]\n넘파이가 데이터 타입을 결정하도록 내버려 두는 대신 dtype 매개변수를 사용해서 배열을 만들 때 명시적으로 지정할 수 있습니다:\nd = np.arange(1, 5, dtype=np.complex64)\nprint(d.dtype, d)\ncomplex64 [1.+0.j 2.+0.j 3.+0.j 4.+0.j]\n가능한 데이터 타입은 int8, int16, int32, int64, uint8|16|32|64, float16|32|64, complex64|128가 있습니다. 전체 리스트는 온라인 문서를 참고하세요.\n\n\n0.6.2 itemsize\nitemsize 속성은 각 아이템의 크기(바이트)를 반환합니다:\ne = np.arange(1, 5, dtype=np.complex64)\nprint(e)\nprint(e.itemsize)\n[1.+0.j 2.+0.j 3.+0.j 4.+0.j]\n8\n\n\n0.6.3 data 버퍼\n배열의 데이터는 1차원 바이트 버퍼로 메모리에 저장됩니다. data 속성을 사용해 참조할 수 있습니다(사용할 일은 거의 없겠지만요).\nf = np.array([[1,2],[1000, 2000]], dtype=np.int32)\nprint(f)\nprint(f.data)\n[[   1    2]\n [1000 2000]]\n<memory at 0x0000020364206380>\n파이썬 2에서는 f.data가 버퍼이고 파이썬 3에서는 memoryview입니다.\nif (hasattr(f.data, \"tobytes\")):\n    data_bytes = f.data.tobytes() # python 3\nelse:\n    data_bytes = memoryview(f.data).tobytes() # python 2\n\ndata_bytes\nb'\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\xe8\\x03\\x00\\x00\\xd0\\x07\\x00\\x00'\n여러 개의 ndarray가 데이터 버퍼를 공유할 수 있습니다. 하나를 수정하면 다른 것도 바뀝니다. 잠시 후에 예를 살펴 보겠습니다."
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#dtype",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#dtype",
    "title": "Data_Mining CH1",
    "section": "16 dtype",
    "text": "16 dtype\n넘파이의 ndarray는 모든 원소가 동일한 타입(보통 숫자)을 가지기 때문에 효율적입니다. dtype 속성으로 쉽게 데이터 타입을 확인할 수 있습니다:\nc = np.arange(1, 5)\nprint(c.dtype, c)\nint32 [1 2 3 4]\nc = np.arange(1.0, 5.0)\nprint(c.dtype, c)\nfloat64 [1. 2. 3. 4.]\n넘파이가 데이터 타입을 결정하도록 내버려 두는 대신 dtype 매개변수를 사용해서 배열을 만들 때 명시적으로 지정할 수 있습니다:\nd = np.arange(1, 5, dtype=np.complex64)\nprint(d.dtype, d)\ncomplex64 [1.+0.j 2.+0.j 3.+0.j 4.+0.j]\n가능한 데이터 타입은 int8, int16, int32, int64, uint8|16|32|64, float16|32|64, complex64|128가 있습니다. 전체 리스트는 온라인 문서를 참고하세요."
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#itemsize",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#itemsize",
    "title": "Data_Mining CH1",
    "section": "17 itemsize",
    "text": "17 itemsize\nitemsize 속성은 각 아이템의 크기(바이트)를 반환합니다:\ne = np.arange(1, 5, dtype=np.complex64)\ne.itemsize\n8"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#data-버퍼",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#data-버퍼",
    "title": "Data_Mining CH1",
    "section": "18 data 버퍼",
    "text": "18 data 버퍼\n배열의 데이터는 1차원 바이트 버퍼로 메모리에 저장됩니다. data 속성을 사용해 참조할 수 있습니다(사용할 일은 거의 없겠지만요).\nf = np.array([[1,2],[1000, 2000]], dtype=np.int32)\nf.data\n<memory at 0x00000250B4976E10>\n파이썬 2에서는 f.data가 버퍼이고 파이썬 3에서는 memoryview입니다.\nif (hasattr(f.data, \"tobytes\")):\n    data_bytes = f.data.tobytes() # python 3\nelse:\n    data_bytes = memoryview(f.data).tobytes() # python 2\n\ndata_bytes\nb'\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\xe8\\x03\\x00\\x00\\xd0\\x07\\x00\\x00'\n여러 개의 ndarray가 데이터 버퍼를 공유할 수 있습니다. 하나를 수정하면 다른 것도 바뀝니다. 잠시 후에 예를 살펴 보겠습니다."
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#배열-크기-변경",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#배열-크기-변경",
    "title": "Data_Mining CH1",
    "section": "0.7 배열 크기 변경",
    "text": "0.7 배열 크기 변경\n\n0.7.1 자신을 변경\nndarray의 shape 속성을 지정하면 간단히 크기를 바꿀 수 있습니다. 배열의 원소 개수는 동일하게 유지됩니다.\ng = np.arange(24)\nprint(g)\nprint(\"랭크:\", g.ndim)\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\n랭크: 1\ng.shape = (6, 4)\nprint(g)\nprint(\"랭크:\", g.ndim)\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]\n [12 13 14 15]\n [16 17 18 19]\n [20 21 22 23]]\n랭크: 2\ng.shape = (2, 3, 4)\nprint(g)\nprint(\"랭크:\", g.ndim)\n[[[ 0  1  2  3]\n  [ 4  5  6  7]\n  [ 8  9 10 11]]\n\n [[12 13 14 15]\n  [16 17 18 19]\n  [20 21 22 23]]]\n랭크: 3\ng[1,1,1] # 3차원 인덱싱 0,1 ~ 순\n17\n\n\n0.7.2 reshape\nreshape 함수는 동일한 데이터를 가리키는 새로운 ndarray 객체를 반환합니다. 한 배열을 수정하면 다른 것도 함께 바뀝니다.\ng2 = g.reshape(4,6)\nprint(g2)\nprint(\"랭크:\", g2.ndim)\n[[ 0  1  2  3  4  5]\n [ 6  7  8  9 10 11]\n [12 13 14 15 16 17]\n [18 19 20 21 22 23]]\n랭크: 2\n행 1, 열 2의 원소를 999로 설정합니다(인덱싱 방식은 아래를 참고하세요).\ng2[1, 2] = 999\ng2\narray([[  0,   1,   2,   3,   4,   5],\n       [  6,   7, 999,   9,  10,  11],\n       [ 12,  13,  14,  15,  16,  17],\n       [ 18,  19,  20,  21,  22,  23]])\n이에 상응하는 g의 원소도 수정됩니다.\ng\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [999,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]]])\n\n\n0.7.3 ravel\n마지막으로 ravel 함수는 동일한 데이터를 가리키는 새로운 1차원 ndarray를 반환합니다:\ng.ravel()\narray([  0,   1,   2,   3,   4,   5,   6,   7, 999,   9,  10,  11,  12,\n        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#자신을-변경",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#자신을-변경",
    "title": "Data_Mining CH1",
    "section": "20 자신을 변경",
    "text": "20 자신을 변경\nndarray의 shape 속성을 지정하면 간단히 크기를 바꿀 수 있습니다. 배열의 원소 개수는 동일하게 유지됩니다.\ng = np.arange(24)\nprint(g)\nprint(\"랭크:\", g.ndim)\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\n랭크: 1\ng.shape = (6, 4)\nprint(g)\nprint(\"랭크:\", g.ndim)\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]\n [12 13 14 15]\n [16 17 18 19]\n [20 21 22 23]]\n랭크: 2\ng.shape = (2, 3, 4)\nprint(g)\nprint(\"랭크:\", g.ndim)\n[[[ 0  1  2  3]\n  [ 4  5  6  7]\n  [ 8  9 10 11]]\n\n [[12 13 14 15]\n  [16 17 18 19]\n  [20 21 22 23]]]\n랭크: 3\ng[1,1,1] # 3차원 인덱싱 0,1 ~ 순\n17"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#reshape",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#reshape",
    "title": "Data_Mining CH1",
    "section": "21 reshape",
    "text": "21 reshape\nreshape 함수는 동일한 데이터를 가리키는 새로운 ndarray 객체를 반환합니다. 한 배열을 수정하면 다른 것도 함께 바뀝니다.\ng2 = g.reshape(4,6)\nprint(g2)\nprint(\"랭크:\", g2.ndim)\n[[ 0  1  2  3  4  5]\n [ 6  7  8  9 10 11]\n [12 13 14 15 16 17]\n [18 19 20 21 22 23]]\n랭크: 2\n행 1, 열 2의 원소를 999로 설정합니다(인덱싱 방식은 아래를 참고하세요).\ng2[1, 2] = 999\ng2\narray([[  0,   1,   2,   3,   4,   5],\n       [  6,   7, 999,   9,  10,  11],\n       [ 12,  13,  14,  15,  16,  17],\n       [ 18,  19,  20,  21,  22,  23]])\n이에 상응하는 g의 원소도 수정됩니다.\ng\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [999,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#ravel",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#ravel",
    "title": "Data_Mining CH1",
    "section": "22 ravel",
    "text": "22 ravel\n마지막으로 ravel 함수는 동일한 데이터를 가리키는 새로운 1차원 ndarray를 반환합니다:\ng.ravel()\narray([  0,   1,   2,   3,   4,   5,   6,   7, 999,   9,  10,  11,  12,\n        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#산술-연산",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#산술-연산",
    "title": "Data_Mining CH1",
    "section": "0.8 산술 연산",
    "text": "0.8 산술 연산\n일반적인 산술 연산자(+, -, *, /, //, ** 등)는 모두 ndarray와 사용할 수 있습니다. 이 연산자는 원소별로 적용됩니다:\na = np.array([14, 23, 32, 41])\nb = np.array([5,  4,  3,  2])\nprint(\"a + b  =\", a + b)\nprint(\"a - b  =\", a - b)\nprint(\"a * b  =\", a * b)\nprint(\"a / b  =\", a / b)\nprint(\"a // b  =\", a // b) # 몫\nprint(\"a % b  =\", a % b)   # 나머지\nprint(\"a ** b =\", a ** b)\na + b  = [19 27 35 43]\na - b  = [ 9 19 29 39]\na * b  = [70 92 96 82]\na / b  = [ 2.8         5.75       10.66666667 20.5       ]\na // b  = [ 2  5 10 20]\na % b  = [4 3 2 1]\na ** b = [537824 279841  32768   1681]\n여기 곱셈은 행렬 곱셈이 아닙니다. 행렬 연산은 아래에서 설명합니다.\n배열의 크기는 같아야 합니다. 그렇지 않으면 넘파이가 브로드캐스팅 규칙을 적용합니다."
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#브로드캐스팅",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#브로드캐스팅",
    "title": "Data_Mining CH1",
    "section": "0.9 브로드캐스팅",
    "text": "0.9 브로드캐스팅\n일반적으로 넘파이는 동일한 크기의 배열을 기대합니다. 그렇지 않은 상황에는 브로드캐시틍 규칙을 적용합니다:\n\n0.9.1 규칙 1\n배열의 랭크가 동일하지 않으면 랭크가 맞을 때까지 랭크가 작은 배열 앞에 1을 추가합니다.\nh = np.arange(5).reshape(1,1, 5)\nh\narray([[[0, 1, 2, 3, 4]]])\n여기에 (1,1,5) 크기의 3D 배열에 (5,) 크기의 1D 배열을 더해 보죠. 브로드캐스팅의 규칙 1이 적용됩니다!\nh + [10, 20, 30, 40, 50]  # 다음과 동일합니다: h + [[[10, 20, 30, 40, 50]]]\narray([[[10, 21, 32, 43, 54]]])\n\n\n0.9.2 규칙 2\n특정 차원이 1인 배열은 그 차원에서 크기가 가장 큰 배열의 크기에 맞춰 동작합니다. 배열의 원소가 차원을 따라 반복됩니다.\nk = np.arange(6).reshape(2, 3)\nk\narray([[0, 1, 2],\n       [3, 4, 5]])\n(2,3) 크기의 2D ndarray에 (2,1) 크기의 2D 배열을 더해 보죠. 넘파이는 브로드캐스팅 규칙 2를 적용합니다:\nk + [[100], [200]]  # 다음과 같습니다: k + [[100, 100, 100], [200, 200, 200]]\narray([[100, 101, 102],\n       [203, 204, 205]])\n규칙 1과 2를 합치면 다음과 같이 동작합니다:\nk + [100, 200, 300]  # 규칙 1 적용: [[100, 200, 300]], 규칙 2 적용: [[100, 200, 300], [100, 200, 300]]\narray([[100, 201, 302],\n       [103, 204, 305]])\n또 매우 간단히 다음 처럼 해도 됩니다:\nk + 1000  # 다음과 같습니다: k + [[1000, 1000, 1000], [1000, 1000, 1000]]\narray([[1000, 1001, 1002],\n       [1003, 1004, 1005]])\n\n\n0.9.3 규칙 3\n규칙 1 & 2을 적용했을 때 모든 배열의 크기가 맞아야 합니다.\n### 차원(배열 크기)가 달라서 생기는 문제\ntry:\n    k + [33, 44]\nexcept ValueError as e:\n    print(e)\noperands could not be broadcast together with shapes (2,3) (2,) \n브로드캐스팅 규칙은 산술 연산 뿐만 아니라 넘파이 연산에서 많이 사용됩니다. 아래에서 더 보도록 하죠. 브로드캐스팅에 관한 더 자세한 정보는 온라인 문서를 참고하세요."
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#규칙-1",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#규칙-1",
    "title": "Data_Mining CH1",
    "section": "25 규칙 1",
    "text": "25 규칙 1\n배열의 랭크가 동일하지 않으면 랭크가 맞을 때까지 랭크가 작은 배열 앞에 1을 추가합니다.\nh = np.arange(5).reshape(1, 1, 5)\nh\narray([[[0, 1, 2, 3, 4]]])\n여기에 (1,1,5) 크기의 3D 배열에 (5,) 크기의 1D 배열을 더해 보죠. 브로드캐스팅의 규칙 1이 적용됩니다!\nh + [10, 20, 30, 40, 50]  # 다음과 동일합니다: h + [[[10, 20, 30, 40, 50]]]\narray([[[10, 21, 32, 43, 54]]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#규칙-2",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#규칙-2",
    "title": "Data_Mining CH1",
    "section": "26 규칙 2",
    "text": "26 규칙 2\n특정 차원이 1인 배열은 그 차원에서 크기가 가장 큰 배열의 크기에 맞춰 동작합니다. 배열의 원소가 차원을 따라 반복됩니다.\nk = np.arange(6).reshape(2, 3)\nk\narray([[0, 1, 2],\n       [3, 4, 5]])\n(2,3) 크기의 2D ndarray에 (2,1) 크기의 2D 배열을 더해 보죠. 넘파이는 브로드캐스팅 규칙 2를 적용합니다:\nk + [[100], [200]]  # 다음과 같습니다: k + [[100, 100, 100], [200, 200, 200]]\narray([[100, 101, 102],\n       [203, 204, 205]])\n규칙 1과 2를 합치면 다음과 같이 동작합니다:\nk + [100, 200, 300]  # 규칙 1 적용: [[100, 200, 300]], 규칙 2 적용: [[100, 200, 300], [100, 200, 300]]\narray([[100, 201, 302],\n       [103, 204, 305]])\n또 매우 간단히 다음 처럼 해도 됩니다:\nk + 1000  # 다음과 같습니다: k + [[1000, 1000, 1000], [1000, 1000, 1000]]\narray([[1000, 1001, 1002],\n       [1003, 1004, 1005]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#규칙-3",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#규칙-3",
    "title": "Data_Mining CH1",
    "section": "27 규칙 3",
    "text": "27 규칙 3\n규칙 1 & 2을 적용했을 때 모든 배열의 크기가 맞아야 합니다.\ntry:\n    k + [33, 44]\nexcept ValueError as e:\n    print(e)\noperands could not be broadcast together with shapes (2,3) (2,) \n브로드캐스팅 규칙은 산술 연산 뿐만 아니라 넘파이 연산에서 많이 사용됩니다. 아래에서 더 보도록 하죠. 브로드캐스팅에 관한 더 자세한 정보는 온라인 문서를 참고하세요."
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#업캐스팅",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#업캐스팅",
    "title": "Data_Mining CH1",
    "section": "0.10 업캐스팅",
    "text": "0.10 업캐스팅\n\n0.10.1 스킵\ndtype이 다른 배열을 합칠 때 넘파이는 (실제 값에 상관없이) 모든 값을 다룰 수 있는 타입으로 업캐스팅합니다.\nk1 = np.arange(5, dtype=np.uint8)\nprint(k1.dtype, k1)\nuint8 [0 1 2 3 4]\nk2 = k1 + np.array([5, 6, 7, 8, 9], dtype=np.int8)\nprint(k2.dtype, k2)\nint16 [ 5  7  9 11 13]\n모든 int8과 uint8 값(-128에서 255까지)을 표현하기 위해 int16이 필요합니다. 이 코드에서는 uint8이면 충분하지만 업캐스팅되었습니다.\nk3 = k1 + 1.5\nprint(k3.dtype, k3)\nfloat64 [1.5 2.5 3.5 4.5 5.5]"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#스킵",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#스킵",
    "title": "Data_Mining CH1",
    "section": "29 스킵",
    "text": "29 스킵\ndtype이 다른 배열을 합칠 때 넘파이는 (실제 값에 상관없이) 모든 값을 다룰 수 있는 타입으로 업캐스팅합니다.\nk1 = np.arange(0, 5, dtype=np.uint8)\nprint(k1.dtype, k1)\nuint8 [0 1 2 3 4]\nk2 = k1 + np.array([5, 6, 7, 8, 9], dtype=np.int8)\nprint(k2.dtype, k2)\nint16 [ 5  7  9 11 13]\n모든 int8과 uint8 값(-128에서 255까지)을 표현하기 위해 int16이 필요합니다. 이 코드에서는 uint8이면 충분하지만 업캐스팅되었습니다.\nk3 = k1 + 1.5\nprint(k3.dtype, k3)\nfloat64 [1.5 2.5 3.5 4.5 5.5]"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#조건-연산자",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#조건-연산자",
    "title": "Data_Mining CH1",
    "section": "0.11 조건 연산자",
    "text": "0.11 조건 연산자\n조건 연산자도 원소별로 적용됩니다:\nm = np.array([20, -5, 30, 40])\nm < [15, 16, 35, 36]\narray([False,  True,  True, False])\n브로드캐스팅을 사용합니다:\nm < 25  # m < [25, 25, 25, 25] 와 동일\narray([ True,  True, False, False])\n불리언 인덱싱과 함께 사용하면 아주 유용합니다(아래에서 설명하겠습니다).\nm[m < 25]\narray([20, -5])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#수학-함수와-통계-함수",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#수학-함수와-통계-함수",
    "title": "Data_Mining CH1",
    "section": "0.12 수학 함수와 통계 함수",
    "text": "0.12 수학 함수와 통계 함수\nndarray에서 사용할 수 있는 수학 함수와 통계 함수가 많습니다.\n\n0.12.1 ndarray 메서드\n일부 함수는 ndarray 메서드로 제공됩니다. 예를 들면:\na = np.array([[-2.5, 3.1, 7], [10, 11, 12]])\nprint(a)\nprint(\"평균 =\", a.mean())\n# 축에 따라서 다르게 : axis= n\n[[-2.5  3.1  7. ]\n [10.  11.  12. ]]\n평균 = 6.766666666666667\n이 명령은 크기에 상관없이 ndarray에 있는 모든 원소의 평균을 계산합니다.\n다음은 유용한 ndarray 메서드입니다:\nfor func in (a.min, a.max, a.sum, a.prod, a.std, a.var):\n    print(func.__name__, \"=\", func())\nmin = -2.5\nmax = 12.0\nsum = 40.6\nprod = -71610.0\nstd = 5.084835843520964\nvar = 25.855555555555554\n이 함수들은 선택적으로 매개변수 axis를 사용합니다. 지정된 축을 따라 원소에 연산을 적용하는데 사용합니다. 예를 들면:\nc=np.arange(24).reshape(2,3,4)\nc\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]])\nc.sum(axis=0)  # 첫 번째 축을 따라 더함, 결과는 3x4 배열\narray([[12, 14, 16, 18],\n       [20, 22, 24, 26],\n       [28, 30, 32, 34]])\nc.sum(axis=1)  # 두 번째 축을 따라 더함, 결과는 2x4 배열\narray([[12, 15, 18, 21],\n       [48, 51, 54, 57]])\nc.sum(axis=2)\narray([[ 6, 22, 38],\n       [54, 70, 86]])\n여러 축에 대해서 더할 수도 있습니다:\nc.sum(axis=(0,2))  # 첫 번째 축과 세 번째 축을 따라 더함, 결과는 (3,) 배열\narray([ 60,  92, 124])\n0+1+2+3 + 12+13+14+15, 4+5+6+7 + 16+17+18+19, 8+9+10+11 + 20+21+22+23\n(60, 92, 124)"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#ndarray-메서드",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#ndarray-메서드",
    "title": "Data_Mining CH1",
    "section": "32 ndarray 메서드",
    "text": "32 ndarray 메서드\n일부 함수는 ndarray 메서드로 제공됩니다. 예를 들면:\na = np.array([[-2.5, 3.1, 7], [10, 11, 12]])\nprint(a)\nprint(\"평균 =\", a.mean())\n# 축에 따라서 다르게 : axis= n\n[[-2.5  3.1  7. ]\n [10.  11.  12. ]]\n평균 = 6.766666666666667\n이 명령은 크기에 상관없이 ndarray에 있는 모든 원소의 평균을 계산합니다.\n다음은 유용한 ndarray 메서드입니다:\nfor func in (a.min, a.max, a.sum, a.prod, a.std, a.var):\n    print(func.__name__, \"=\", func())\nmin = -2.5\nmax = 12.0\nsum = 40.6\nprod = -71610.0\nstd = 5.084835843520964\nvar = 25.855555555555554\n이 함수들은 선택적으로 매개변수 axis를 사용합니다. 지정된 축을 따라 원소에 연산을 적용하는데 사용합니다. 예를 들면:\nc=np.arange(24).reshape(2,3,4)\nc\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]])\nc.sum(axis=0)  # 첫 번째 축을 따라 더함, 결과는 3x4 배열\narray([[12, 14, 16, 18],\n       [20, 22, 24, 26],\n       [28, 30, 32, 34]])\nc.sum(axis=1)  # 두 번째 축을 따라 더함, 결과는 2x4 배열\narray([[12, 15, 18, 21],\n       [48, 51, 54, 57]])\nc.sum(axis=2)\narray([[ 6, 22, 38],\n       [54, 70, 86]])\n여러 축에 대해서 더할 수도 있습니다:\nc.sum(axis=(0,2))  # 첫 번째 축과 세 번째 축을 따라 더함, 결과는 (3,) 배열\narray([ 60,  92, 124])\n0+1+2+3 + 12+13+14+15, 4+5+6+7 + 16+17+18+19, 8+9+10+11 + 20+21+22+23\n(60, 92, 124)"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#일반-함수",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#일반-함수",
    "title": "Data_Mining CH1",
    "section": "0.13 일반 함수",
    "text": "0.13 일반 함수\n넘파이는 일반 함수(universal function) 또는 ufunc라고 부르는 원소별 함수를 제공합니다. 예를 들면 square 함수는 원본 ndarray를 복사하여 각 원소를 제곱한 새로운 ndarray 객체를 반환합니다:\na = np.array([[-2.5, 3.1, 7], [10, 11, 12]])\nnp.square(a)\narray([[  6.25,   9.61,  49.  ],\n       [100.  , 121.  , 144.  ]])\n다음은 유용한 단항 일반 함수들입니다:\nprint(\"원본 ndarray\")\nprint(a)\nfor func in (np.abs, np.sqrt, np.exp, np.log, np.sign, np.ceil, np.modf, np.isnan, np.cos):\n    print(\"\\n\", func.__name__)\n    print(func(a))\n원본 ndarray\n[[-2.5  3.1  7. ]\n [10.  11.  12. ]]\n\n absolute\n[[ 2.5  3.1  7. ]\n [10.  11.  12. ]]\n\n sqrt\n[[       nan 1.76068169 2.64575131]\n [3.16227766 3.31662479 3.46410162]]\n\n exp\n[[8.20849986e-02 2.21979513e+01 1.09663316e+03]\n [2.20264658e+04 5.98741417e+04 1.62754791e+05]]\n\n log\n[[       nan 1.13140211 1.94591015]\n [2.30258509 2.39789527 2.48490665]]\n\n sign\n[[-1.  1.  1.]\n [ 1.  1.  1.]]\n\n ceil\n[[-2.  4.  7.]\n [10. 11. 12.]]\n\n modf\n(array([[-0.5,  0.1,  0. ],\n       [ 0. ,  0. ,  0. ]]), array([[-2.,  3.,  7.],\n       [10., 11., 12.]]))\n\n isnan\n[[False False False]\n [False False False]]\n\n cos\n[[-0.80114362 -0.99913515  0.75390225]\n [-0.83907153  0.0044257   0.84385396]]\n\n\nC:\\Users\\seong taek\\AppData\\Local\\Temp\\ipykernel_8916\\4103705789.py:5: RuntimeWarning: invalid value encountered in sqrt\n  print(func(a))\nC:\\Users\\seong taek\\AppData\\Local\\Temp\\ipykernel_8916\\4103705789.py:5: RuntimeWarning: invalid value encountered in log\n  print(func(a))"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#이항-일반-함수",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#이항-일반-함수",
    "title": "Data_Mining CH1",
    "section": "0.14 이항 일반 함수",
    "text": "0.14 이항 일반 함수\n두 개의 ndarray에 원소별로 적용되는 이항 함수도 많습니다. 두 배열이 동일한 크기가 아니면 브로드캐스팅 규칙이 적용됩니다:\na = np.array([1, -2, 3, 4])\nb = np.array([2, 8, -1, 7])\nnp.add(a, b)  # a + b 와 동일\narray([ 3,  6,  2, 11])\nnp.greater(a, b)  # a > b 와 동일\narray([False, False,  True, False])\nnp.maximum(a, b)\narray([2, 8, 3, 7])\nnp.copysign(a, b)\narray([ 1.,  2., -3.,  4.])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#배열-인덱싱",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#배열-인덱싱",
    "title": "Data_Mining CH1",
    "section": "0.15 배열 인덱싱",
    "text": "0.15 배열 인덱싱\n\n0.15.1 1차원 배열\n1차원 넘파이 배열은 보통의 파이썬 배열과 비슷하게 사용할 수 있습니다:\na = np.array([1, 5, 3, 19, 13, 7, 3])\na[3]\n19\na[2:5]\narray([ 3, 19, 13])\na[2:-1]\narray([ 3, 19, 13,  7])\na[:2]\narray([1, 5])\na[2::2]\narray([ 3, 13,  3])\na[::-1]\narray([ 3,  7, 13, 19,  3,  5,  1])\n물론 원소를 수정할 수 있죠:\na[3]=999\na\narray([  1,   5,   3, 999,  13,   7,   3])\n슬라이싱을 사용해 ndarray를 수정할 수 있습니다:\na[2:5] = [997, 998, 999]\na\narray([  1,   5, 997, 998, 999,   7,   3])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#차원-배열",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#차원-배열",
    "title": "Data_Mining CH1",
    "section": "36 1차원 배열",
    "text": "36 1차원 배열\n1차원 넘파이 배열은 보통의 파이썬 배열과 비슷하게 사용할 수 있습니다:\na = np.array([1, 5, 3, 19, 13, 7, 3])\na[3]\n19\na[2:5]\narray([ 3, 19, 13])\na[2:-1]\narray([ 3, 19, 13,  7])\na[:2]\narray([1, 5])\na[2::2]\narray([ 3, 13,  3])\na[::-1]\narray([ 3,  7, 13, 19,  3,  5,  1])\n물론 원소를 수정할 수 있죠:\na[3]=999\na\narray([  1,   5,   3, 999,  13,   7,   3])\n슬라이싱을 사용해 ndarray를 수정할 수 있습니다:\na[2:5] = [997, 998, 999]\na\narray([  1,   5, 997, 998, 999,   7,   3])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#보통의-파이썬-배열과-차이점",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#보통의-파이썬-배열과-차이점",
    "title": "Data_Mining CH1",
    "section": "0.16 보통의 파이썬 배열과 차이점",
    "text": "0.16 보통의 파이썬 배열과 차이점\n보통의 파이썬 배열과 대조적으로 ndarray 슬라이싱에 하나의 값을 할당하면 슬라이싱 전체에 복사됩니다. 위에서 언급한 브로드캐스팅 덕택입니다.\na[2:5] = -1\na\narray([ 1,  5, -1, -1, -1,  7,  3])\n또한 이런 식으로 ndarray 크기를 늘리거나 줄일 수 없습니다:\ntry:\n    a[2:5] = [1,2,3,4,5,6]  # 너무 길어요\nexcept ValueError as e:\n    print(e)\ncould not broadcast input array from shape (6,) into shape (3,)\n원소를 삭제할 수도 없습니다:\ntry:\n    del a[2:5]\nexcept ValueError as e:\n    print(e)\ncannot delete array elements\n중요한 점은 ndarray의 슬라이싱은 같은 데이터 버퍼를 바라보는 뷰(view)입니다. 슬라이싱된 객체를 수정하면 실제 원본 ndarray가 수정됩니다!\na_slice = a[2:6]\nprint(a_slice)\n\na_slice[1] = 1000\nprint(a_slice)\n\nprint(a)  # 원본 배열이 수정됩니다!\n[-1 -1 -1  7]\n[  -1 1000   -1    7]\n[   1    5   -1 1000   -1    7    3]\na[3] = 2000\na_slice  # 비슷하게 원본 배열을 수정하면 슬라이싱 객체에도 반영됩니다!\narray([  -1, 2000,   -1,    7])\n데이터를 복사하려면 copy 메서드를 사용해야 합니다:\nprint(a)\n\nanother_slice = a[2:6].copy()\nanother_slice[1] = 3000\n\nprint(a)  # 원본 배열이 수정되지 않습니다\n[   1    5   -1 2000   -1    7    3]\n[   1    5   -1 2000   -1    7    3]\na[3] = 4000\nanother_slice  # 마찬가지로 원본 배열을 수정해도 복사된 배열은 바뀌지 않습니다\narray([  -1, 3000,   -1,    7])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#다차원-배열",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#다차원-배열",
    "title": "Data_Mining CH1",
    "section": "0.17 다차원 배열",
    "text": "0.17 다차원 배열\n다차원 배열은 비슷한 방식으로 각 축을 따라 인덱싱 또는 슬라이싱해서 사용합니다. 콤마로 구분합니다:\nb = np.arange(48).reshape(4, 12)\nb\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n       [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])\nb[1, 2]  # 행 1, 열 2\n14\nb[1, :]  # 행 1, 모든 열\narray([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\nb[1, :].shape\n(12,)\nb[:, 1]  # 모든 행, 열 1\narray([ 1, 13, 25, 37])\n주의: 다음 두 표현에는 미묘한 차이가 있습니다:\nb[1, :]\narray([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\nb[1:2, :]\narray([[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]])\nb[1, :].shape\n(12,)\nb[1:2, :].shape\n(1, 12)\n첫 번째 표현식은 (12,) 크기인 1D 배열로 행이 하나입니다. 두 번째는 (1, 12) 크기인 2D 배열로 같은 행을 반환합니다."
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#팬시-인덱싱fancy-indexing",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#팬시-인덱싱fancy-indexing",
    "title": "Data_Mining CH1",
    "section": "0.18 팬시 인덱싱(Fancy indexing)",
    "text": "0.18 팬시 인덱싱(Fancy indexing)\n관심 대상의 인덱스 리스트를 지정할 수도 있습니다. 이를 팬시 인덱싱이라고 부릅니다.\nb\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n       [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])\nb[(0,2), 2:5]  # 행 0과 2, 열 2에서 4(5-1)까지\narray([[ 2,  3,  4],\n       [26, 27, 28]])\nb[:, (-1, 2, -1)]  # 모든 행, 열 -1 (마지막), 2와 -1 (다시 반대 방향으로)\narray([[11,  2, 11],\n       [23, 14, 23],\n       [35, 26, 35],\n       [47, 38, 47]])\n여러 개의 인덱스 리스트를 지정하면 인덱스에 맞는 값이 포함된 1D ndarray를 반환됩니다.\nb[(-1, 2, -1, 2), (5, 9, 1, 9)]  # returns a 1D array with b[-1, 5], b[2, 9], b[-1, 1] and b[2, 9] (again)\narray([41, 33, 37, 33])\n\n0.18.1 Quiz\narray([[24, 25], [36, 37]])\n출력해보기\nb[(2,3),0:2], b[2:4,0:2]\n(array([[24, 25],\n        [36, 37]]),\n array([[24, 25],\n        [36, 37]]))\n\n\n0.18.2 Quiz\n\n2차원 배열 ’array_2d’에서 첫번째 행의 모든 요소 선택\n2차원 배열 ’array_2d’에서 두번째 열의 모든 요소 선택\n2차원 배열 ’array_2d’에서 25,30,40,45 선택\n\narray_2d = np.array([[5, 10, 15],\n                    [20, 25, 30],\n                    [35, 40, 45]])\narray_2d\narray([[ 5, 10, 15],\n       [20, 25, 30],\n       [35, 40, 45]])\n# 1\nprint(array_2d[0,:])\n# 2\nprint(array_2d[:,1])\n# 3\nprint(array_2d[1:3,1:3])\n[ 5 10 15]\n[10 25 40]\n[[25 30]\n [40 45]]\n# 또는\narray_2d[0,:],array_2d[:,1], array_2d[(1,2),1:3]\n(array([ 5, 10, 15]),\n array([10, 25, 40]),\n array([[25, 30],\n        [40, 45]]))"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#quiz",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#quiz",
    "title": "Data_Mining CH1",
    "section": "40 Quiz",
    "text": "40 Quiz\narray([[24, 25], [36, 37]])\n출력해보기\nb[(2,3),0:2]\narray([[24, 25],\n       [36, 37]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#quiz-1",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#quiz-1",
    "title": "Data_Mining CH1",
    "section": "41 Quiz",
    "text": "41 Quiz\n\n2차원 배열 ’array_2d’에서 첫번째 행의 모든 요소 선택\n2차원 배열 ’array_2d’에서 두번째 열의 모든 요소 선택\n2차원 배열 ’array_2d’에서 25,30,40,45 선택\n\narray_2d = np.array([[5, 10, 15],\n                    [20, 25, 30],\n                    [35, 40, 45]])\narray_2d\narray([[ 5, 10, 15],\n       [20, 25, 30],\n       [35, 40, 45]])\n#답\narray_2d[0,:],array_2d[:,1], array_2d[(1,2),1:3]\n(array([ 5, 10, 15]),\n array([10, 25, 40]),\n array([[25, 30],\n        [40, 45]]))"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#고차원",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#고차원",
    "title": "Data_Mining CH1",
    "section": "0.19 고차원",
    "text": "0.19 고차원\n고차원에서도 동일한 방식이 적용됩니다. 몇 가지 예를 살펴 보겠습니다:\nc = b.reshape(4,2,6)\nc\narray([[[ 0,  1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10, 11]],\n\n       [[12, 13, 14, 15, 16, 17],\n        [18, 19, 20, 21, 22, 23]],\n\n       [[24, 25, 26, 27, 28, 29],\n        [30, 31, 32, 33, 34, 35]],\n\n       [[36, 37, 38, 39, 40, 41],\n        [42, 43, 44, 45, 46, 47]]])\nc[2, 1, 4]  # 행렬 2, 행 1, 열 4\n34\nc[2, :, 3]  # 행렬 2, 모든 행, 열 3\narray([27, 33])\n어떤 축에 대한 인덱스를 지정하지 않으면 이 축의 모든 원소가 반환됩니다:\nc[2, 1]  # 행렬 2, 행 1, 모든 열이 반환됩니다. c[2, 1, :]와 동일합니다.\narray([30, 31, 32, 33, 34, 35])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#생략-부호-...",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#생략-부호-...",
    "title": "Data_Mining CH1",
    "section": "0.20 생략 부호 (...)",
    "text": "0.20 생략 부호 (...)\n생략 부호(...)를 쓰면 모든 지정하지 않은 축의 원소를 포함합니다.\nc[2, ...]  #  행렬 2, 모든 행, 모든 열. c[2, :, :]와 동일\narray([[24, 25, 26, 27, 28, 29],\n       [30, 31, 32, 33, 34, 35]])\nc[2, 1, ...]  # 행렬 2, 행 1, 모든 열. c[2, 1, :]와 동일\narray([30, 31, 32, 33, 34, 35])\nc[2, ..., 3]  # 행렬 2, 모든 행, 열 3. c[2, :, 3]와 동일\narray([27, 33])\nc[..., 3]  # 모든 행렬, 모든 행, 열 3. c[:, :, 3]와 동일\narray([[ 3,  9],\n       [15, 21],\n       [27, 33],\n       [39, 45]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#불리언-인덱싱",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#불리언-인덱싱",
    "title": "Data_Mining CH1",
    "section": "0.21 불리언 인덱싱",
    "text": "0.21 불리언 인덱싱\n불리언 값을 가진 ndarray를 사용해 축의 인덱스를 지정할 수 있습니다.\nb = np.arange(48).reshape(4, 12)\nb\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n       [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])\nrows_on = np.array([True, False, True, False])\nb[rows_on, :]  # True에 해당하는 값 가져오기, 행 0과 2, 모든 열. b[(0, 2), :]와 동일\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]])\ncols_on = np.array([False, True, False] * 4)\nb[:, cols_on]  # 모든 행, 열 1, 4, 7, 10\narray([[ 1,  4,  7, 10],\n       [13, 16, 19, 22],\n       [25, 28, 31, 34],\n       [37, 40, 43, 46]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#np.ix_",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#np.ix_",
    "title": "Data_Mining CH1",
    "section": "0.22 np.ix_",
    "text": "0.22 np.ix_\n여러 축에 걸쳐서는 불리언 인덱싱을 사용할 수 없고 ix_ 함수를 사용합니다:\nb[np.ix_(rows_on, cols_on)]\narray([[ 1,  4,  7, 10],\n       [25, 28, 31, 34]])\nnp.ix_(rows_on, cols_on) # 추출 위치 출력\n(array([[0],\n        [2]], dtype=int64),\n array([[ 1,  4,  7, 10]], dtype=int64))\nndarray와 같은 크기의 불리언 배열을 사용하면 해당 위치가 True인 모든 원소를 담은 1D 배열이 반환됩니다. 일반적으로 조건 연산자와 함께 사용합니다:\nb[b % 3 == 1]\narray([ 1,  4,  7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#반복",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#반복",
    "title": "Data_Mining CH1",
    "section": "46 반복",
    "text": "46 반복\nndarray를 반복하는 것은 일반적인 파이썬 배열을 반복한는 것과 매우 유사합니다. 다차원 배열을 반복하면 첫 번째 축에 대해서 수행됩니다.\nc = np.arange(24).reshape(2, 3, 4)  # 3D 배열 (두 개의 3x4 행렬로 구성됨)\nc\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]])\nfor m in c:\n    print(\"아이템:\")\n    print(m)\n아이템:\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]]\n아이템:\n[[12 13 14 15]\n [16 17 18 19]\n [20 21 22 23]]\nfor i in range(len(c)):  # len(c) == c.shape[0]\n    print(\"아이템:\")\n    print(c[i])\n아이템:\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]]\n아이템:\n[[12 13 14 15]\n [16 17 18 19]\n [20 21 22 23]]\nndarray에 있는 모든 원소를 반복하려면 flat 속성을 사용합니다:\nfor i in c.flat:\n    print(\"아이템:\", i)\n아이템: 0\n아이템: 1\n아이템: 2\n아이템: 3\n아이템: 4\n아이템: 5\n아이템: 6\n아이템: 7\n아이템: 8\n아이템: 9\n아이템: 10\n아이템: 11\n아이템: 12\n아이템: 13\n아이템: 14\n아이템: 15\n아이템: 16\n아이템: 17\n아이템: 18\n아이템: 19\n아이템: 20\n아이템: 21\n아이템: 22\n아이템: 23"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#배열-쌓기",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#배열-쌓기",
    "title": "Data_Mining CH1",
    "section": "1.1 배열 쌓기",
    "text": "1.1 배열 쌓기\n종종 다른 배열을 쌓아야 할 때가 있습니다. 넘파이는 이를 위해 몇 개의 함수를 제공합니다. 먼저 배열 몇 개를 만들어 보죠.\nq1 = np.full((3,4), 1.0)\nq1\n# axis\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.]])\nq2 = np.full((4,4), 2.0)\nq2\narray([[2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.]])\nq3 = np.full((3,4), 3.0)\nq3\narray([[3., 3., 3., 3.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.]])\n\n1.1.1 vstack\nvstack 함수를 사용하여 수직으로 쌓아보죠:\nq4 = np.vstack((q1, q2, q3))\nq4\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.]])\nq4.shape\n(10, 4)\nq1, q2, q3가 모두 같은 크기이므로 가능합니다(수직으로 쌓기 때문에 수직 축은 크기가 달라도 됩니다).\n\n\n1.1.2 hstack\nhstack을 사용해 수평으로도 쌓을 수 있습니다:\nq5 = np.hstack((q1, q3))\nq5\narray([[1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.]])\n# concatenate도 가능\nnp.concatenate((q1,q3),axis=1)\narray([[1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.]])\nq5.shape\n(3, 8)\nq1과 q3가 모두 3개의 행을 가지고 있기 때문에 가능합니다. q2는 4개의 행을 가지고 있기 때문에 q1, q3와 수평으로 쌓을 수 없습니다:\ntry:\n    q5 = np.hstack((q1, q2, q3))\nexcept ValueError as e:\n    print(e)\nall the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 3 and the array at index 1 has size 4\n\n\n1.1.3 concatenate\nconcatenate 함수는 지정한 축으로도 배열을 쌓습니다.\nq7 = np.concatenate((q1, q2, q3), axis=0)  # vstack과 동일\nq7\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.]])\nq7.shape\n(10, 4)\n예상했겠지만 hstack은 axis=1으로 concatenate를 호출하는 것과 같습니다.\n\n\n1.1.4 stack\nstack 함수는 새로운 축을 따라 배열을 쌓습니다. 모든 배열은 같은 크기를 가져야 합니다.\nq8 = np.stack((q1, q3))\nq8\narray([[[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]],\n\n       [[3., 3., 3., 3.],\n        [3., 3., 3., 3.],\n        [3., 3., 3., 3.]]])\nq8.shape\n(2, 3, 4)"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#vstack",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#vstack",
    "title": "Data_Mining CH1",
    "section": "48 vstack",
    "text": "48 vstack\nvstack 함수를 사용하여 수직으로 쌓아보죠:\nq4 = np.vstack((q1, q2, q3))\nq4\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.]])\nq4.shape\n(10, 4)\nq1, q2, q3가 모두 같은 크기이므로 가능합니다(수직으로 쌓기 때문에 수직 축은 크기가 달라도 됩니다)."
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#hstack",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#hstack",
    "title": "Data_Mining CH1",
    "section": "49 hstack",
    "text": "49 hstack\nhstack을 사용해 수평으로도 쌓을 수 있습니다:\nq5 = np.hstack((q1, q3))\nq5\narray([[1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.]])\n# 문제\nnp.concatenate((q1,q3),axis=1)\narray([[1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.]])\nq5.shape\n(3, 8)\nq1과 q3가 모두 3개의 행을 가지고 있기 때문에 가능합니다. q2는 4개의 행을 가지고 있기 때문에 q1, q3와 수평으로 쌓을 수 없습니다:\ntry:\n    q5 = np.hstack((q1, q2, q3))\nexcept ValueError as e:\n    print(e)\nall the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 3 and the array at index 1 has size 4"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#concatenate",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#concatenate",
    "title": "Data_Mining CH1",
    "section": "50 concatenate",
    "text": "50 concatenate\nconcatenate 함수는 지정한 축으로도 배열을 쌓습니다.\nq7 = np.concatenate((q1, q2, q3), axis=0)  # vstack과 동일\nq7\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.]])\nq7.shape\n(10, 4)\n예상했겠지만 hstack은 axis=1으로 concatenate를 호출하는 것과 같습니다."
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#stack",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#stack",
    "title": "Data_Mining CH1",
    "section": "51 stack",
    "text": "51 stack\nstack 함수는 새로운 축을 따라 배열을 쌓습니다. 모든 배열은 같은 크기를 가져야 합니다.\nq8 = np.stack((q1, q3))\nq8\narray([[[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]],\n\n       [[3., 3., 3., 3.],\n        [3., 3., 3., 3.],\n        [3., 3., 3., 3.]]])\nq8.shape\n(2, 3, 4)"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#배열-분할",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#배열-분할",
    "title": "Data_Mining CH1",
    "section": "1.2 배열 분할",
    "text": "1.2 배열 분할\n분할은 쌓기의 반대입니다. 예를 들어 vsplit 함수는 행렬을 수직으로 분할합니다.\n먼저 6x4 행렬을 만들어 보죠:\nr = np.arange(24).reshape(6,4)\nr\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11],\n       [12, 13, 14, 15],\n       [16, 17, 18, 19],\n       [20, 21, 22, 23]])\n수직으로 동일한 크기로 나누어 보겠습니다:\n### 3개의 변수로 분할 - 수직\nr1, r2, r3 = np.vsplit(r, 3)\nr1\narray([[0, 1, 2, 3],\n       [4, 5, 6, 7]])\nr2\narray([[ 8,  9, 10, 11],\n       [12, 13, 14, 15]])\nr3\narray([[16, 17, 18, 19],\n       [20, 21, 22, 23]])\nsplit 함수는 주어진 축을 따라 배열을 분할합니다. vsplit는 axis=0으로 split를 호출하는 것과 같습니다. hsplit 함수는 axis=1로 split를 호출하는 것과 같습니다:\n### 2개의 변수로 분할 - 수평\nr4, r5 = np.hsplit(r, 2)\nr4\narray([[ 0,  1],\n       [ 4,  5],\n       [ 8,  9],\n       [12, 13],\n       [16, 17],\n       [20, 21]])\nr5\narray([[ 2,  3],\n       [ 6,  7],\n       [10, 11],\n       [14, 15],\n       [18, 19],\n       [22, 23]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#배열-전치",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#배열-전치",
    "title": "Data_Mining CH1",
    "section": "1.3 배열 전치",
    "text": "1.3 배열 전치\n읽어만 보기\ntranspose 메서드는 주어진 순서대로 축을 뒤바꾸어 ndarray 데이터에 대한 새로운 뷰를 만듭니다.\n예를 위해 3D 배열을 만들어 보죠:\nt = np.arange(24).reshape(4,2,3)\nt\narray([[[ 0,  1,  2],\n        [ 3,  4,  5]],\n\n       [[ 6,  7,  8],\n        [ 9, 10, 11]],\n\n       [[12, 13, 14],\n        [15, 16, 17]],\n\n       [[18, 19, 20],\n        [21, 22, 23]]])\n0, 1, 2(깊이, 높이, 너비) 축을 1, 2, 0 (깊이→너비, 높이→깊이, 너비→높이) 순서로 바꾼 ndarray를 만들어 보겠습니다:\nt1 = t.transpose((1,2,0))\nt1\narray([[[ 0,  6, 12, 18],\n        [ 1,  7, 13, 19],\n        [ 2,  8, 14, 20]],\n\n       [[ 3,  9, 15, 21],\n        [ 4, 10, 16, 22],\n        [ 5, 11, 17, 23]]])\nt1.shape\n(2, 3, 4)\ntranspose 기본값은 차원의 순서를 역전시킵니다:\nt2 = t.transpose()  # t.transpose((2, 1, 0))와 동일\nt2\narray([[[ 0,  6, 12, 18],\n        [ 3,  9, 15, 21]],\n\n       [[ 1,  7, 13, 19],\n        [ 4, 10, 16, 22]],\n\n       [[ 2,  8, 14, 20],\n        [ 5, 11, 17, 23]]])\nt2.shape\n(3, 2, 4)\n넘파이는 두 축을 바꾸는 swapaxes 함수를 제공합니다. 예를 들어 깊이와 높이를 뒤바꾸어 t의 새로운 뷰를 만들어 보죠:\nt3 = t.swapaxes(0,1)  # t.transpose((1, 0, 2))와 동일\nt3\narray([[[ 0,  1,  2],\n        [ 6,  7,  8],\n        [12, 13, 14],\n        [18, 19, 20]],\n\n       [[ 3,  4,  5],\n        [ 9, 10, 11],\n        [15, 16, 17],\n        [21, 22, 23]]])\nt3.shape\n(2, 4, 3)"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#선형-대수학",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#선형-대수학",
    "title": "Data_Mining CH1",
    "section": "1.4 선형 대수학",
    "text": "1.4 선형 대수학\n넘파이 2D 배열을 사용하면 파이썬에서 행렬을 효율적으로 표현할 수 있습니다. 주요 행렬 연산을 간단히 둘러 보겠습니다. 선형 대수학, 벡터와 행렬에 관한 자세한 내용은 Linear Algebra tutorial를 참고하세요."
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#행렬-전치",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#행렬-전치",
    "title": "Data_Mining CH1",
    "section": "1.5 행렬 전치",
    "text": "1.5 행렬 전치\nT 속성은 랭크가 2보다 크거나 같을 때 transpose()를 호출하는 것과 같습니다:\nm1 = np.arange(10).reshape(2,5)\nm1\narray([[0, 1, 2, 3, 4],\n       [5, 6, 7, 8, 9]])\nm1.T\narray([[0, 5],\n       [1, 6],\n       [2, 7],\n       [3, 8],\n       [4, 9]])\nT 속성은 랭크가 0이거나 1인 배열에는 아무런 영향을 미치지 않습니다:\nm2 = np.arange(5)\nm2\narray([0, 1, 2, 3, 4])\nm2.T\narray([0, 1, 2, 3, 4])\n먼저 1D 배열을 하나의 행이 있는 행렬(2D)로 바꾼다음 전치를 수행할 수 있습니다:\nm2r = m2.reshape(1,5)\nm2r\narray([[0, 1, 2, 3, 4]])\nm2.shape, m2r.shape\n((5,), (1, 5))\nm2r.T\narray([[0],\n       [1],\n       [2],\n       [3],\n       [4]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#행렬-곱셈",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#행렬-곱셈",
    "title": "Data_Mining CH1",
    "section": "1.6 행렬 곱셈",
    "text": "1.6 행렬 곱셈\n두 개의 행렬을 만들어 dot 메서드로 행렬 곱셈을 실행해 보죠.\nn1 = np.arange(10).reshape(2, 5)\nn1\narray([[0, 1, 2, 3, 4],\n       [5, 6, 7, 8, 9]])\nn2 = np.arange(15).reshape(5,3)\nn2\narray([[ 0,  1,  2],\n       [ 3,  4,  5],\n       [ 6,  7,  8],\n       [ 9, 10, 11],\n       [12, 13, 14]])\nn1.dot(n2)\narray([[ 90, 100, 110],\n       [240, 275, 310]])\n주의: 앞서 언급한 것처럼 n1*n2는 행렬 곱셈이 아니라 원소별 곱셈(또는 아다마르 곱이라 부릅니다)입니다."
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#역행렬과-유사-역행렬",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#역행렬과-유사-역행렬",
    "title": "Data_Mining CH1",
    "section": "1.7 역행렬과 유사 역행렬",
    "text": "1.7 역행렬과 유사 역행렬\nnumpy.linalg 모듈 안에 많은 선형 대수 함수들이 있습니다. 특히 inv 함수는 정방 행렬의 역행렬을 계산합니다:\nimport numpy.linalg as linalg\n\nm3 = np.array([[1,2,3],[5,7,11],[21,29,31]])\nm3\narray([[ 1,  2,  3],\n       [ 5,  7, 11],\n       [21, 29, 31]])\nlinalg.inv(m3)\narray([[-2.31818182,  0.56818182,  0.02272727],\n       [ 1.72727273, -0.72727273,  0.09090909],\n       [-0.04545455,  0.29545455, -0.06818182]])\npinv 함수를 사용하여 유사 역행렬을 계산할 수도 있습니다:\nlinalg.pinv(m3)\narray([[-2.31818182,  0.56818182,  0.02272727],\n       [ 1.72727273, -0.72727273,  0.09090909],\n       [-0.04545455,  0.29545455, -0.06818182]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#단위-행렬",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#단위-행렬",
    "title": "Data_Mining CH1",
    "section": "1.8 단위 행렬",
    "text": "1.8 단위 행렬\n행렬과 그 행렬의 역행렬을 곱하면 단위 행렬이 됩니다(작은 소숫점 오차가 있습니다):\nm3.dot(linalg.inv(m3))\narray([[ 1.00000000e+00, -5.55111512e-17,  0.00000000e+00],\n       [-2.98372438e-16,  1.00000000e+00, -5.55111512e-17],\n       [ 5.78009862e-15,  1.27675648e-15,  1.00000000e+00]])\neye 함수는 NxN 크기의 단위 행렬을 만듭니다:\nnp.eye(3)\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#qr-분해",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#qr-분해",
    "title": "Data_Mining CH1",
    "section": "1.9 QR 분해",
    "text": "1.9 QR 분해\nqr 함수는 행렬을 QR 분해합니다:\nq, r = linalg.qr(m3)\nq\narray([[-0.04627448,  0.98786672,  0.14824986],\n       [-0.23137241,  0.13377362, -0.96362411],\n       [-0.97176411, -0.07889213,  0.22237479]])\nr\narray([[-21.61018278, -29.89331494, -32.80860727],\n       [  0.        ,   0.62427688,   1.9894538 ],\n       [  0.        ,   0.        ,  -3.26149699]])\nq.dot(r)  # q.r는 m3와 같습니다\narray([[ 1.,  2.,  3.],\n       [ 5.,  7., 11.],\n       [21., 29., 31.]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#행렬식",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#행렬식",
    "title": "Data_Mining CH1",
    "section": "1.10 행렬식",
    "text": "1.10 행렬식\ndet 함수는 행렬식을 계산합니다:\nlinalg.det(m3)  # 행렬식 계산\n43.99999999999999"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#고윳값과-고유벡터",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#고윳값과-고유벡터",
    "title": "Data_Mining CH1",
    "section": "1.11 고윳값과 고유벡터",
    "text": "1.11 고윳값과 고유벡터\neig 함수는 정방 행렬의 고윳값과 고유벡터를 계산합니다:\neigenvalues, eigenvectors = linalg.eig(m3)\neigenvalues # λ\narray([42.26600592, -0.35798416, -2.90802176])\neigenvectors # v\narray([[-0.08381182, -0.76283526, -0.18913107],\n       [-0.3075286 ,  0.64133975, -0.6853186 ],\n       [-0.94784057, -0.08225377,  0.70325518]])\nm3.dot(eigenvectors) - eigenvalues * eigenvectors  # m3.v - λ*v = 0\narray([[6.66133815e-15, 1.11022302e-16, 4.44089210e-16],\n       [7.10542736e-15, 2.88657986e-15, 3.10862447e-15],\n       [3.55271368e-14, 6.81746326e-15, 4.88498131e-15]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#특잇값-분해",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#특잇값-분해",
    "title": "Data_Mining CH1",
    "section": "1.12 특잇값 분해",
    "text": "1.12 특잇값 분해\nsvd 함수는 행렬을 입력으로 받아 그 행렬의 특잇값 분해를 반환합니다:\nm4 = np.array([[1,0,0,0,2], [0,0,3,0,0], [0,0,0,0,0], [0,2,0,0,0]])\nm4\narray([[1, 0, 0, 0, 2],\n       [0, 0, 3, 0, 0],\n       [0, 0, 0, 0, 0],\n       [0, 2, 0, 0, 0]])\nU, S_diag, V = linalg.svd(m4)\nU\narray([[ 0.,  1.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 0.,  0.,  0., -1.],\n       [ 0.,  0.,  1.,  0.]])\nS_diag\narray([3.        , 2.23606798, 2.        , 0.        ])\nsvd 함수는 Σ의 대각 원소 값만 반환합니다. 전체 Σ 행렬은 다음과 같이 만듭니다:\nS = np.zeros((4, 5))\nS[np.diag_indices(4)] = S_diag\nS  # Σ\narray([[3.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 2.23606798, 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 2.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ]])\nV\narray([[-0.        ,  0.        ,  1.        ,  0.        ,  0.        ],\n       [ 0.4472136 ,  0.        ,  0.        ,  0.        ,  0.89442719],\n       [-0.        ,  1.        ,  0.        ,  0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ],\n       [-0.89442719,  0.        ,  0.        ,  0.        ,  0.4472136 ]])\nU.dot(S).dot(V) # U.Σ.V == m4\narray([[1., 0., 0., 0., 2.],\n       [0., 0., 3., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 2., 0., 0., 0.]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#대각원소와-대각합",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#대각원소와-대각합",
    "title": "Data_Mining CH1",
    "section": "1.13 대각원소와 대각합",
    "text": "1.13 대각원소와 대각합\nnp.diag(m3)  # m3의 대각 원소입니다(왼쪽 위에서 오른쪽 아래)\narray([ 1,  7, 31])\nnp.trace(m3)  # np.diag(m3).sum()와 같습니다\n39"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#선형-방정식-풀기",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#선형-방정식-풀기",
    "title": "Data_Mining CH1",
    "section": "1.14 선형 방정식 풀기",
    "text": "1.14 선형 방정식 풀기\nsolve 함수는 다음과 같은 선형 방정식을 풉니다:\n\n\\(2x + 6y = 6\\)\n\\(5x + 3y = -9\\)\n\ncoeffs  = np.array([[2, 6], [5, 3]])\ndepvars = np.array([6, -9])\nsolution = linalg.solve(coeffs, depvars)\nsolution\narray([-3.,  2.])\nsolution을 확인해 보죠:\ncoeffs.dot(solution), depvars  # 네 같네요\n(array([ 6., -9.]), array([ 6, -9]))\n좋습니다! 다른 방식으로도 solution을 확인해 보죠:\nnp.allclose(coeffs.dot(solution), depvars)\nTrue"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#벡터화",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#벡터화",
    "title": "Data_Mining CH1",
    "section": "1.15 벡터화",
    "text": "1.15 벡터화\n한 번에 하나씩 개별 배열 원소에 대해 연산을 실행하는 대신 배열 연산을 사용하면 훨씬 효율적인 코드를 만들 수 있습니다. 이를 벡터화라고 합니다. 이를 사용하여 넘파이의 최적화된 성능을 활용할 수 있습니다.\n예를 들어, \\(sin(xy/40.5)\\) 식을 기반으로 768x1024 크기 배열을 생성하려고 합니다. 중첩 반복문 안에 파이썬의 math 함수를 사용하는 것은 나쁜 방법입니다:\nimport math\ndata = np.empty((768, 1024))\nfor y in range(768):\n    for x in range(1024):\n        data[y, x] = math.sin(x*y/40.5)  # 매우 비효율적입니다!\n작동은 하지만 순수한 파이썬 코드로 반복문이 진행되기 때문에 아주 비효율적입니다. 이 알고리즘을 벡터화해 보죠. 먼저 넘파이 meshgrid 함수로 좌표 벡터를 사용해 행렬을 만듭니다.\nx_coords = np.arange(0, 1024)  # [0, 1, 2, ..., 1023]\ny_coords = np.arange(0, 768)   # [0, 1, 2, ..., 767]\nX, Y = np.meshgrid(x_coords, y_coords)\nX\narray([[   0,    1,    2, ..., 1021, 1022, 1023],\n       [   0,    1,    2, ..., 1021, 1022, 1023],\n       [   0,    1,    2, ..., 1021, 1022, 1023],\n       ...,\n       [   0,    1,    2, ..., 1021, 1022, 1023],\n       [   0,    1,    2, ..., 1021, 1022, 1023],\n       [   0,    1,    2, ..., 1021, 1022, 1023]])\nY\narray([[  0,   0,   0, ...,   0,   0,   0],\n       [  1,   1,   1, ...,   1,   1,   1],\n       [  2,   2,   2, ...,   2,   2,   2],\n       ...,\n       [765, 765, 765, ..., 765, 765, 765],\n       [766, 766, 766, ..., 766, 766, 766],\n       [767, 767, 767, ..., 767, 767, 767]])\n여기서 볼 수 있듯이 X와 Y 모두 768x1024 배열입니다. X에 있는 모든 값은 수평 좌표에 해당합니다. Y에 있는 모든 값은 수직 좌표에 해당합니다.\n이제 간단히 배열 연산을 사용해 계산할 수 있습니다:\ndata = np.sin(X*Y/40.5)\n맷플롯립의 imshow 함수를 사용해 이 데이터를 그려보죠(matplotlib tutorial을 참조하세요).\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfig = plt.figure(1, figsize=(7, 6))\nplt.imshow(data, cmap=cm.hot)\nplt.show()\n\n\n\npng"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#저장과-로딩",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#저장과-로딩",
    "title": "Data_Mining CH1",
    "section": "1.16 저장과 로딩",
    "text": "1.16 저장과 로딩\n넘파이는 ndarray를 바이너리 또는 텍스트 포맷으로 손쉽게 저장하고 로드할 수 있습니다."
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#바이너리-.npy-포맷",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#바이너리-.npy-포맷",
    "title": "Data_Mining CH1",
    "section": "1.17 바이너리 .npy 포맷",
    "text": "1.17 바이너리 .npy 포맷\n랜덤 배열을 만들고 저장해 보죠.\na = np.random.rand(2,3)\na\narray([[0.82824112, 0.34529149, 0.09610369],\n       [0.81324126, 0.16441513, 0.64848436]])\nnp.save(\"my_array\", a)\n끝입니다! 파일 이름의 확장자를 지정하지 않았기 때문에 넘파이는 자동으로 .npy를 붙입니다. 파일 내용을 확인해 보겠습니다:\nwith open(\"my_array.npy\", \"rb\") as f:\n    content = f.read()\n\ncontent\nb\"\\x93NUMPY\\x01\\x00v\\x00{'descr': '<f8', 'fortran_order': False, 'shape': (2, 3), }                                                          \\n\\xfe\\x8b\\xd2\\x83\\xf3\\x80\\xea?$\\xefIwA\\x19\\xd6?(\\xca\\x9d]@\\x9a\\xb8?\\xf8\\xd9\\xcf\\x87\\x12\\x06\\xea?\\xfcF\\xd3\\x14\\x8e\\x0b\\xc5?\\xc8)dCb\\xc0\\xe4?\"\n이 파일을 넘파이 배열로 로드하려면 load 함수를 사용합니다:\na_loaded = np.load(\"my_array.npy\")\na_loaded\narray([[0.82824112, 0.34529149, 0.09610369],\n       [0.81324126, 0.16441513, 0.64848436]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#텍스트-포맷",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#텍스트-포맷",
    "title": "Data_Mining CH1",
    "section": "1.18 텍스트 포맷",
    "text": "1.18 텍스트 포맷\n배열을 텍스트 포맷으로 저장해 보죠:\nnp.savetxt(\"my_array.csv\", a)\n파일 내용을 확인해 보겠습니다:\nwith open(\"my_array.csv\", \"rt\") as f:\n    print(f.read())\n8.282411169678878249e-01 3.452914872102488264e-01 9.610369000964935626e-02\n8.132412579132610730e-01 1.644151307760280956e-01 6.484843555675121607e-01\n이 파일은 탭으로 구분된 CSV 파일입니다. 다른 구분자를 지정할 수도 있습니다:\nnp.savetxt(\"my_array.csv\", a, delimiter=\",\")\n이 파일을 로드하려면 loadtxt 함수를 사용합니다:\na_loaded = np.loadtxt(\"my_array.csv\", delimiter=\",\")\na_loaded\narray([[0.82824112, 0.34529149, 0.09610369],\n       [0.81324126, 0.16441513, 0.64848436]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#압축된-.npz-포맷",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#압축된-.npz-포맷",
    "title": "Data_Mining CH1",
    "section": "1.19 압축된 .npz 포맷",
    "text": "1.19 압축된 .npz 포맷\n여러 개의 배열을 압축된 한 파일로 저장하는 것도 가능합니다:\nb = np.arange(24, dtype=np.uint8).reshape(2, 3, 4)\nb\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]], dtype=uint8)\nnp.savez(\"my_arrays\", my_a=a, my_b=b)\n파일 내용을 확인해 보죠. .npz 파일 확장자가 자동으로 추가되었습니다.\nwith open(\"my_arrays.npz\", \"rb\") as f:\n    content = f.read()\n\nrepr(content)[:180] + \"[...]\"\n'b\"PK\\\\x03\\\\x04\\\\x14\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00!\\\\x00\\\\xfe\\\\xb3\\\\xe3\\\\xa4\\\\xb0\\\\x00\\\\x00\\\\x00\\\\xb0\\\\x00\\\\x00\\\\x00\\\\x08\\\\x00\\\\x14\\\\x00my_a.npy\\\\x01\\\\x00\\\\x10\\\\x00\\\\xb0\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\xb0\\\\x00\\\\x0[...]'\n다음과 같이 이 파일을 로드할 수 있습니다:\nmy_arrays = np.load(\"my_arrays.npz\")\nmy_arrays\n<numpy.lib.npyio.NpzFile at 0x19163d4cc70>\n게으른 로딩을 수행하는 딕셔너리와 유사한 객체입니다:\nmy_arrays.keys()\nKeysView(<numpy.lib.npyio.NpzFile object at 0x0000019163D4CC70>)\nmy_arrays[\"my_a\"]\narray([[0.82824112, 0.34529149, 0.09610369],\n       [0.81324126, 0.16441513, 0.64848436]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부.html#그-다음은",
    "href": "posts/Data_Mining_Numpy/numpy 공부.html#그-다음은",
    "title": "Data_Mining CH1",
    "section": "1.20 그 다음은?",
    "text": "1.20 그 다음은?\n넘파이 기본 요소를 모두 배웠지만 훨씬 더 많은 기능이 있습니다. 이를 배우는 가장 좋은 방법은 넘파이를 직접 실습해 보고 훌륭한 넘파이 문서에서 필요한 함수와 기능을 찾아 보세요."
  },
  {
    "objectID": "posts/Data_Visualize_Ch1/데이터시각화 2주차 ggplot2 기초.html",
    "href": "posts/Data_Visualize_Ch1/데이터시각화 2주차 ggplot2 기초.html",
    "title": "Data_Visualization CH1",
    "section": "",
    "text": "HTML파일로 보기\nggplot2 Basic"
  },
  {
    "objectID": "posts/Data_Visualize_Ch1/데이터시각화 2주차 ggplot2 기초.html#그래프-패키지packages",
    "href": "posts/Data_Visualize_Ch1/데이터시각화 2주차 ggplot2 기초.html#그래프-패키지packages",
    "title": "Data_Visualization CH1",
    "section": "1 그래프 패키지(Packages)",
    "text": "1 그래프 패키지(Packages)\n\n1.0.0.1 [ggplot2] package : R에서 사용할 수 있는 대표적인 그래픽 도구\n\n그래프 결과를 그림 파일로 출력(정적)\nTop 50 ggplot2 Visualizations\n\n참고 http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html\n\n\n\n\n1.0.0.2 [plotly] package - 대화형 그래프 라이브러리로 Python, R, JavaScript 라이브러리를 제공\n\n그래프 결과를 HTML 파일로 출력(동적)\n\n값 확인, 확대/축소, 이동, 필터 등의 기능을 제공하여 자료 탐색 가능\n\nPlotly R Open Source Graphing Library\n\n참고 https://plotly.com/r/ - Plotly 패키지에 ggplot2 패키지가 포함되어 있음"
  },
  {
    "objectID": "posts/Data_Visualize_Ch1/데이터시각화 2주차 ggplot2 기초.html#ggplot2-package",
    "href": "posts/Data_Visualize_Ch1/데이터시각화 2주차 ggplot2 기초.html#ggplot2-package",
    "title": "Data_Visualization CH1",
    "section": "2 [ggplot2] package",
    "text": "2 [ggplot2] package\n\n2.0.0.1 기본 3요소\n\ndata: 시각화 자료 - 데이터프레임\ngeom: 시각화 종류\n\n점: geom_point()\n선: geom_line()\n막대: eom_bar()\n\naesthetics: 시각화 특성 aes()\n\n위치: x, y\n크기: size\n색상: col, fill\n농도: alpha\n\n\n\n\n2.0.0.2 레이어 추가\n\n‘+’ 연산자를 사용하여 레이어를 추가하는 형식으로 그래프를 추가하거나 수정함\n\n\n\n2.0.0.3 ggplot() 함수\n\n그래프를 그리는 기본 함수로 다양한 함수를 추가(+)하여 정교한 그래프를 생성\n\n\n\n2.0.0.4 제목과 축의 이름 설정\n\nggtitle(), xlab(), ylab() 함수\nlabs() 함수\n\n\n\n2.0.0.5 qplot() 함수\n\nQuick plot: 기본 plot() 함수에 익숙한 사용자를 위한 빠른 그래프 생성"
  },
  {
    "objectID": "posts/Data_Visualize_Ch1/데이터시각화 2주차 ggplot2 기초.html#ggplot2-실습",
    "href": "posts/Data_Visualize_Ch1/데이터시각화 2주차 ggplot2 기초.html#ggplot2-실습",
    "title": "Data_Visualization CH1",
    "section": "3 ggplot2 실습",
    "text": "3 ggplot2 실습\n\n### 패키지 설치, 라이브러리 실행\n\n#install.packages(\"ggplot2\")\n#install.packages(\"dplyr\")\nlibrary(ggplot2)\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\n\n\n### [mpg]데이터셋 불러오기, 파악\n\ndata_raw <- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/ggplot2/mpg.csv\")\n\n# 데이터 차원 파악\ndata_raw %>% dim()\n#> [1] 234  12\n# 데이터 앞부분 출력\ndata_raw %>% head()\n#>   X manufacturer model displ year cyl      trans drv cty hwy fl   class\n#> 1 1         audi    a4   1.8 1999   4   auto(l5)   f  18  29  p compact\n#> 2 2         audi    a4   1.8 1999   4 manual(m5)   f  21  29  p compact\n#> 3 3         audi    a4   2.0 2008   4 manual(m6)   f  20  31  p compact\n#> 4 4         audi    a4   2.0 2008   4   auto(av)   f  21  30  p compact\n#> 5 5         audi    a4   2.8 1999   6   auto(l5)   f  16  26  p compact\n#> 6 6         audi    a4   2.8 1999   6 manual(m5)   f  18  26  p compact\n# 데이터 요약 정보\ndata_raw %>% summary()\n#>        X          manufacturer          model               displ      \n#>  Min.   :  1.00   Length:234         Length:234         Min.   :1.600  \n#>  1st Qu.: 59.25   Class :character   Class :character   1st Qu.:2.400  \n#>  Median :117.50   Mode  :character   Mode  :character   Median :3.300  \n#>  Mean   :117.50                                         Mean   :3.472  \n#>  3rd Qu.:175.75                                         3rd Qu.:4.600  \n#>  Max.   :234.00                                         Max.   :7.000  \n#>       year           cyl           trans               drv           \n#>  Min.   :1999   Min.   :4.000   Length:234         Length:234        \n#>  1st Qu.:1999   1st Qu.:4.000   Class :character   Class :character  \n#>  Median :2004   Median :6.000   Mode  :character   Mode  :character  \n#>  Mean   :2004   Mean   :5.889                                        \n#>  3rd Qu.:2008   3rd Qu.:8.000                                        \n#>  Max.   :2008   Max.   :8.000                                        \n#>       cty             hwy             fl               class          \n#>  Min.   : 9.00   Min.   :12.00   Length:234         Length:234        \n#>  1st Qu.:14.00   1st Qu.:18.00   Class :character   Class :character  \n#>  Median :17.00   Median :24.00   Mode  :character   Mode  :character  \n#>  Mean   :16.86   Mean   :23.44                                        \n#>  3rd Qu.:19.00   3rd Qu.:27.00                                        \n#>  Max.   :35.00   Max.   :44.00\n\n\n### 첫 번째 컬럼 제거 (X 컬럼 제거)\n\ndata_use <- data_raw %>% select(-1)\ndata_use %>% head()\n#>   manufacturer model displ year cyl      trans drv cty hwy fl   class\n#> 1         audi    a4   1.8 1999   4   auto(l5)   f  18  29  p compact\n#> 2         audi    a4   1.8 1999   4 manual(m5)   f  21  29  p compact\n#> 3         audi    a4   2.0 2008   4 manual(m6)   f  20  31  p compact\n#> 4         audi    a4   2.0 2008   4   auto(av)   f  21  30  p compact\n#> 5         audi    a4   2.8 1999   6   auto(l5)   f  16  26  p compact\n#> 6         audi    a4   2.8 1999   6 manual(m5)   f  18  26  p compact\n\n\n### ggplot() : 시각화 자료 + 시각화 특성\n\n# x축, y축 지정\nggplot(data_use, aes(x=displ, y=hwy))\n\n\n\n\n\n### ggplot: 시각화 자료 + 시각화 특성 + 시각화 종류\n\n# x=displ, y=hwy 값을 point로 그리기\n# 첫 번째 방법\nggplot(data_use, aes(x=displ, y=hwy)) + geom_point()\n\n\n\n# 두 번째 방법\nggplot(data_use) + geom_point(aes(x=displ, y=hwy))\n\n\n\n# 세 번째 방법\nggplot() + geom_point(aes(x=displ, y=hwy), data=data_use)\n\n\n\n\n\n### Smooth line & SE\n\n# 스무스한 선으로 그리기\nggplot(data_use, aes(x=displ, y=hwy)) + geom_smooth()\n#> `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n# point 추가\nggplot(data_use, aes(x=displ,y=hwy)) + geom_smooth() + geom_point()\n#> `geom_smooth()` using method = 'loess' and formula = 'y ~ x'"
  },
  {
    "objectID": "posts/Data_Visualize_Ch2/CH2.html",
    "href": "posts/Data_Visualize_Ch2/CH2.html",
    "title": "Data_Visualization CH2",
    "section": "",
    "text": "HTML파일로 보기\nFigure 2.3, Figure 2.4"
  },
  {
    "objectID": "posts/Data_Visualize_Ch2/CH2.html#데이터-시각화-실습-계절성-그래프-figure-2.3-figure-2.4",
    "href": "posts/Data_Visualize_Ch2/CH2.html#데이터-시각화-실습-계절성-그래프-figure-2.3-figure-2.4",
    "title": "Data_Visualization CH2",
    "section": "1 데이터 시각화 실습 : 계절성 그래프 Figure 2.3, Figure 2.4",
    "text": "1 데이터 시각화 실습 : 계절성 그래프 Figure 2.3, Figure 2.4\n\n1.1 패키지 불러오기\n\nlibrary(ggplot2)\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\n\n\n\n1.2 파일 불러오기\n\nncdc_normals.csv (날짜별 온도 등 데이터셋)\n\n\nncdc_normals <- read.csv('C:/Users/seong taek/Desktop/3-1 DataVisualize/data_visualize/ncdc_normals.csv')\n\n\n\n1.3 불러온 데이터셋 파악\n\n### 차원 파악\nncdc_normals %>% dim()\n#> [1] 2745366       6\n\n### 앞부분 훑어보기\nncdc_normals %>% head()\n#>    station_id month day temperature flag       date\n#> 1 AQW00061705     1   1        82.4    C 0000-01-01\n#> 2 AQW00061705     1   2        82.4    C 0000-01-02\n#> 3 AQW00061705     1   3        82.4    C 0000-01-03\n#> 4 AQW00061705     1   4        82.4    C 0000-01-04\n#> 5 AQW00061705     1   5        82.4    C 0000-01-05\n#> 6 AQW00061705     1   6        82.4    C 0000-01-06\n\n### 통계 요약 정보\nncdc_normals %>% summary()\n#>   station_id            month             day         temperature    \n#>  Length:2745366     Min.   : 1.000   Min.   : 1.00   Min.   :-21.80  \n#>  Class :character   1st Qu.: 4.000   1st Qu.: 8.00   1st Qu.: 39.70  \n#>  Mode  :character   Median : 7.000   Median :16.00   Median : 54.60  \n#>                     Mean   : 6.514   Mean   :15.76   Mean   : 53.17  \n#>                     3rd Qu.:10.000   3rd Qu.:23.00   3rd Qu.: 68.10  \n#>                     Max.   :12.000   Max.   :31.00   Max.   :103.20  \n#>      flag               date          \n#>  Length:2745366     Length:2745366    \n#>  Class :character   Class :character  \n#>  Mode  :character   Mode  :character  \n#>                                       \n#>                                       \n#> \n\n### 각 컬럼 클래스(타입) 확인\nncdc_normals %>% sapply(class)\n#>  station_id       month         day temperature        flag        date \n#> \"character\"   \"integer\"   \"integer\"   \"numeric\" \"character\" \"character\"\n\n### 각 컬럼 자료형 확인\nncdc_normals %>% sapply(typeof)\n#>  station_id       month         day temperature        flag        date \n#> \"character\"   \"integer\"   \"integer\"    \"double\" \"character\" \"character\"\n\n\n\n1.4 전처리\n\n### character → date 형식 변환\n\nncdc_normals$date <- ncdc_normals$date%>% as.Date(\"%Y-%m-%d\")\n\nncdc_normals %>% sapply(class)\n#>  station_id       month         day temperature        flag        date \n#> \"character\"   \"integer\"   \"integer\"   \"numeric\" \"character\"      \"Date\"\n\nncdc_normals %>% sapply(typeof)\n#>  station_id       month         day temperature        flag        date \n#> \"character\"   \"integer\"   \"integer\"    \"double\" \"character\"    \"double\"\n\n\n### station_id 종류 개수\n\nncdc_normals$station_id %>% unique() %>% length()\n#> [1] 7501\n\n\n###  station_id 선정 후, location(컬럼 이름) 지정한 data.frame 생성\n\nstation_loc <- data.frame(station_id = c(\"USW00014819\",\"USC00042319\",\"USW00093107\",\"USW00012918\"),\n                          location = c(\"Chicago\",\"Death Valley\",\"San Diego\",\"Houston\"))\nstation_loc\n#>    station_id     location\n#> 1 USW00014819      Chicago\n#> 2 USC00042319 Death Valley\n#> 3 USW00093107    San Diego\n#> 4 USW00012918      Houston\n\n\n### station_id로 ncdc_normals와 station_loc 두 컬럼을 `inner_join`\n\ntemps_long <- ncdc_normals %>% inner_join(station_loc, by=\"station_id\")\n\ntemps_long %>% head()\n#>    station_id month day temperature flag       date     location\n#> 1 USC00042319     1   1        51.0    S 0000-01-01 Death Valley\n#> 2 USC00042319     1   2        51.2    S 0000-01-02 Death Valley\n#> 3 USC00042319     1   3        51.3    S 0000-01-03 Death Valley\n#> 4 USC00042319     1   4        51.4    S 0000-01-04 Death Valley\n#> 5 USC00042319     1   5        51.6    S 0000-01-05 Death Valley\n#> 6 USC00042319     1   6        51.7    S 0000-01-06 Death Valley\n\ntemps_long %>% dim()\n#> [1] 1464    7\n\ntemps_long$date %>% class()\n#> [1] \"Date\"\n\n\n\n1.5 x축에 표시할 눈금 생성\n\n### x축 범위 설정을 위해 date 범위 파악\ntemps_long$date %>% head()\n#> [1] \"0000-01-01\" \"0000-01-02\" \"0000-01-03\" \"0000-01-04\" \"0000-01-05\"\n#> [6] \"0000-01-06\"\ntemps_long$date %>% tail()\n#> [1] \"0000-12-26\" \"0000-12-27\" \"0000-12-28\" \"0000-12-29\" \"0000-12-30\"\n#> [6] \"0000-12-31\"\n\n### 범위 설정\ndate_s <- '0000-01-01' %>% as.Date('%Y-%m-%d')\ndate_e <- '0001-01-01' %>% as.Date('%Y-%m-%d')\n\n### data_s ~ date_e까지 3개월 단위로 나누기\nbreak_date <- seq(date_s, date_e, by = '3 month')\n\nbreak_date\n#> [1] \"0000-01-01\" \"0000-04-01\" \"0000-07-01\" \"0000-10-01\" \"0001-01-01\"\n\n\n\n1.6 Fiqure 2.3 ggplot + 축 설정\n\n사용 데이터셋 : temps_long\nx=date, y=temperature\ncolor : location별\nscale_x_date\n\n이름 : month\n간격 : break_date (3개월)\n간격 라벨 (Jan ~ Jan)\n\nscale_y_continuous\n\n이름 : temp\n범위 : 0 ~ 110\n\nlabs : 제목, 부제목\n테마 : 밝게\n\n\nggplot(temps_long, aes(x=date, y=temperature, color=location)) +\n  geom_line() +\n  scale_x_date(name = 'month',\n               breaks = break_date,\n               labels = c('Jan','Apr','Jul','Oct','Jan')) +\n  scale_y_continuous(name = 'temp', limits = c(0,110)) + # continuous 연속형 (온도)\n  # ylab('Temp') +\n  labs(title = 'Fig. 2.3', subtitle = 'Daily temperature normals') +\n  theme_light()\n\n\n\n\n\n\n1.7 Figure 2.4 그래프 데이터셋 전처리\n\ntemps_long %>% head()\n#>    station_id month day temperature flag       date     location\n#> 1 USC00042319     1   1        51.0    S 0000-01-01 Death Valley\n#> 2 USC00042319     1   2        51.2    S 0000-01-02 Death Valley\n#> 3 USC00042319     1   3        51.3    S 0000-01-03 Death Valley\n#> 4 USC00042319     1   4        51.4    S 0000-01-04 Death Valley\n#> 5 USC00042319     1   5        51.6    S 0000-01-05 Death Valley\n#> 6 USC00042319     1   6        51.7    S 0000-01-06 Death Valley\n\ntemps_long %>% names()\n#> [1] \"station_id\"  \"month\"       \"day\"         \"temperature\" \"flag\"       \n#> [6] \"date\"        \"location\"\n\nmean_temps <- temps_long %>%              \n  group_by(location, month) %>%           # location, month로 그룹화 \n  summarise(mean = mean(temperature)) %>% # 그룹화된 데이터의 집계값 요약\n  ungroup() %>%                           # 그룹화를 해제하여 일반적인 데이터 프레임 형태로 사용\n  mutate(month = factor(month %>%         # month값을 factor 형태로 수정해서 원하는 levels 지정가능\n                          paste(), levels = 1:12 %>% paste())) \n#> `summarise()` has grouped output by 'location'. You can override using the\n#> `.groups` argument.\n\nmean_temps\n#> # A tibble: 48 × 3\n#>    location month  mean\n#>    <chr>    <fct> <dbl>\n#>  1 Chicago  1      24.8\n#>  2 Chicago  2      28.9\n#>  3 Chicago  3      38.8\n#>  4 Chicago  4      50.4\n#>  5 Chicago  5      60.9\n#>  6 Chicago  6      71.0\n#>  7 Chicago  7      75.8\n#>  8 Chicago  8      74.1\n#>  9 Chicago  9      66.4\n#> 10 Chicago  10     54.3\n#> # … with 38 more rows\n\n\n\n1.8 Figure 2.4 ggplot + geom_tile + fill color\n\n사용 데이터셋 : mean_temps\nx=month, y=location\nfill : mean\ngeom_tile : 넓이, 높이 지정\nscale_fill_viridis_c\n\n이름 : temperature\n옵션 : B (밝기)\nbegin, end : 색상의 시작,끝\n\ncoord_fixed\n\nexpand = F : 공백 제거\n\nylab(NULL) : y축 이름 제거\n\n\nggplot(mean_temps, aes(x = month, y = location, fill = mean)) +\n  geom_tile(width = .95, height = 0.95) +\n  scale_fill_viridis_c(option = 'B', begin = 0.15, end =  0.98,\n                       name = 'temperature') +\n  coord_fixed(expand = FALSE) +\n  ylab(NULL) #ylab('')"
  },
  {
    "objectID": "posts/Data_Visualize_Ch2/CH2.html#예제-2021년-기상청-자료-figure-2.3-figure-2.4",
    "href": "posts/Data_Visualize_Ch2/CH2.html#예제-2021년-기상청-자료-figure-2.3-figure-2.4",
    "title": "Data_Visualization CH2",
    "section": "2 예제 : 2021년 기상청 자료 Figure 2.3, Figure 2.4",
    "text": "2 예제 : 2021년 기상청 자료 Figure 2.3, Figure 2.4\n\n2.1 csv파일 불러오기, 데이터 파악\n\ndata_2021 <- read.csv('C:/Users/seong taek/Desktop/3-1 DataVisualize/data_visualize/OBS_ASOS_DD_20220308125952.csv', fileEncoding = 'cp949')\n\ndata_2021 %>% dim()\n#> [1] 1460    6\n\ndata_2021 %>% head()\n#>   지점 지점명       일시 평균기온..C. 최저기온..C. 최고기온..C.\n#> 1  108   서울 2021-01-01         -4.2         -9.8          1.6\n#> 2  108   서울 2021-01-02         -5.0         -8.4         -1.4\n#> 3  108   서울 2021-01-03         -5.6         -9.1         -2.0\n#> 4  108   서울 2021-01-04         -3.5         -8.4          0.3\n#> 5  108   서울 2021-01-05         -5.5         -9.9         -2.1\n#> 6  108   서울 2021-01-06         -7.4        -12.0         -1.9\n\ndata_2021 %>% sapply(class)\n#>         지점       지점명         일시 평균기온..C. 최저기온..C. 최고기온..C. \n#>    \"integer\"  \"character\"  \"character\"    \"numeric\"    \"numeric\"    \"numeric\"\n\n\n\n2.2 일시를 character → date형식으로 변환\n\ndata_2021$일시 <- data_2021$일시 %>% as.Date('%Y-%m-%d')\n\ndata_2021 %>% sapply(class)\n#>         지점       지점명         일시 평균기온..C. 최저기온..C. 최고기온..C. \n#>    \"integer\"  \"character\"       \"Date\"    \"numeric\"    \"numeric\"    \"numeric\"\n\n\n\n2.3 기초통계량 파악\n\ndata_2021 %>% summary()\n#>       지점          지점명               일시             평균기온..C.   \n#>  Min.   :108.0   Length:1460        Min.   :2021-01-01   Min.   :-14.90  \n#>  1st Qu.:126.8   Class :character   1st Qu.:2021-04-02   1st Qu.:  7.90  \n#>  Median :158.5   Mode  :character   Median :2021-07-02   Median : 15.00  \n#>  Mean   :166.0                      Mean   :2021-07-02   Mean   : 14.77  \n#>  3rd Qu.:197.8                      3rd Qu.:2021-10-01   3rd Qu.: 23.10  \n#>  Max.   :239.0                      Max.   :2021-12-31   Max.   : 31.70  \n#>   최저기온..C.     최고기온..C.   \n#>  Min.   :-19.10   Min.   :-10.70  \n#>  1st Qu.:  3.10   1st Qu.: 13.18  \n#>  Median : 11.10   Median : 20.15  \n#>  Mean   : 10.69   Mean   : 19.56  \n#>  3rd Qu.: 19.60   3rd Qu.: 27.70  \n#>  Max.   : 28.10   Max.   : 36.50\n\n\n\n2.4 x축에 표시할 눈금\n\ndata_2021$일시 %>% head()\n#> [1] \"2021-01-01\" \"2021-01-02\" \"2021-01-03\" \"2021-01-04\" \"2021-01-05\"\n#> [6] \"2021-01-06\"\n\ndata_2021$일시 %>% tail()\n#> [1] \"2021-12-26\" \"2021-12-27\" \"2021-12-28\" \"2021-12-29\" \"2021-12-30\"\n#> [6] \"2021-12-31\"\n\ndate_s <- '2021-01-01' %>% as.Date('%Y-%m-%d')\ndate_e <- '2022-01-01' %>% as.Date('%Y-%m-%d')\n\nbreak_date <- seq.Date(date_s, date_e, by='3 month')\nbreak_date\n#> [1] \"2021-01-01\" \"2021-04-01\" \"2021-07-01\" \"2021-10-01\" \"2022-01-01\"\n\n### 월만 문자 값으로 뽑아내기\ndate_lab <- format(break_date, '%B')\ndate_lab\n#> [1] \"1월\"  \"4월\"  \"7월\"  \"10월\" \"1월\"\n\n\n\n2.5 Figure 2.3 ggplot + 축 설정\n\n사용 데이터셋 : data_2021\nx=일시, y=평균기온..C.\ncolor : 지점명 별\nscale_x_date\n\n이름 : 월\n간격 : break_date (3개월)\n간격 라벨 (1월 ~ 1월)\n\nscale_y_continuous\n\n이름 : 평균기온\n\nlabs : 제목, 부제목\n테마 : 밝게\n\n\n### 축 값 지정하기 전 컬럼이름 파악\ndata_2021 %>% names()\n#> [1] \"지점\"         \"지점명\"       \"일시\"         \"평균기온..C.\" \"최저기온..C.\"\n#> [6] \"최고기온..C.\"\n\nggplot(data_2021, aes(x=일시, y=평균기온..C., color=지점명)) +\n  geom_smooth(linewidth = 0.9,se=F,span=0.1) + # linewidth (선 두께), geom_line → smooth 가능\n  scale_x_date(name = '월',                  # se : 테두리, span : 스무스 정도\n               breaks = break_date,\n               labels = date_lab) +\n  scale_y_continuous(name = '평균기온') +\n  labs(title = '2021년 평균온도', subtitle = '4개의 주요 도시') +\n  theme_light()\n#> `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n2.6 Figure 2.4 그래프 데이터셋 전처리\n\ndata_2021 %>% names()\n#> [1] \"지점\"         \"지점명\"       \"일시\"         \"평균기온..C.\" \"최저기온..C.\"\n#> [6] \"최고기온..C.\"\n\ndata_2021 %>% head()\n#>   지점 지점명       일시 평균기온..C. 최저기온..C. 최고기온..C.\n#> 1  108   서울 2021-01-01         -4.2         -9.8          1.6\n#> 2  108   서울 2021-01-02         -5.0         -8.4         -1.4\n#> 3  108   서울 2021-01-03         -5.6         -9.1         -2.0\n#> 4  108   서울 2021-01-04         -3.5         -8.4          0.3\n#> 5  108   서울 2021-01-05         -5.5         -9.9         -2.1\n#> 6  108   서울 2021-01-06         -7.4        -12.0         -1.9\n\ndata_2021_month <- data_2021 %>%\n  mutate(month = format(일시, '%B')) %>%    # 일시에서 월만 뽑아낸 month 컬럼 생성\n  group_by(지점명, month) %>%               # 지점명, month로 그룹화\n  summarise(mean = mean(평균기온..C.)) %>%  # 그룹화된 데이터의 집계값 요약\n  ungroup() %>%                            # 그룹화를 해제하여 일반적인 데이터 프레임 형태로 사용\n  mutate(month = factor(month,             # month값을 factor 형태로 수정해서 원하는 levels 지정가능\n                        levels = paste(1:12, '월',sep = ''))) # sep='' : 간격없이 붙이기\n#> `summarise()` has grouped output by '지점명'. You can override using the\n#> `.groups` argument.\n\ndata_2021_month\n#> # A tibble: 48 × 3\n#>    지점명 month   mean\n#>    <chr>  <fct>  <dbl>\n#>  1 대전   10월  15.7  \n#>  2 대전   11월   8.91 \n#>  3 대전   12월   1.89 \n#>  4 대전   1월   -0.984\n#>  5 대전   2월    3.99 \n#>  6 대전   3월    9.61 \n#>  7 대전   4월   14.7  \n#>  8 대전   5월   17.7  \n#>  9 대전   6월   23.5  \n#> 10 대전   7월   27.8  \n#> # … with 38 more rows\n\n\n\n2.7 Figure 2.4 ggplot + geom_tile + fill color\n\n사용 데이터셋 : data_2021_month\nx=month, y=지점명\nfill : mean(평균온도)\ngeom_tile : 넓이, 높이 지정\nscale_fill_viridis_c\n\n이름 : temperature\n옵션 : B (밝기)\nbegin, end : 색상의 시작,끝\n\ncoord_fixed\n\nexpand = F : 공백 제거\n\nylab(NULL) : y축 이름 제거\n\n\nggplot(data_2021_month, aes(x = month, y = 지점명, fill = mean)) +\n  geom_tile(width = 0.95, height = 0.95) +\n  scale_fill_viridis_c(option = 'B', begin = 0.15, end =  0.98,\n                       name = 'temperature') +\n  coord_fixed(expand = FALSE) +\n  ylab(NULL) # ylab('')"
  },
  {
    "objectID": "posts/Data_Visualize_Ch2/CH2.html#과제-2022년-기상청-자료-figure-2.3-figure-2.4",
    "href": "posts/Data_Visualize_Ch2/CH2.html#과제-2022년-기상청-자료-figure-2.3-figure-2.4",
    "title": "Data_Visualization CH2",
    "section": "3 과제 : 2022년 기상청 자료 Figure 2.3, Figure 2.4",
    "text": "3 과제 : 2022년 기상청 자료 Figure 2.3, Figure 2.4\n\n3.1 csv파일 불러오기, 데이터 파악\n\ndata_2022 <- read.csv('C:/Users/seong taek/Desktop/3-1 DataVisualize/data_visualize/OBS_ASOS_DD_20230322080932.csv', fileEncoding = 'cp949')\n\ndata_2022 %>% dim()\n#> [1] 2555    6\n\ndata_2022 %>% head()\n#>   지점 지점명       일시 평균기온..C. 최저기온..C. 최고기온..C.\n#> 1  108   서울 2022-01-01         -4.3        -10.2          2.3\n#> 2  108   서울 2022-01-02         -1.3         -5.2          3.0\n#> 3  108   서울 2022-01-03         -1.9         -8.0          2.5\n#> 4  108   서울 2022-01-04         -2.5         -5.6          1.0\n#> 5  108   서울 2022-01-05         -2.8         -7.8          1.9\n#> 6  108   서울 2022-01-06         -2.2         -5.9          3.3\n\ndata_2022 %>% sapply(class)\n#>         지점       지점명         일시 평균기온..C. 최저기온..C. 최고기온..C. \n#>    \"integer\"  \"character\"  \"character\"    \"numeric\"    \"numeric\"    \"numeric\"\n\n### 결측값 있는지 확인\nsum(is.na(data_2022))\n#> [1] 4\n\n### 결측값 제거\ndata_2022 <- data_2022 %>% na.omit()\n\n\n\n3.2 일시를 character → date형식으로 변환\n\ndata_2022$일시 <- data_2022$일시 %>% as.Date('%Y-%m-%d')\n\ndata_2022 %>% sapply(class)\n#>         지점       지점명         일시 평균기온..C. 최저기온..C. 최고기온..C. \n#>    \"integer\"  \"character\"       \"Date\"    \"numeric\"    \"numeric\"    \"numeric\"\n\n\n\n3.3 기초통계량 파악\n\ndata_2022 %>% summary()\n#>       지점          지점명               일시             평균기온..C.   \n#>  Min.   :108.0   Length:2552        Min.   :2022-01-01   Min.   :-11.80  \n#>  1st Qu.:133.0   Class :character   1st Qu.:2022-04-02   1st Qu.:  8.20  \n#>  Median :185.0   Mode  :character   Median :2022-07-02   Median : 16.40  \n#>  Mean   :175.1                      Mean   :2022-07-01   Mean   : 15.26  \n#>  3rd Qu.:189.0                      3rd Qu.:2022-10-01   3rd Qu.: 23.00  \n#>  Max.   :239.0                      Max.   :2022-12-31   Max.   : 32.20  \n#>   최저기온..C.     최고기온..C.  \n#>  Min.   :-13.80   Min.   :-8.60  \n#>  1st Qu.:  4.20   1st Qu.:12.30  \n#>  Median : 12.60   Median :20.70  \n#>  Mean   : 11.62   Mean   :19.49  \n#>  3rd Qu.: 19.80   3rd Qu.:27.20  \n#>  Max.   : 28.90   Max.   :37.50\n\n\n\n3.4 x축에 표시할 눈금\n\ndata_2022$일시 %>% head()\n#> [1] \"2022-01-01\" \"2022-01-02\" \"2022-01-03\" \"2022-01-04\" \"2022-01-05\"\n#> [6] \"2022-01-06\"\n\ndata_2022$일시 %>% tail()\n#> [1] \"2022-12-26\" \"2022-12-27\" \"2022-12-28\" \"2022-12-29\" \"2022-12-30\"\n#> [6] \"2022-12-31\"\n\ndate_s <- '2022-01-01' %>% as.Date('%Y-%m-%d')\ndate_e <- '2023-01-01' %>% as.Date('%Y-%m-%d')\n\nbreak_date <- seq.Date(date_s, date_e, by='3 month')\nbreak_date\n#> [1] \"2022-01-01\" \"2022-04-01\" \"2022-07-01\" \"2022-10-01\" \"2023-01-01\"\n\n### 월만 문자 값으로 뽑아내기\ndate_lab <- format(break_date, '%B')\ndate_lab\n#> [1] \"1월\"  \"4월\"  \"7월\"  \"10월\" \"1월\"\n\n\n\n3.5 Figure 2.3 ggplot + 축 설정\n\n사용 데이터셋 : data_2022\nx=일시, y=평균기온..C.\ncolor : 지점명 별\nscale_x_date\n\n이름 : 월\n간격 : break_date (3개월)\n간격 라벨 (1월 ~ 1월)\n\nscale_y_continuous\n\n이름 : 평균기온\n\nlabs : 제목, 부제목\n테마 : 밝게\n\n\n### 축 값 지정하기 전 컬럼이름 파악\ndata_2022 %>% names()\n#> [1] \"지점\"         \"지점명\"       \"일시\"         \"평균기온..C.\" \"최저기온..C.\"\n#> [6] \"최고기온..C.\"\n\nggplot(data_2022, aes(x=일시, y=평균기온..C., color=지점명)) +\n  geom_smooth(linewidth = 1.1,se=F,span=0.08) + # linewidth (선 두께), geom_line → smooth 가능\n  scale_x_date(name = '월',                     # se : 테두리, span : 스무스 정도\n               breaks = break_date,\n               labels = date_lab) +\n  scale_y_continuous(name = '평균기온') +\n  labs(title = '2022년 평균온도', subtitle = '7개 지점 관측') +\n  theme_light()\n#> `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n3.6 Figure 2.4 그래프 데이터셋 전처리\n\ndata_2022 %>% names()\n#> [1] \"지점\"         \"지점명\"       \"일시\"         \"평균기온..C.\" \"최저기온..C.\"\n#> [6] \"최고기온..C.\"\n\ndata_2022 %>% head()\n#>   지점 지점명       일시 평균기온..C. 최저기온..C. 최고기온..C.\n#> 1  108   서울 2022-01-01         -4.3        -10.2          2.3\n#> 2  108   서울 2022-01-02         -1.3         -5.2          3.0\n#> 3  108   서울 2022-01-03         -1.9         -8.0          2.5\n#> 4  108   서울 2022-01-04         -2.5         -5.6          1.0\n#> 5  108   서울 2022-01-05         -2.8         -7.8          1.9\n#> 6  108   서울 2022-01-06         -2.2         -5.9          3.3\n\ndata_2022_month <- data_2022 %>%\n  mutate(month = format(일시, '%B')) %>%    # 일시에서 월만 뽑아낸 month 컬럼 생성\n  group_by(지점명, month) %>%               # 지점명, month로 그룹화\n  summarise(mean = mean(평균기온..C.)) %>%  # 그룹화된 데이터의 집계값 요약\n  ungroup() %>%                            # 그룹화를 해제하여 일반적인 데이터 프레임 형태로 사용\n  mutate(month = factor(month,             # month값을 factor 형태로 수정해서 원하는 levels 지정가능\n                        levels = paste(1:12, '월',sep = ''))) # sep='' : 간격없이 붙이기\n#> `summarise()` has grouped output by '지점명'. You can override using the\n#> `.groups` argument.\n\ndata_2022_month\n#> # A tibble: 84 × 3\n#>    지점명 month  mean\n#>    <chr>  <fct> <dbl>\n#>  1 고산   10월  18.0 \n#>  2 고산   11월  15.5 \n#>  3 고산   12월   7.28\n#>  4 고산   1월    6.08\n#>  5 고산   2월    5.24\n#>  6 고산   3월   10.8 \n#>  7 고산   4월   14.8 \n#>  8 고산   5월   18.1 \n#>  9 고산   6월   22.1 \n#> 10 고산   7월   27.0 \n#> # … with 74 more rows\n\n\n\n3.7 Figure 2.4 ggplot + geom_tile + fill color\n\n사용 데이터셋 : data_2022_month\nx=month, y=지점명\nfill : mean(평균온도)\ngeom_tile : 넓이, 높이 지정\nscale_fill_viridis_c\n\n이름 : temperature\n옵션 : B (밝기)\nbegin, end : 색상의 시작,끝\n\ncoord_fixed\n\nexpand = F : 공백 제거\n\nylab(NULL) : y축 이름 제거\n\n\nggplot(data_2022_month, aes(x = month, y = 지점명, fill = mean)) +\n  geom_tile(width = 0.95, height = 0.95) +\n  scale_fill_viridis_c(option = 'B', begin = 0.15, end =  0.98,\n                       name = 'temperature') +\n  coord_fixed(expand = FALSE) +\n  ylab(NULL) # ylab('')"
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html",
    "href": "posts/Data_Mining_Pandas/pandas.html",
    "title": "Data_Mining CH2",
    "section": "",
    "text": "Pandas Basic"
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#pandas-기본",
    "href": "posts/Data_Mining_Pandas/pandas.html#pandas-기본",
    "title": "Data_Mining CH2",
    "section": "1 Pandas 기본",
    "text": "1 Pandas 기본\n도구 - 판다스(pandas)\n\npandas 라이브러리는 사용하기 쉬운 고성능 데이터 구조와 데이터 분석 도구를 제공\n주 데이터 구조는 DataFrame, 인-메모리(in-memory) 2D 테이블로 생각할 수 있음 (열 이름과 행 레이블이 있는 스프레드시트와 비슷)\n엑셀에 있는 많은 기능을 프로그램에서 사용 가능 (피봇 테이블이나 다른 열을 기반으로 열을 계산하고 그래프 출력하는 기능 등이 포함)\n열 값으로 행 그룹핑 가능, SQL과 비슷하게 테이블 조인 가능, 판다스는 시계열 데이터를 다루는데도 효과적\n\n필요 라이브러리:\n\n넘파이(NumPy) – 넘파이에 익숙하지 않다면 지금 넘파이 튜토리얼을 둘러 보세요.\n\n\n\n\nJupyter에서 실행하기\n\n\n\n\n\n구글 코랩에서 실행하기"
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#설정",
    "href": "posts/Data_Mining_Pandas/pandas.html#설정",
    "title": "Data_Mining CH2",
    "section": "2 설정",
    "text": "2 설정\n먼저 pandas를 임포트합니다. 보통 pd로 임포트합니다:\nimport pandas as pd"
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#series-객체",
    "href": "posts/Data_Mining_Pandas/pandas.html#series-객체",
    "title": "Data_Mining CH2",
    "section": "3 Series 객체",
    "text": "3 Series 객체\npandas 라이브러리는 다음과 같은 유용한 데이터 구조를 포함하고 있습니다:\n\nSeries 객체를 곧 이어서 설명하겠습니다. Series 객체는 1D 배열입니다. (열 이름과 행 레이블을 가진) 스프레드시트의 열과 비슷합니다.\nDataFrame 객체는 2D 테이블입니다. (열 이름과 행 레이블을 가진) 스프레드시트와 비슷합니다."
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#series-만들기",
    "href": "posts/Data_Mining_Pandas/pandas.html#series-만들기",
    "title": "Data_Mining CH2",
    "section": "4 Series 만들기",
    "text": "4 Series 만들기\n첫 번째 Series 객체를 만들어 보죠!\ns = pd.Series([2,-1,3,5])\ns\n0    2\n1   -1\n2    3\n3    5\ndtype: int64"
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#d-ndarray와-비슷합니다",
    "href": "posts/Data_Mining_Pandas/pandas.html#d-ndarray와-비슷합니다",
    "title": "Data_Mining CH2",
    "section": "5 1D ndarray와 비슷합니다",
    "text": "5 1D ndarray와 비슷합니다\nSeries 객체는 넘파이 ndarray와 비슷하게 동작합니다. 넘파이 함수에 매개변수로 종종 전달할 수 있습니다:\nimport numpy as np\nnp.exp(s)\n0      7.389056\n1      0.367879\n2     20.085537\n3    148.413159\ndtype: float64\nSeries 객체에 대한 산술 연산도 가능합니다. ndarray와 비슷하게 원소별로 적용됩니다:\ns + [1000,2000,3000,4000]\n0    1002\n1    1999\n2    3003\n3    4005\ndtype: int64\n넘파이와 비슷하게 Series에 하나의 숫자를 더하면 Series에 있는 모든 원소에 더해집니다. 이를 브로드캐스팅(broadcasting)이라고 합니다:\ns + 1000\n0    1002\n1     999\n2    1003\n3    1005\ndtype: int64\n*나 / 같은 모든 이항 연산과 심지어 조건 연산에서도 마찬가지입니다:\ns < 0\n0    False\n1     True\n2    False\n3    False\ndtype: bool"
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#인덱스-레이블",
    "href": "posts/Data_Mining_Pandas/pandas.html#인덱스-레이블",
    "title": "Data_Mining CH2",
    "section": "6 인덱스 레이블",
    "text": "6 인덱스 레이블\nSeries 객체에 있는 각 원소는 인덱스 레이블(index label)이라 불리는 고유한 식별자를 가지고 있습니다. 기본적으로 Series에 있는 원소의 순서입니다(0에서 시작합니다). 하지만 수동으로 인덱스 레이블을 지정할 수도 있습니다:\ns2 = pd.Series([68, 83, 112, 68], index=[\"alice\", \"bob\", \"charles\", \"darwin\"])\ns2\nalice       68\nbob         83\ncharles    112\ndarwin      68\ndtype: int64\n그다음 dict처럼 Series를 사용할 수 있습니다:\ns2[\"bob\"]\n83\n일반 배열처럼 정수 인덱스를 사용하여 계속 원소에 접근할 수 있습니다:\ns2[1]\n83\n레이블이나 정수를 사용해 접근할 때 명확하게 하기 위해 레이블은 loc 속성을 사용하고 정수는 iloc 속성을 사용하는 것이 좋습니다:\ns2.loc[\"bob\"]\n83\ns2.iloc[1]\n83\nSeries는 인덱스 레이블을 슬라이싱할 수도 있습니다:\ns2.iloc[1:3]\nbob         83\ncharles    112\ndtype: int64\n기본 정수 레이블을 사용할 때 예상 외의 결과를 만들 수 있기 때문에 주의해야 합니다:\nsurprise = pd.Series([1000, 1001, 1002, 1003])\nsurprise\n0    1000\n1    1001\n2    1002\n3    1003\ndtype: int64\nsurprise_slice = surprise[2:]\nsurprise_slice\n2    1002\n3    1003\ndtype: int64\n보세요. 첫 번째 원소의 인덱스 레이블이 2입니다. 따라서 슬라이싱 결과에서 인덱스 레이블 0인 원소는 없습니다:\ntry:\n    surprise_slice[0]\nexcept KeyError as e:\n    print(\"키 에러:\", e)\n키 에러: 0\n하지만 iloc 속성을 사용해 정수 인덱스로 원소에 접근할 수 있습니다. Series 객체를 사용할 때 loc와 iloc를 사용하는 것이 좋은 이유입니다:\nsurprise_slice.iloc[0]\n1002\n### \nsurprise_slice.loc[2]\n1002"
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#dict에서-초기화",
    "href": "posts/Data_Mining_Pandas/pandas.html#dict에서-초기화",
    "title": "Data_Mining CH2",
    "section": "7 dict에서 초기화",
    "text": "7 dict에서 초기화\ndict에서 Series 객체를 만들 수 있습니다. 키는 인덱스 레이블로 사용됩니다:\nweights = {\"alice\": 68, \"bob\": 83, \"colin\": 86, \"darwin\": 68}\ns3 = pd.Series(weights)\ns3\nalice     68\nbob       83\ncolin     86\ndarwin    68\ndtype: int64\nSeries에 포함할 원소를 제어하고 index를 지정하여 명시적으로 순서를 결정할 수 있습니다:\ns4 = pd.Series(weights, index = [\"colin\", \"alice\"])\ns4\ncolin    86\nalice    68\ndtype: int64"
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#자동-정렬",
    "href": "posts/Data_Mining_Pandas/pandas.html#자동-정렬",
    "title": "Data_Mining CH2",
    "section": "8 자동 정렬",
    "text": "8 자동 정렬\n여러 개의 Series 객체를 다룰 때 pandas는 자동으로 인덱스 레이블에 따라 원소를 정렬합니다.\nprint(s2.keys())\nprint(s3.keys())\n\ns2 + s3\nIndex(['alice', 'bob', 'charles', 'darwin'], dtype='object')\nIndex(['alice', 'bob', 'colin', 'darwin'], dtype='object')\n\n\n\n\n\nalice      136.0\nbob        166.0\ncharles      NaN\ncolin        NaN\ndarwin     136.0\ndtype: float64\n만들어진 Series는 s2와 s3의 인덱스 레이블의 합집합을 담고 있습니다. s2에 \"colin\"이 없고 s3에 \"charles\"가 없기 때문에 이 원소는 NaN 값을 가집니다(Not-a-Number는 누락이란 의미입니다).\n자동 정렬은 구조가 다고 누락된 값이 있는 여러 데이터를 다룰 때 매우 편리합니다. 하지만 올바른 인덱스 레이블을 지정하는 것을 잊는다면 원치않는 결과를 얻을 수 있습니다:\ns5 = pd.Series([1000,1000,1000,1000])\nprint(\"s2 =\", s2.values)\nprint(\"s5 =\", s5.values)\n\ns2 + s5\ns2 = [ 68  83 112  68]\ns5 = [1000 1000 1000 1000]\n\n\n\n\n\nalice     NaN\nbob       NaN\ncharles   NaN\ndarwin    NaN\n0         NaN\n1         NaN\n2         NaN\n3         NaN\ndtype: float64\n레이블이 하나도 맞지 않기 때문에 판다스가 이 Series를 정렬할 수 없습니다. 따라서 모두 NaN이 되었습니다."
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#스칼라로-초기화",
    "href": "posts/Data_Mining_Pandas/pandas.html#스칼라로-초기화",
    "title": "Data_Mining CH2",
    "section": "9 스칼라로 초기화",
    "text": "9 스칼라로 초기화\n스칼라와 인덱스 레이블의 리스트로 Series 객체를 초기화할 수도 있습니다: 모든 원소가 이 스칼라 값으로 설정됩니다.\nmeaning = pd.Series(42, [\"life\", \"universe\", \"everything\"])\nmeaning\nlife          42\nuniverse      42\neverything    42\ndtype: int64"
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#series-이름",
    "href": "posts/Data_Mining_Pandas/pandas.html#series-이름",
    "title": "Data_Mining CH2",
    "section": "10 Series 이름",
    "text": "10 Series 이름\nSeries는 name을 가질 수 있습니다:\ns6 = pd.Series([83, 68], index=[\"bob\", \"alice\"], name=\"weights\")\ns6\nbob      83\nalice    68\nName: weights, dtype: int64"
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#series-그래프-출력",
    "href": "posts/Data_Mining_Pandas/pandas.html#series-그래프-출력",
    "title": "Data_Mining CH2",
    "section": "11 Series 그래프 출력",
    "text": "11 Series 그래프 출력\n맷플롯립을 사용해 Series 데이터를 쉽게 그래프로 출력할 수 있습니다(맷플롯립에 대한 자세한 설명은 맷플롯립 튜토리얼을 참고하세요). 맷플롯립을 임포트하고 plot() 메서드를 호출하면 끝입니다:\n%matplotlib inline\nimport matplotlib.pyplot as plt\ntemperatures = [4.4,5.1,6.1,6.2,6.1,6.1,5.7,5.2,4.7,4.1,3.9,3.5]\ns7 = pd.Series(temperatures, name=\"Temperature\")\ns7.plot()\nplt.show()\n\n\n\npng\n\n\n데이터를 그래프로 출력하는데 많은 옵션이 있습니다. 여기에서 모두 나열할 필요는 없습니다. 특정 종류의 그래프(히스토그램, 파이 차트 등)가 필요하면 판다스 문서의 시각화 섹션에서 예제 코드를 참고하세요."
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#시간-다루기",
    "href": "posts/Data_Mining_Pandas/pandas.html#시간-다루기",
    "title": "Data_Mining CH2",
    "section": "12 시간 다루기",
    "text": "12 시간 다루기\n많은 데이터셋에 타임스탬프가 포함되어 있습니다. 판다스는 이런 데이터를 다루는데 뛰어납니다: * (2016Q3 같은) 기간과 (“monthly” 같은) 빈도를 표현할 수 있습니다. * 기간을 실제 타임스탬프로 변환하거나 그 반대로 변환할 수 있습니다. * 데이터를 리샘플링하고 원하는 방식으로 값을 모을 수 있습니다. * 시간대를 다룰 수 있습니다."
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#시간-범위",
    "href": "posts/Data_Mining_Pandas/pandas.html#시간-범위",
    "title": "Data_Mining CH2",
    "section": "13 시간 범위",
    "text": "13 시간 범위\n먼저 pd.date_range()를 사용해 시계열을 만들어 보죠. 이 함수는 2016년 10월 29일 5:30pm에서 시작하여 12시간마다 하나의 datetime을 담고 있는 DatetimeIndex를 반환합니다.\ndates = pd.date_range('2016/10/29 5:30pm', periods=12, freq='H')\ndates\nDatetimeIndex(['2016-10-29 17:30:00', '2016-10-29 18:30:00',\n               '2016-10-29 19:30:00', '2016-10-29 20:30:00',\n               '2016-10-29 21:30:00', '2016-10-29 22:30:00',\n               '2016-10-29 23:30:00', '2016-10-30 00:30:00',\n               '2016-10-30 01:30:00', '2016-10-30 02:30:00',\n               '2016-10-30 03:30:00', '2016-10-30 04:30:00'],\n              dtype='datetime64[ns]', freq='H')\n이 DatetimeIndex를 Series의 인덱스로 사용할수 있습니다:\ntemp_series = pd.Series(temperatures, dates)\ntemp_series\n2016-10-29 17:30:00    4.4\n2016-10-29 18:30:00    5.1\n2016-10-29 19:30:00    6.1\n2016-10-29 20:30:00    6.2\n2016-10-29 21:30:00    6.1\n2016-10-29 22:30:00    6.1\n2016-10-29 23:30:00    5.7\n2016-10-30 00:30:00    5.2\n2016-10-30 01:30:00    4.7\n2016-10-30 02:30:00    4.1\n2016-10-30 03:30:00    3.9\n2016-10-30 04:30:00    3.5\nFreq: H, dtype: float64\n이 시리즈를 그래프로 출력해 보죠:\ntemp_series.plot(kind=\"bar\")\n\nplt.grid(True)\nplt.show()\n\n\n\npng"
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#리샘플링",
    "href": "posts/Data_Mining_Pandas/pandas.html#리샘플링",
    "title": "Data_Mining CH2",
    "section": "14 리샘플링",
    "text": "14 리샘플링\n판다스는 매우 간단하게 시계열을 리샘플링할 수 있습니다. resample() 메서드를 호출하고 새로운 주기를 지정하면 됩니다:\ntemp_series_freq_2H = temp_series.resample(\"2H\")\ntemp_series_freq_2H\n<pandas.core.resample.DatetimeIndexResampler object at 0x000001F68E3F4760>\n리샘플링 연산은 사실 지연된 연산입니다. 그래서 Series 객체 대신 DatetimeIndexResampler 객체가 반환됩니다. 실제 리샘플링 연산을 수행하려면 mean() 같은 메서드를 호출할 수 있습니다. 이 메서드는 연속적인 시간 쌍에 대해 평균을 계산합니다:\ntemp_series_freq_2H = temp_series_freq_2H.mean()\n결과를 그래프로 출력해 보죠:\ntemp_series_freq_2H.plot(kind=\"bar\")\nplt.show()\n\n\n\npng\n\n\n2시간 간격으로 어떻게 값이 수집되었는지 확인해 보세요. 예를 들어 6-8pm 간격을 보면 6:30pm에서 5.1이고 7:30pm에서 6.1입니다. 리샘플링 후에 5.1과 6.1의 평균인 5.6 하나를 얻었습니다. 평균말고 어떤 집계 함수(aggregation function)도 사용할 수 있습니다. 예를 들어 각 기간에서 최솟값을 찾을 수 있습니다:\ntemp_series_freq_2H = temp_series.resample(\"2H\").min()\ntemp_series_freq_2H\n2016-10-29 16:00:00    4.4\n2016-10-29 18:00:00    5.1\n2016-10-29 20:00:00    6.1\n2016-10-29 22:00:00    5.7\n2016-10-30 00:00:00    4.7\n2016-10-30 02:00:00    3.9\n2016-10-30 04:00:00    3.5\nFreq: 2H, dtype: float64\n또는 동일한 효과를 내는 apply() 메서드를 사용할 수 있습니다:\ntemp_series_freq_2H = temp_series.resample(\"2H\").apply(np.min)\ntemp_series_freq_2H\n2016-10-29 16:00:00    4.4\n2016-10-29 18:00:00    5.1\n2016-10-29 20:00:00    6.1\n2016-10-29 22:00:00    5.7\n2016-10-30 00:00:00    4.7\n2016-10-30 02:00:00    3.9\n2016-10-30 04:00:00    3.5\nFreq: 2H, dtype: float64"
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#업샘플링과-보간",
    "href": "posts/Data_Mining_Pandas/pandas.html#업샘플링과-보간",
    "title": "Data_Mining CH2",
    "section": "15 업샘플링과 보간",
    "text": "15 업샘플링과 보간\n다운샘플링의 예를 보았습니다. 하지만 업샘플링(즉, 빈도를 높입니다)도 할 수 있습니다. 하지만 데이터에 구멍을 만듭니다:\ntemp_series_freq_15min = temp_series.resample(\"15Min\").mean()\ntemp_series_freq_15min.head(n=10) # `head`는 상위 n 개의 값만 출력합니다\n2016-10-29 17:30:00    4.4\n2016-10-29 17:45:00    NaN\n2016-10-29 18:00:00    NaN\n2016-10-29 18:15:00    NaN\n2016-10-29 18:30:00    5.1\n2016-10-29 18:45:00    NaN\n2016-10-29 19:00:00    NaN\n2016-10-29 19:15:00    NaN\n2016-10-29 19:30:00    6.1\n2016-10-29 19:45:00    NaN\nFreq: 15T, dtype: float64\n한가지 방법은 보간으로 사이를 채우는 것입니다. 이렇게 하려면 interpolate() 메서드를 호출합니다. 기본값은 선형 보간이지만 3차 보간(cubic interpolation) 같은 다른 방법을 선택할 수 있습니다:\ntemp_series_freq_15min = temp_series.resample(\"15Min\").interpolate(method=\"cubic\")\ntemp_series_freq_15min.head(n=10)\n2016-10-29 17:30:00    4.400000\n2016-10-29 17:45:00    4.452911\n2016-10-29 18:00:00    4.605113\n2016-10-29 18:15:00    4.829758\n2016-10-29 18:30:00    5.100000\n2016-10-29 18:45:00    5.388992\n2016-10-29 19:00:00    5.669887\n2016-10-29 19:15:00    5.915839\n2016-10-29 19:30:00    6.100000\n2016-10-29 19:45:00    6.203621\nFreq: 15T, dtype: float64\ntemp_series.plot(label=\"Period: 1 hour\")\ntemp_series_freq_15min.plot(label=\"Period: 15 minutes\")\nplt.legend()\nplt.show()\n\n\n\npng"
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#시간대",
    "href": "posts/Data_Mining_Pandas/pandas.html#시간대",
    "title": "Data_Mining CH2",
    "section": "16 시간대",
    "text": "16 시간대\n기본적으로 datetime은 단순합니다. 시간대(timezone)을 고려하지 않죠. 따라서 2016-10-30 02:30는 파리나 뉴욕이나 2016년 10월 30일 2:30pm입니다. tz_localize() 메서드로 시간대를 고려한 datetime을 만들 수 있습니다:\ntemp_series_ny = temp_series.tz_localize(\"America/New_York\")\ntemp_series_ny\n2016-10-29 17:30:00-04:00    4.4\n2016-10-29 18:30:00-04:00    5.1\n2016-10-29 19:30:00-04:00    6.1\n2016-10-29 20:30:00-04:00    6.2\n2016-10-29 21:30:00-04:00    6.1\n2016-10-29 22:30:00-04:00    6.1\n2016-10-29 23:30:00-04:00    5.7\n2016-10-30 00:30:00-04:00    5.2\n2016-10-30 01:30:00-04:00    4.7\n2016-10-30 02:30:00-04:00    4.1\n2016-10-30 03:30:00-04:00    3.9\n2016-10-30 04:30:00-04:00    3.5\ndtype: float64\n모든 datetime에 -04:00이 추가됩니다. 즉 모든 시간은 UTC - 4시간을 의미합니다.\n다음처럼 파리 시간대로 바꿀 수 있습니다:\ntemp_series_paris = temp_series_ny.tz_convert(\"Europe/Paris\")\ntemp_series_paris\n2016-10-29 23:30:00+02:00    4.4\n2016-10-30 00:30:00+02:00    5.1\n2016-10-30 01:30:00+02:00    6.1\n2016-10-30 02:30:00+02:00    6.2\n2016-10-30 02:30:00+01:00    6.1\n2016-10-30 03:30:00+01:00    6.1\n2016-10-30 04:30:00+01:00    5.7\n2016-10-30 05:30:00+01:00    5.2\n2016-10-30 06:30:00+01:00    4.7\n2016-10-30 07:30:00+01:00    4.1\n2016-10-30 08:30:00+01:00    3.9\n2016-10-30 09:30:00+01:00    3.5\ndtype: float64\nUTC와의 차이가 +02:00에서 +01:00으로 바뀐 것을 알 수 있습니다. 이는 프랑스가 10월 30일 3am에 겨울 시간으로 바꾸기 때문입니다(2am으로 바뀝니다). 따라서 2:30am이 두 번 등장합니다! 시간대가 없는 표현으로 돌아가 보죠(시간대가 없이 지역 시간으로 매시간 로그를 기록하는 경우 이와 비슷할 것입니다):\ntemp_series_paris_naive = temp_series_paris.tz_localize(None)\ntemp_series_paris_naive\n2016-10-29 23:30:00    4.4\n2016-10-30 00:30:00    5.1\n2016-10-30 01:30:00    6.1\n2016-10-30 02:30:00    6.2\n2016-10-30 02:30:00    6.1\n2016-10-30 03:30:00    6.1\n2016-10-30 04:30:00    5.7\n2016-10-30 05:30:00    5.2\n2016-10-30 06:30:00    4.7\n2016-10-30 07:30:00    4.1\n2016-10-30 08:30:00    3.9\n2016-10-30 09:30:00    3.5\ndtype: float64\n이렇게 되면 02:30이 정말 애매합니다. 시간대가 없는 datetime을 파리 시간대로 바꿀 때 에러가 발생합니다:\ntry:\n    temp_series_paris_naive.tz_localize(\"Europe/Paris\")\nexcept Exception as e:\n    print(type(e))\n    print(e)\n<class 'pytz.exceptions.AmbiguousTimeError'>\nCannot infer dst time from 2016-10-30 02:30:00, try using the 'ambiguous' argument\n다행히 ambiguous 매개변수를 사용하면 판다스가 타임스탬프의 순서를 기반으로 적절한 DST(일광 절약 시간제)를 추측합니다:\ntemp_series_paris_naive.tz_localize(\"Europe/Paris\", ambiguous=\"infer\")\n2016-10-29 23:30:00+02:00    4.4\n2016-10-30 00:30:00+02:00    5.1\n2016-10-30 01:30:00+02:00    6.1\n2016-10-30 02:30:00+02:00    6.2\n2016-10-30 02:30:00+01:00    6.1\n2016-10-30 03:30:00+01:00    6.1\n2016-10-30 04:30:00+01:00    5.7\n2016-10-30 05:30:00+01:00    5.2\n2016-10-30 06:30:00+01:00    4.7\n2016-10-30 07:30:00+01:00    4.1\n2016-10-30 08:30:00+01:00    3.9\n2016-10-30 09:30:00+01:00    3.5\ndtype: float64"
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#기간",
    "href": "posts/Data_Mining_Pandas/pandas.html#기간",
    "title": "Data_Mining CH2",
    "section": "17 기간",
    "text": "17 기간\npd.period_range() 함수는 DatetimeIndex가 아니라 PeriodIndex를 반환합니다. 예를 들어 2016과 2017년의 전체 분기를 가져와 보죠:\nquarters = pd.period_range('2016Q1', periods=8, freq='Q')\nquarters\nPeriodIndex(['2016Q1', '2016Q2', '2016Q3', '2016Q4', '2017Q1', '2017Q2',\n             '2017Q3', '2017Q4'],\n            dtype='period[Q-DEC]')\nPeriodIndex에 숫자 N을 추가하면 PeriodIndex 빈도의 N 배만큼 이동시킵니다:\nquarters + 3\nPeriodIndex(['2016Q4', '2017Q1', '2017Q2', '2017Q3', '2017Q4', '2018Q1',\n             '2018Q2', '2018Q3'],\n            dtype='period[Q-DEC]')\nasfreq() 메서드를 사용하면 PeriodIndex의 빈도를 바꿀 수 있습니다. 모든 기간이 늘어나거나 줄어듭니다. 예를 들어 분기 기간을 모두 월별 기간으로 바꾸어 보죠:\nquarters.asfreq(\"M\")\nPeriodIndex(['2016-03', '2016-06', '2016-09', '2016-12', '2017-03', '2017-06',\n             '2017-09', '2017-12'],\n            dtype='period[M]')\n기본적으로 asfreq는 각 기간의 끝에 맞춥니다. 기간의 시작에 맞추도록 변경할 수 있습니다:\nquarters.asfreq(\"M\", how=\"start\")\nPeriodIndex(['2016-01', '2016-04', '2016-07', '2016-10', '2017-01', '2017-04',\n             '2017-07', '2017-10'],\n            dtype='period[M]')\n간격을 늘릴 수도 있습니다:\nquarters.asfreq(\"A\")\nPeriodIndex(['2016', '2016', '2016', '2016', '2017', '2017', '2017', '2017'], dtype='period[A-DEC]')\n물론 PeriodIndex로 Series를 만들 수 있습니다:\nquarterly_revenue = pd.Series([300, 320, 290, 390, 320, 360, 310, 410], index = quarters)\nquarterly_revenue\n2016Q1    300\n2016Q2    320\n2016Q3    290\n2016Q4    390\n2017Q1    320\n2017Q2    360\n2017Q3    310\n2017Q4    410\nFreq: Q-DEC, dtype: int64\nquarterly_revenue.plot(kind=\"line\")\nplt.show()\n\n\n\npng\n\n\nto_timestamp를 호출해서 기간을 타임스탬프로 변경할 수 있습니다. 기본적으로 기간의 첫 번째 날을 반환합니다. 하지만 how와 freq를 지정해서 기간의 마지막 시간을 얻을 수 있습니다:\nlast_hours = quarterly_revenue.to_timestamp(how=\"end\", freq=\"H\")\nlast_hours\n2016-03-31 23:59:59.999999999    300\n2016-06-30 23:59:59.999999999    320\n2016-09-30 23:59:59.999999999    290\n2016-12-31 23:59:59.999999999    390\n2017-03-31 23:59:59.999999999    320\n2017-06-30 23:59:59.999999999    360\n2017-09-30 23:59:59.999999999    310\n2017-12-31 23:59:59.999999999    410\ndtype: int64\nto_peroid를 호출하면 다시 기간으로 돌아갑니다:\nlast_hours.to_period()\n2016Q1    300\n2016Q2    320\n2016Q3    290\n2016Q4    390\n2017Q1    320\n2017Q2    360\n2017Q3    310\n2017Q4    410\nFreq: Q-DEC, dtype: int64\n판다스는 여러 가지 시간 관련 함수를 많이 제공합니다. 온라인 문서를 확인해 보세요. 예를 하나 들면 2016년 매월 마지막 업무일의 9시를 얻는 방법은 다음과 같습니다:\nmonths_2016 = pd.period_range(\"2016\", periods=12, freq=\"M\")\none_day_after_last_days = months_2016.asfreq(\"D\") + 1\nlast_bdays = one_day_after_last_days.to_timestamp() - pd.tseries.offsets.BDay()\nlast_bdays.to_period(\"H\") + 9\nPeriodIndex(['2016-01-29 09:00', '2016-02-29 09:00', '2016-03-31 09:00',\n             '2016-04-29 09:00', '2016-05-31 09:00', '2016-06-30 09:00',\n             '2016-07-29 09:00', '2016-08-31 09:00', '2016-09-30 09:00',\n             '2016-10-31 09:00', '2016-11-30 09:00', '2016-12-30 09:00'],\n            dtype='period[H]')"
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#dataframe-객체",
    "href": "posts/Data_Mining_Pandas/pandas.html#dataframe-객체",
    "title": "Data_Mining CH2",
    "section": "18 DataFrame 객체",
    "text": "18 DataFrame 객체\n데이터프레임 객체는 스프레드시트를 표현합니다. 셀 값, 열 이름, 행 인덱스 레이블을 가집니다. 다른 열을 바탕으로 열을 계산하는 식을 쓸 수 있고 피봇 테이블을 만들고, 행을 그룹핑하고, 그래프를 그릴 수 있습니다. DataFrame을 Series의 딕셔너리로 볼 수 있습니다."
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#dataframe-만들기",
    "href": "posts/Data_Mining_Pandas/pandas.html#dataframe-만들기",
    "title": "Data_Mining CH2",
    "section": "19 DataFrame 만들기",
    "text": "19 DataFrame 만들기\nSeries 객체의 딕셔너리를 전달하여 데이터프레임을 만들 수 있습니다:\npeople_dict = {\n    \"weight\": pd.Series([68, 83, 112], index=[\"alice\", \"bob\", \"charles\"]),\n    \"birthyear\": pd.Series([1984, 1985, 1992], index=[\"bob\", \"alice\", \"charles\"], name=\"year\"),\n    \"children\": pd.Series([0, 3], index=[\"charles\", \"bob\"]),\n    \"hobby\": pd.Series([\"Biking\", \"Dancing\"], index=[\"alice\", \"bob\"]),\n}\npeople = pd.DataFrame(people_dict)\npeople\n\n\n\n\n\n\n\n\n\nweight\n\n\nbirthyear\n\n\nchildren\n\n\nhobby\n\n\n\n\n\n\nalice\n\n\n68\n\n\n1985\n\n\nNaN\n\n\nBiking\n\n\n\n\nbob\n\n\n83\n\n\n1984\n\n\n3.0\n\n\nDancing\n\n\n\n\ncharles\n\n\n112\n\n\n1992\n\n\n0.0\n\n\nNaN\n\n\n\n\n\n\n몇가지 알아 두어야 할 것은 다음과 같습니다:\n\nSeries는 인덱스를 기반으로 자동으로 정렬됩니다.\n누란된 값은 NaN으로 표현됩니다.\nSeries 이름은 무시됩니다(\"year\"란 이름은 삭제됩니다).\nDataFrame은 주피터 노트북에서 멋지게 출력됩니다!\n\n예상하는 방식으로 열을 참조할 수 있고 Serires 객체가 반환됩니다:\npeople[\"birthyear\"]\nalice      1985\nbob        1984\ncharles    1992\nName: birthyear, dtype: int64\n동시에 여러 개의 열을 선택할 수 있습니다:\npeople[[\"birthyear\", \"hobby\"]]\n\n\n\n\n\n\n\n\n\nbirthyear\n\n\nhobby\n\n\n\n\n\n\nalice\n\n\n1985\n\n\nBiking\n\n\n\n\nbob\n\n\n1984\n\n\nDancing\n\n\n\n\ncharles\n\n\n1992\n\n\nNaN\n\n\n\n\n\n\n열 리스트나 행 인덱스 레이블을 DataFrame 생성자에 전달하면 해당 열과 행으로 채워진 데이터프레임이 반환됩니다. 예를 들면:\nd2 = pd.DataFrame(\n        people_dict,\n        columns=[\"birthyear\", \"weight\", \"height\"],\n        index=[\"bob\", \"alice\", \"eugene\"]\n     )\nd2\n\n\n\n\n\n\n\n\n\nbirthyear\n\n\nweight\n\n\nheight\n\n\n\n\n\n\nbob\n\n\n1984.0\n\n\n83.0\n\n\nNaN\n\n\n\n\nalice\n\n\n1985.0\n\n\n68.0\n\n\nNaN\n\n\n\n\neugene\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n\n\nDataFrame을 만드는 또 다른 편리한 방법은 ndarray나 리스트의 리스트로 모든 값을 생성자에게 전달하고 열 이름과 행 인덱스 레이블을 각기 지정하는 것입니다:\nvalues = [\n            [1985, np.nan, \"Biking\",   68],\n            [1984, 3,      \"Dancing\",  83],\n            [1992, 0,      np.nan,    112]\n         ]\nd3 = pd.DataFrame(\n        values,\n        columns=[\"birthyear\", \"children\", \"hobby\", \"weight\"],\n        index=[\"alice\", \"bob\", \"charles\"]\n     )\nd3\n\n\n\n\n\n\n\n\n\nbirthyear\n\n\nchildren\n\n\nhobby\n\n\nweight\n\n\n\n\n\n\nalice\n\n\n1985\n\n\nNaN\n\n\nBiking\n\n\n68\n\n\n\n\nbob\n\n\n1984\n\n\n3.0\n\n\nDancing\n\n\n83\n\n\n\n\ncharles\n\n\n1992\n\n\n0.0\n\n\nNaN\n\n\n112\n\n\n\n\n\n\n누락된 값을 지정하려면 np.nan이나 넘파이 마스크 배열을 사용합니다:\nmasked_array = np.ma.asarray(values, dtype=np.object)\nmasked_array[(0, 2), (1, 2)] = np.ma.masked\nd3 = pd.DataFrame(\n        masked_array,\n        columns=[\"birthyear\", \"children\", \"hobby\", \"weight\"],\n        index=[\"alice\", \"bob\", \"charles\"]\n     )\nd3\nC:\\Users\\seong taek\\AppData\\Local\\Temp\\ipykernel_9428\\1456005722.py:1: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  masked_array = np.ma.asarray(values, dtype=np.object)\n\n\n\n\n\n\n\n\n\nbirthyear\n\n\nchildren\n\n\nhobby\n\n\nweight\n\n\n\n\n\n\nalice\n\n\n1985\n\n\nNaN\n\n\nBiking\n\n\n68\n\n\n\n\nbob\n\n\n1984\n\n\n3\n\n\nDancing\n\n\n83\n\n\n\n\ncharles\n\n\n1992\n\n\n0\n\n\nNaN\n\n\n112\n\n\n\n\n\n\nndarray 대신에 DataFrame 객체를 전달할 수도 있습니다:\nd4 = pd.DataFrame(\n         d3,\n         columns=[\"hobby\", \"children\"],\n         index=[\"alice\", \"bob\"]\n     )\nd4\n\n\n\n\n\n\n\n\n\nhobby\n\n\nchildren\n\n\n\n\n\n\nalice\n\n\nBiking\n\n\nNaN\n\n\n\n\nbob\n\n\nDancing\n\n\n3\n\n\n\n\n\n\n딕셔너리의 딕셔너리(또는 리스트의 리스트)로 DataFrame을 만들 수 있습니다:\npeople = pd.DataFrame({\n    \"birthyear\": {\"alice\":1985, \"bob\": 1984, \"charles\": 1992},\n    \"hobby\": {\"alice\":\"Biking\", \"bob\": \"Dancing\"},\n    \"weight\": {\"alice\":68, \"bob\": 83, \"charles\": 112},\n    \"children\": {\"bob\": 3, \"charles\": 0}\n})\npeople\n\n\n\n\n\n\n\n\n\nbirthyear\n\n\nhobby\n\n\nweight\n\n\nchildren\n\n\n\n\n\n\nalice\n\n\n1985\n\n\nBiking\n\n\n68\n\n\nNaN\n\n\n\n\nbob\n\n\n1984\n\n\nDancing\n\n\n83\n\n\n3.0\n\n\n\n\ncharles\n\n\n1992\n\n\nNaN\n\n\n112\n\n\n0.0"
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#멀티-인덱싱",
    "href": "posts/Data_Mining_Pandas/pandas.html#멀티-인덱싱",
    "title": "Data_Mining CH2",
    "section": "20 멀티 인덱싱",
    "text": "20 멀티 인덱싱\n모든 열이 같은 크기의 튜플이면 멀티 인덱스로 인식합니다. 열 인덱스 레이블에도 같은 방식이 적용됩니다. 예를 들면:\nd5 = pd.DataFrame(\n  {\n    (\"public\", \"birthyear\"):\n        {(\"Paris\",\"alice\"):1985, (\"Paris\",\"bob\"): 1984, (\"London\",\"charles\"): 1992},\n    (\"public\", \"hobby\"):\n        {(\"Paris\",\"alice\"):\"Biking\", (\"Paris\",\"bob\"): \"Dancing\"},\n    (\"private\", \"weight\"):\n        {(\"Paris\",\"alice\"):68, (\"Paris\",\"bob\"): 83, (\"London\",\"charles\"): 112},\n    (\"private\", \"children\"):\n        {(\"Paris\", \"alice\"):np.nan, (\"Paris\",\"bob\"): 3, (\"London\",\"charles\"): 0}\n  }\n)\nd5\n\n\n\n\n\n\n\n\n\n\n\npublic\n\n\nprivate\n\n\n\n\n\n\n\n\nbirthyear\n\n\nhobby\n\n\nweight\n\n\nchildren\n\n\n\n\n\n\nParis\n\n\nalice\n\n\n1985\n\n\nBiking\n\n\n68\n\n\nNaN\n\n\n\n\nbob\n\n\n1984\n\n\nDancing\n\n\n83\n\n\n3.0\n\n\n\n\nLondon\n\n\ncharles\n\n\n1992\n\n\nNaN\n\n\n112\n\n\n0.0\n\n\n\n\n\n\n이제 \"public\" 열을 모두 담은 DataFrame을 손쉽게 만들 수 있습니다:\nd5[\"public\"]\n\n\n\n\n\n\n\n\n\n\n\nbirthyear\n\n\nhobby\n\n\n\n\n\n\nParis\n\n\nalice\n\n\n1985\n\n\nBiking\n\n\n\n\nbob\n\n\n1984\n\n\nDancing\n\n\n\n\nLondon\n\n\ncharles\n\n\n1992\n\n\nNaN\n\n\n\n\n\n\nd5[\"public\", \"hobby\"]  # d5[\"public\"][\"hobby\"]와 같습니다.\nParis   alice       Biking\n        bob        Dancing\nLondon  charles        NaN\nName: (public, hobby), dtype: object"
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#레벨-낮추기",
    "href": "posts/Data_Mining_Pandas/pandas.html#레벨-낮추기",
    "title": "Data_Mining CH2",
    "section": "21 레벨 낮추기",
    "text": "21 레벨 낮추기\nd5를 다시 확인해 보죠:\nd5\n\n\n\n\n\n\n\n\n\n\n\npublic\n\n\nprivate\n\n\n\n\n\n\n\n\nbirthyear\n\n\nhobby\n\n\nweight\n\n\nchildren\n\n\n\n\n\n\nParis\n\n\nalice\n\n\n1985\n\n\nBiking\n\n\n68\n\n\nNaN\n\n\n\n\nbob\n\n\n1984\n\n\nDancing\n\n\n83\n\n\n3.0\n\n\n\n\nLondon\n\n\ncharles\n\n\n1992\n\n\nNaN\n\n\n112\n\n\n0.0\n\n\n\n\n\n\n열의 레벨(level)이 2개이고 인덱스 레벨이 2개입니다. droplevel()을 사용해 열 레벨을 낮출 수 있습니다(인덱스도 마찬가지입니다):\nd5.columns = d5.columns.droplevel(level = 0)\nd5\n\n\n\n\n\n\n\n\n\n\n\nbirthyear\n\n\nhobby\n\n\nweight\n\n\nchildren\n\n\n\n\n\n\nParis\n\n\nalice\n\n\n1985\n\n\nBiking\n\n\n68\n\n\nNaN\n\n\n\n\nbob\n\n\n1984\n\n\nDancing\n\n\n83\n\n\n3.0\n\n\n\n\nLondon\n\n\ncharles\n\n\n1992\n\n\nNaN\n\n\n112\n\n\n0.0"
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#전치",
    "href": "posts/Data_Mining_Pandas/pandas.html#전치",
    "title": "Data_Mining CH2",
    "section": "22 전치",
    "text": "22 전치\nT 속성을 사용해 열과 인덱스를 바꿀 수 있습니다:\nd6 = d5.T\nd6\n\n\n\n\n\n\n\n\n\nParis\n\n\nLondon\n\n\n\n\n\n\nalice\n\n\nbob\n\n\ncharles\n\n\n\n\n\n\nbirthyear\n\n\n1985\n\n\n1984\n\n\n1992\n\n\n\n\nhobby\n\n\nBiking\n\n\nDancing\n\n\nNaN\n\n\n\n\nweight\n\n\n68\n\n\n83\n\n\n112\n\n\n\n\nchildren\n\n\nNaN\n\n\n3.0\n\n\n0.0"
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#레벨-스택과-언스택",
    "href": "posts/Data_Mining_Pandas/pandas.html#레벨-스택과-언스택",
    "title": "Data_Mining CH2",
    "section": "23 레벨 스택과 언스택",
    "text": "23 레벨 스택과 언스택\nstack() 메서드는 가장 낮은 열 레벨을 가장 낮은 인덱스 뒤에 추가합니다:\nd7 = d6.stack()\nd7\n\n\n\n\n\n\n\n\n\n\n\nLondon\n\n\nParis\n\n\n\n\n\n\nbirthyear\n\n\nalice\n\n\nNaN\n\n\n1985\n\n\n\n\nbob\n\n\nNaN\n\n\n1984\n\n\n\n\ncharles\n\n\n1992\n\n\nNaN\n\n\n\n\nhobby\n\n\nalice\n\n\nNaN\n\n\nBiking\n\n\n\n\nbob\n\n\nNaN\n\n\nDancing\n\n\n\n\nweight\n\n\nalice\n\n\nNaN\n\n\n68\n\n\n\n\nbob\n\n\nNaN\n\n\n83\n\n\n\n\ncharles\n\n\n112\n\n\nNaN\n\n\n\n\nchildren\n\n\nbob\n\n\nNaN\n\n\n3.0\n\n\n\n\ncharles\n\n\n0.0\n\n\nNaN\n\n\n\n\n\n\nNaN 값이 생겼습니다. 이전에 없던 조합이 생겼기 때문입니다(예를 들어 London에 bob이 없었습니다).\nunstack()을 호출하면 반대가 됩니다. 여기에서도 많은 NaN 값이 생성됩니다.\nd8 = d7.unstack()\nd8\n\n\n\n\n\n\n\n\n\nLondon\n\n\nParis\n\n\n\n\n\n\nalice\n\n\nbob\n\n\ncharles\n\n\nalice\n\n\nbob\n\n\ncharles\n\n\n\n\n\n\nbirthyear\n\n\nNaN\n\n\nNaN\n\n\n1992\n\n\n1985\n\n\n1984\n\n\nNaN\n\n\n\n\nchildren\n\n\nNaN\n\n\nNaN\n\n\n0.0\n\n\nNaN\n\n\n3.0\n\n\nNaN\n\n\n\n\nhobby\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nBiking\n\n\nDancing\n\n\nNaN\n\n\n\n\nweight\n\n\nNaN\n\n\nNaN\n\n\n112\n\n\n68\n\n\n83\n\n\nNaN\n\n\n\n\n\n\nunstack을 다시 호출하면 Series 객체가 만들어 집니다:\nd9 = d8.unstack()\nd9\nLondon  alice    birthyear        NaN\n                 children         NaN\n                 hobby            NaN\n                 weight           NaN\n        bob      birthyear        NaN\n                 children         NaN\n                 hobby            NaN\n                 weight           NaN\n        charles  birthyear       1992\n                 children         0.0\n                 hobby            NaN\n                 weight           112\nParis   alice    birthyear       1985\n                 children         NaN\n                 hobby         Biking\n                 weight            68\n        bob      birthyear       1984\n                 children         3.0\n                 hobby        Dancing\n                 weight            83\n        charles  birthyear        NaN\n                 children         NaN\n                 hobby            NaN\n                 weight           NaN\ndtype: object\nstack()과 unstack() 메서드를 사용할 때 스택/언스택할 level을 선택할 수 있습니다. 심지어 한 번에 여러 개의 레벨을 스택/언스택할 수도 있습니다:\nd10 = d9.unstack(level = (0,1))\nd10\n\n\n\n\n\n\n\n\n\nLondon\n\n\nParis\n\n\n\n\n\n\nalice\n\n\nbob\n\n\ncharles\n\n\nalice\n\n\nbob\n\n\ncharles\n\n\n\n\n\n\nbirthyear\n\n\nNaN\n\n\nNaN\n\n\n1992\n\n\n1985\n\n\n1984\n\n\nNaN\n\n\n\n\nchildren\n\n\nNaN\n\n\nNaN\n\n\n0.0\n\n\nNaN\n\n\n3.0\n\n\nNaN\n\n\n\n\nhobby\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nBiking\n\n\nDancing\n\n\nNaN\n\n\n\n\nweight\n\n\nNaN\n\n\nNaN\n\n\n112\n\n\n68\n\n\n83\n\n\nNaN"
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#대부분의-메서드는-수정된-복사본을-반환합니다",
    "href": "posts/Data_Mining_Pandas/pandas.html#대부분의-메서드는-수정된-복사본을-반환합니다",
    "title": "Data_Mining CH2",
    "section": "24 대부분의 메서드는 수정된 복사본을 반환합니다",
    "text": "24 대부분의 메서드는 수정된 복사본을 반환합니다\n눈치챘겠지만 stack()과 unstack() 메서드는 객체를 수정하지 않습니다. 대신 복사본을 만들어 반환합니다. 판다스에 있는 대부분의 메서드들이 이렇게 동작합니다."
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#행-참조하기",
    "href": "posts/Data_Mining_Pandas/pandas.html#행-참조하기",
    "title": "Data_Mining CH2",
    "section": "25 행 참조하기",
    "text": "25 행 참조하기\npeople DataFrame으로 돌아가 보죠:\npeople\n\n\n\n\n\n\n\n\n\nbirthyear\n\n\nhobby\n\n\nweight\n\n\nchildren\n\n\n\n\n\n\nalice\n\n\n1985\n\n\nBiking\n\n\n68\n\n\nNaN\n\n\n\n\nbob\n\n\n1984\n\n\nDancing\n\n\n83\n\n\n3.0\n\n\n\n\ncharles\n\n\n1992\n\n\nNaN\n\n\n112\n\n\n0.0\n\n\n\n\n\n\nloc 속성으로 열 대신 행을 참조할 수 있습니다. DataFrame의 열 이름이 행 인덱스 레이블로 매핑된 Series 객체가 반환됩니다:\npeople.loc[\"charles\"]\nbirthyear    1992\nhobby         NaN\nweight        112\nchildren      0.0\nName: charles, dtype: object\niloc 속성을 사용해 정수 인덱스로 행을 참조할 수 있습니다:\npeople.iloc[2]\nbirthyear    1992\nhobby         NaN\nweight        112\nchildren      0.0\nName: charles, dtype: object\n행을 슬라이싱할 수 있으며 DataFrame 객체가 반환됩니다:\npeople.iloc[1:3]\n\n\n\n\n\n\n\n\n\nbirthyear\n\n\nhobby\n\n\nweight\n\n\nchildren\n\n\n\n\n\n\nbob\n\n\n1984\n\n\nDancing\n\n\n83\n\n\n3.0\n\n\n\n\ncharles\n\n\n1992\n\n\nNaN\n\n\n112\n\n\n0.0\n\n\n\n\n\n\n마자믹으로 불리언 배열을 전달하여 해당하는 행을 가져올 수 있습니다:\npeople[np.array([True, False, True])]\n\n\n\n\n\n\n\n\n\nbirthyear\n\n\nhobby\n\n\nweight\n\n\nchildren\n\n\n\n\n\n\nalice\n\n\n1985\n\n\nBiking\n\n\n68\n\n\nNaN\n\n\n\n\ncharles\n\n\n1992\n\n\nNaN\n\n\n112\n\n\n0.0\n\n\n\n\n\n\n불리언 표현식을 사용할 때 아주 유용합니다:\npeople[people[\"birthyear\"] < 1990]\n\n\n\n\n\n\n\n\n\nbirthyear\n\n\nhobby\n\n\nweight\n\n\nchildren\n\n\n\n\n\n\nalice\n\n\n1985\n\n\nBiking\n\n\n68\n\n\nNaN\n\n\n\n\nbob\n\n\n1984\n\n\nDancing\n\n\n83\n\n\n3.0"
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#열-추가-삭제",
    "href": "posts/Data_Mining_Pandas/pandas.html#열-추가-삭제",
    "title": "Data_Mining CH2",
    "section": "26 열 추가, 삭제",
    "text": "26 열 추가, 삭제\nDataFrame을 Series의 딕셔너리처럼 다룰 수 있습니다. 따라서 다음 같이 쓸 수 있습니다:\npeople\n\n\n\n\n\n\n\n\n\nbirthyear\n\n\nhobby\n\n\nweight\n\n\nchildren\n\n\n\n\n\n\nalice\n\n\n1985\n\n\nBiking\n\n\n68\n\n\nNaN\n\n\n\n\nbob\n\n\n1984\n\n\nDancing\n\n\n83\n\n\n3.0\n\n\n\n\ncharles\n\n\n1992\n\n\nNaN\n\n\n112\n\n\n0.0\n\n\n\n\n\n\npeople[\"age\"] = 2018 - people[\"birthyear\"]  # \"age\" 열을 추가합니다\npeople[\"over 30\"] = people[\"age\"] > 30      # \"over 30\" 열을 추가합니다\nbirthyears = people.pop(\"birthyear\")\ndel people[\"children\"]\n\npeople\n\n\n\n\n\n\n\n\n\nhobby\n\n\nweight\n\n\nage\n\n\nover 30\n\n\n\n\n\n\nalice\n\n\nBiking\n\n\n68\n\n\n33\n\n\nTrue\n\n\n\n\nbob\n\n\nDancing\n\n\n83\n\n\n34\n\n\nTrue\n\n\n\n\ncharles\n\n\nNaN\n\n\n112\n\n\n26\n\n\nFalse\n\n\n\n\n\n\nbirthyears\nalice      1985\nbob        1984\ncharles    1992\nName: birthyear, dtype: int64\n새로운 열을 추가할 때 행의 개수는 같아야 합니다. 누락된 행은 NaN으로 채워지고 추가적인 행은 무시됩니다:\npeople[\"pets\"] = pd.Series({\"bob\": 0, \"charles\": 5, \"eugene\":1})  # alice 누락됨, eugene은 무시됨\npeople\n\n\n\n\n\n\n\n\n\nhobby\n\n\nweight\n\n\nage\n\n\nover 30\n\n\npets\n\n\n\n\n\n\nalice\n\n\nBiking\n\n\n68\n\n\n33\n\n\nTrue\n\n\nNaN\n\n\n\n\nbob\n\n\nDancing\n\n\n83\n\n\n34\n\n\nTrue\n\n\n0.0\n\n\n\n\ncharles\n\n\nNaN\n\n\n112\n\n\n26\n\n\nFalse\n\n\n5.0\n\n\n\n\n\n\n새로운 열을 추가할 때 기본적으로 (오른쪽) 끝에 추가됩니다. insert() 메서드를 사용해 다른 곳에 열을 추가할 수 있습니다:\npeople.insert(1, \"height\", [172, 181, 185])\npeople\n\n\n\n\n\n\n\n\n\nhobby\n\n\nheight\n\n\nweight\n\n\nage\n\n\nover 30\n\n\npets\n\n\n\n\n\n\nalice\n\n\nBiking\n\n\n172\n\n\n68\n\n\n33\n\n\nTrue\n\n\nNaN\n\n\n\n\nbob\n\n\nDancing\n\n\n181\n\n\n83\n\n\n34\n\n\nTrue\n\n\n0.0\n\n\n\n\ncharles\n\n\nNaN\n\n\n185\n\n\n112\n\n\n26\n\n\nFalse\n\n\n5.0"
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#새로운-열-할당하기",
    "href": "posts/Data_Mining_Pandas/pandas.html#새로운-열-할당하기",
    "title": "Data_Mining CH2",
    "section": "27 새로운 열 할당하기",
    "text": "27 새로운 열 할당하기\nassign() 메서드를 호출하여 새로운 열을 만들 수도 있습니다. 이는 새로운 DataFrame 객체를 반환하며 원본 객체는 변경되지 않습니다:\npeople.assign(\n    body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2,\n    has_pets = people[\"pets\"] > 0\n)\n\n\n\n\n\n\n\n\n\nhobby\n\n\nheight\n\n\nweight\n\n\nage\n\n\nover 30\n\n\npets\n\n\nbody_mass_index\n\n\nhas_pets\n\n\n\n\n\n\nalice\n\n\nBiking\n\n\n172\n\n\n68\n\n\n33\n\n\nTrue\n\n\nNaN\n\n\n22.985398\n\n\nFalse\n\n\n\n\nbob\n\n\nDancing\n\n\n181\n\n\n83\n\n\n34\n\n\nTrue\n\n\n0.0\n\n\n25.335002\n\n\nFalse\n\n\n\n\ncharles\n\n\nNaN\n\n\n185\n\n\n112\n\n\n26\n\n\nFalse\n\n\n5.0\n\n\n32.724617\n\n\nTrue\n\n\n\n\n\n\n할당문 안에서 만든 열은 접근할 수 없습니다:\ntry:\n    people.assign(\n        body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2,\n        overweight = people[\"body_mass_index\"] > 25\n    )\nexcept KeyError as e:\n    print(\"키 에러:\", e)\n키 에러: 'body_mass_index'\n해결책은 두 개의 연속된 할당문으로 나누는 것입니다:\nd6 = people.assign(body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2)\nd6.assign(overweight = d6[\"body_mass_index\"] > 25)\n\n\n\n\n\n\n\n\n\nhobby\n\n\nheight\n\n\nweight\n\n\nage\n\n\nover 30\n\n\npets\n\n\nbody_mass_index\n\n\noverweight\n\n\n\n\n\n\nalice\n\n\nBiking\n\n\n172\n\n\n68\n\n\n33\n\n\nTrue\n\n\nNaN\n\n\n22.985398\n\n\nFalse\n\n\n\n\nbob\n\n\nDancing\n\n\n181\n\n\n83\n\n\n34\n\n\nTrue\n\n\n0.0\n\n\n25.335002\n\n\nTrue\n\n\n\n\ncharles\n\n\nNaN\n\n\n185\n\n\n112\n\n\n26\n\n\nFalse\n\n\n5.0\n\n\n32.724617\n\n\nTrue\n\n\n\n\n\n\n임시 변수 d6를 만들면 불편합니다. assign() 메서드를 연결하고 싶겠지만 people 객체가 첫 번째 할당문에서 실제로 수정되지 않기 때문에 작동하지 않습니다:\ntry:\n    (people\n         .assign(body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2)\n         .assign(overweight = people[\"body_mass_index\"] > 25)\n    )\nexcept KeyError as e:\n    print(\"키 에러:\", e)\n키 에러: 'body_mass_index'\n하지만 걱정하지 마세요. 간단한 방법이 있습니다. assign() 메서드에 함수(전형적으로 lambda 함수)를 전달하면 DataFrame을 매개변수로 이 함수를 호출할 것입니다:\n(people\n     .assign(body_mass_index = lambda df: df[\"weight\"] / (df[\"height\"] / 100) ** 2)\n     .assign(overweight = lambda df: df[\"body_mass_index\"] > 25)\n)\n\n\n\n\n\n\n\n\n\nhobby\n\n\nheight\n\n\nweight\n\n\nage\n\n\nover 30\n\n\npets\n\n\nbody_mass_index\n\n\noverweight\n\n\n\n\n\n\nalice\n\n\nBiking\n\n\n172\n\n\n68\n\n\n33\n\n\nTrue\n\n\nNaN\n\n\n22.985398\n\n\nFalse\n\n\n\n\nbob\n\n\nDancing\n\n\n181\n\n\n83\n\n\n34\n\n\nTrue\n\n\n0.0\n\n\n25.335002\n\n\nTrue\n\n\n\n\ncharles\n\n\nNaN\n\n\n185\n\n\n112\n\n\n26\n\n\nFalse\n\n\n5.0\n\n\n32.724617\n\n\nTrue\n\n\n\n\n\n\n문제가 해결되었군요!"
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#표현식-평가",
    "href": "posts/Data_Mining_Pandas/pandas.html#표현식-평가",
    "title": "Data_Mining CH2",
    "section": "28 표현식 평가",
    "text": "28 표현식 평가\n판다스가 제공하는 뛰어난 기능 하나는 표현식 평가입니다. 이는 numexpr 라이브러리에 의존하기 때문에 설치가 되어 있어야 합니다.\npeople.eval(\"weight / (height/100) ** 2 > 25\")\nalice      False\nbob         True\ncharles     True\ndtype: bool\n할당 표현식도 지원됩니다. inplace=True로 지정하면 수정된 복사본을 만들지 않고 바로 DataFrame을 변경합니다:\npeople.eval(\"body_mass_index = weight / (height/100) ** 2\", inplace=True)\npeople\n\n\n\n\n\n\n\n\n\nhobby\n\n\nheight\n\n\nweight\n\n\nage\n\n\nover 30\n\n\npets\n\n\nbody_mass_index\n\n\n\n\n\n\nalice\n\n\nBiking\n\n\n172\n\n\n68\n\n\n33\n\n\nTrue\n\n\nNaN\n\n\n22.985398\n\n\n\n\nbob\n\n\nDancing\n\n\n181\n\n\n83\n\n\n34\n\n\nTrue\n\n\n0.0\n\n\n25.335002\n\n\n\n\ncharles\n\n\nNaN\n\n\n185\n\n\n112\n\n\n26\n\n\nFalse\n\n\n5.0\n\n\n32.724617\n\n\n\n\n\n\n'@'를 접두어로 사용하여 지역 변수나 전역 변수를 참조할 수 있습니다:\noverweight_threshold = 30\npeople.eval(\"overweight = body_mass_index > @overweight_threshold\", inplace=True)\npeople\n\n\n\n\n\n\n\n\n\nhobby\n\n\nheight\n\n\nweight\n\n\nage\n\n\nover 30\n\n\npets\n\n\nbody_mass_index\n\n\noverweight\n\n\n\n\n\n\nalice\n\n\nBiking\n\n\n172\n\n\n68\n\n\n33\n\n\nTrue\n\n\nNaN\n\n\n22.985398\n\n\nFalse\n\n\n\n\nbob\n\n\nDancing\n\n\n181\n\n\n83\n\n\n34\n\n\nTrue\n\n\n0.0\n\n\n25.335002\n\n\nFalse\n\n\n\n\ncharles\n\n\nNaN\n\n\n185\n\n\n112\n\n\n26\n\n\nFalse\n\n\n5.0\n\n\n32.724617\n\n\nTrue"
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#dataframe-쿼리하기",
    "href": "posts/Data_Mining_Pandas/pandas.html#dataframe-쿼리하기",
    "title": "Data_Mining CH2",
    "section": "29 DataFrame 쿼리하기",
    "text": "29 DataFrame 쿼리하기\nquery() 메서드를 사용하면 쿼리 표현식에 기반하여 DataFrame을 필터링할 수 있습니다:\npeople.query(\"age > 30 and pets == 0\")\n\n\n\n\n\n\n\n\n\nhobby\n\n\nheight\n\n\nweight\n\n\nage\n\n\nover 30\n\n\npets\n\n\nbody_mass_index\n\n\noverweight\n\n\n\n\n\n\nbob\n\n\nDancing\n\n\n181\n\n\n83\n\n\n34\n\n\nTrue\n\n\n0.0\n\n\n25.335002\n\n\nFalse"
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#dataframe-정렬",
    "href": "posts/Data_Mining_Pandas/pandas.html#dataframe-정렬",
    "title": "Data_Mining CH2",
    "section": "30 DataFrame 정렬",
    "text": "30 DataFrame 정렬\nsort_index 메서드를 호출하여 DataFrame을 정렬할 수 있습니다. 기본적으로 인덱스 레이블을 기준으로 오름차순으로 행을 정렬합니다. 여기에서는 내림차순으로 정렬해 보죠:\npeople.sort_index(ascending=False)\n\n\n\n\n\n\n\n\n\nhobby\n\n\nheight\n\n\nweight\n\n\nage\n\n\nover 30\n\n\npets\n\n\nbody_mass_index\n\n\noverweight\n\n\n\n\n\n\ncharles\n\n\nNaN\n\n\n185\n\n\n112\n\n\n26\n\n\nFalse\n\n\n5.0\n\n\n32.724617\n\n\nTrue\n\n\n\n\nbob\n\n\nDancing\n\n\n181\n\n\n83\n\n\n34\n\n\nTrue\n\n\n0.0\n\n\n25.335002\n\n\nFalse\n\n\n\n\nalice\n\n\nBiking\n\n\n172\n\n\n68\n\n\n33\n\n\nTrue\n\n\nNaN\n\n\n22.985398\n\n\nFalse\n\n\n\n\n\n\nsort_index는 DataFrame의 정렬된 복사본을 반환합니다. people을 직접 수정하려면 inplace 매개변수를 True로 지정합니다. 또한 axis=1로 지정하여 열 대신 행을 정렬할 수 있습니다:\npeople.sort_index(axis=1, inplace=True)\npeople\n\n\n\n\n\n\n\n\n\nage\n\n\nbody_mass_index\n\n\nheight\n\n\nhobby\n\n\nover 30\n\n\noverweight\n\n\npets\n\n\nweight\n\n\n\n\n\n\nalice\n\n\n33\n\n\n22.985398\n\n\n172\n\n\nBiking\n\n\nTrue\n\n\nFalse\n\n\nNaN\n\n\n68\n\n\n\n\nbob\n\n\n34\n\n\n25.335002\n\n\n181\n\n\nDancing\n\n\nTrue\n\n\nFalse\n\n\n0.0\n\n\n83\n\n\n\n\ncharles\n\n\n26\n\n\n32.724617\n\n\n185\n\n\nNaN\n\n\nFalse\n\n\nTrue\n\n\n5.0\n\n\n112\n\n\n\n\n\n\n레이블이 아니라 값을 기준으로 DataFrame을 정렬하려면 sort_values에 정렬하려는 열을 지정합니다:\npeople.sort_values(by=\"age\", inplace=True)\npeople\n\n\n\n\n\n\n\n\n\nage\n\n\nbody_mass_index\n\n\nheight\n\n\nhobby\n\n\nover 30\n\n\noverweight\n\n\npets\n\n\nweight\n\n\n\n\n\n\ncharles\n\n\n26\n\n\n32.724617\n\n\n185\n\n\nNaN\n\n\nFalse\n\n\nTrue\n\n\n5.0\n\n\n112\n\n\n\n\nalice\n\n\n33\n\n\n22.985398\n\n\n172\n\n\nBiking\n\n\nTrue\n\n\nFalse\n\n\nNaN\n\n\n68\n\n\n\n\nbob\n\n\n34\n\n\n25.335002\n\n\n181\n\n\nDancing\n\n\nTrue\n\n\nFalse\n\n\n0.0\n\n\n83"
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#dataframe-그래프-그리기",
    "href": "posts/Data_Mining_Pandas/pandas.html#dataframe-그래프-그리기",
    "title": "Data_Mining CH2",
    "section": "31 DataFrame 그래프 그리기",
    "text": "31 DataFrame 그래프 그리기\nSeries와 마찬가지로 판다스는 DataFrame 기반으로 멋진 그래프를 손쉽게 그릴 수 있습니다.\n예를 들어 plot 메서드를 호출하여 DataFrame의 데이터에서 선 그래프를 쉽게 그릴 수 있습니다:\npeople.plot(kind = \"line\", x = \"body_mass_index\", y = [\"height\", \"weight\"])\nplt.show()\n\n\n\npng\n\n\n맷플롯립의 함수가 지원하는 다른 매개변수를 사용할 수 있습니다. 예를 들어, 산점도를 그릴 때 맷플롯립의 scatter() 함수의 s 매개변수를 사용해 크기를 지정할 수 있습니다:\npeople.plot(kind = \"scatter\", x = \"height\", y = \"weight\", s=[40, 120, 200])\nplt.show()\n\n\n\npng\n\n\n선택할 수 있는 옵션이 많습니다. 판다스 문서의 시각화 페이지에서 마음에 드는 그래프를 찾아 예제 코드를 살펴 보세요."
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#dataframe-연산",
    "href": "posts/Data_Mining_Pandas/pandas.html#dataframe-연산",
    "title": "Data_Mining CH2",
    "section": "32 DataFrame 연산",
    "text": "32 DataFrame 연산\nDataFrame이 넘파이 배열을 흉내내려는 것은 아니지만 몇 가지 비슷한 점이 있습니다. 예제 DataFrame을 만들어 보죠:\ngrades_array = np.array([[8,8,9],[10,9,9],[4, 8, 2], [9, 10, 10]])\ngrades = pd.DataFrame(grades_array, columns=[\"sep\", \"oct\", \"nov\"], index=[\"alice\",\"bob\",\"charles\",\"darwin\"])\ngrades\n\n\n\n\n\n\n\n\n\nsep\n\n\noct\n\n\nnov\n\n\n\n\n\n\nalice\n\n\n8\n\n\n8\n\n\n9\n\n\n\n\nbob\n\n\n10\n\n\n9\n\n\n9\n\n\n\n\ncharles\n\n\n4\n\n\n8\n\n\n2\n\n\n\n\ndarwin\n\n\n9\n\n\n10\n\n\n10\n\n\n\n\n\n\nDataFrame에 넘파이 수학 함수를 적용하면 모든 값에 이 함수가 적용됩니다:\nnp.sqrt(grades)\n\n\n\n\n\n\n\n\n\nsep\n\n\noct\n\n\nnov\n\n\n\n\n\n\nalice\n\n\n2.828427\n\n\n2.828427\n\n\n3.000000\n\n\n\n\nbob\n\n\n3.162278\n\n\n3.000000\n\n\n3.000000\n\n\n\n\ncharles\n\n\n2.000000\n\n\n2.828427\n\n\n1.414214\n\n\n\n\ndarwin\n\n\n3.000000\n\n\n3.162278\n\n\n3.162278\n\n\n\n\n\n\n비슷하게 DataFrame에 하나의 값을 더하면 DataFrame의 모든 원소에 이 값이 더해집니다. 이를 브로드캐스팅이라고 합니다:\ngrades + 1\n\n\n\n\n\n\n\n\n\nsep\n\n\noct\n\n\nnov\n\n\n\n\n\n\nalice\n\n\n9\n\n\n9\n\n\n10\n\n\n\n\nbob\n\n\n11\n\n\n10\n\n\n10\n\n\n\n\ncharles\n\n\n5\n\n\n9\n\n\n3\n\n\n\n\ndarwin\n\n\n10\n\n\n11\n\n\n11\n\n\n\n\n\n\n물론 산술 연산(*,/,**…)과 조건 연산(>, ==…)을 포함해 모든 이항 연산에도 마찬가지 입니다:\ngrades >= 5\n\n\n\n\n\n\n\n\n\nsep\n\n\noct\n\n\nnov\n\n\n\n\n\n\nalice\n\n\nTrue\n\n\nTrue\n\n\nTrue\n\n\n\n\nbob\n\n\nTrue\n\n\nTrue\n\n\nTrue\n\n\n\n\ncharles\n\n\nFalse\n\n\nTrue\n\n\nFalse\n\n\n\n\ndarwin\n\n\nTrue\n\n\nTrue\n\n\nTrue\n\n\n\n\n\n\nDataFrame의 max, sum, mean 같은 집계 연산은 각 열에 적용되어 Series 객체가 반환됩니다:\ngrades.mean()\nsep    7.75\noct    8.75\nnov    7.50\ndtype: float64\nall 메서드도 집계 연산입니다: 모든 값이 True인지 아닌지 확인합니다. 모든 학생의 점수가 5 이상인 월을 찾아 보죠:\n(grades > 5).all()\nsep    False\noct     True\nnov    False\ndtype: bool\nMost of these functions take an optional axis parameter which lets you specify along which axis of the DataFrame you want the operation executed. The default is axis=0, meaning that the operation is executed vertically (on each column). You can set axis=1 to execute the operation horizontally (on each row). For example, let’s find out which students had all grades greater than 5:\n(grades > 5).all(axis = 1)\nalice       True\nbob         True\ncharles    False\ndarwin      True\ndtype: bool\nany 메서드는 하나라도 참이면 True를 반환합니다. 한 번이라도 10점을 받은 사람을 찾아 보죠:\n(grades == 10).any(axis = 1)\nalice      False\nbob         True\ncharles    False\ndarwin      True\ndtype: bool\nDataFrame에 Series 객체를 더하면 (또는 다른 이항 연산을 수행하면) 판다스는 DataFrame에 있는 모든 행에 이 연산을 브로드캐스팅합니다. 이는 Series 객체가 DataFrame의 행의 개수와 크기가 같을 때만 동작합니다. 예를 들어 DataFrame의 mean(Series 객체)을 빼보죠:\ngrades - grades.mean()  # grades - [7.75, 8.75, 7.50] 와 동일\n\n\n\n\n\n\n\n\n\nsep\n\n\noct\n\n\nnov\n\n\n\n\n\n\nalice\n\n\n0.25\n\n\n-0.75\n\n\n1.5\n\n\n\n\nbob\n\n\n2.25\n\n\n0.25\n\n\n1.5\n\n\n\n\ncharles\n\n\n-3.75\n\n\n-0.75\n\n\n-5.5\n\n\n\n\ndarwin\n\n\n1.25\n\n\n1.25\n\n\n2.5\n\n\n\n\n\n\n모든 9월 성적에서 7.75를 빼고, 10월 성적에서 8.75를 빼고, 11월 성적에서 7.50을 뺍니다. 이는 다음 DataFrame을 빼는 것과 같습니다:\npd.DataFrame([[7.75, 8.75, 7.50]]*4, index=grades.index, columns=grades.columns)\n\n\n\n\n\n\n\n\n\nsep\n\n\noct\n\n\nnov\n\n\n\n\n\n\nalice\n\n\n7.75\n\n\n8.75\n\n\n7.5\n\n\n\n\nbob\n\n\n7.75\n\n\n8.75\n\n\n7.5\n\n\n\n\ncharles\n\n\n7.75\n\n\n8.75\n\n\n7.5\n\n\n\n\ndarwin\n\n\n7.75\n\n\n8.75\n\n\n7.5\n\n\n\n\n\n\n모든 성적의 전체 평균을 빼고 싶다면 다음과 같은 방법을 사용합니다:\ngrades - grades.values.mean() # 모든 점수에서 전체 평균(8.00)을 뺍니다\n\n\n\n\n\n\n\n\n\nsep\n\n\noct\n\n\nnov\n\n\n\n\n\n\nalice\n\n\n0.0\n\n\n0.0\n\n\n1.0\n\n\n\n\nbob\n\n\n2.0\n\n\n1.0\n\n\n1.0\n\n\n\n\ncharles\n\n\n-4.0\n\n\n0.0\n\n\n-6.0\n\n\n\n\ndarwin\n\n\n1.0\n\n\n2.0\n\n\n2.0"
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#자동-정렬-1",
    "href": "posts/Data_Mining_Pandas/pandas.html#자동-정렬-1",
    "title": "Data_Mining CH2",
    "section": "33 자동 정렬",
    "text": "33 자동 정렬\nSeries와 비슷하게 여러 개의 DataFrame에 대한 연산을 수행하면 판다스는 자동으로 행 인덱스 레이블로 정렬하지만 열 이름으로도 정렬할 수 있습니다. 10월부터 12월까지 보너스 포인트를 담은 DataFrame을 만들어 보겠습니다:\nbonus_array = np.array([[0,np.nan,2],[np.nan,1,0],[0, 1, 0], [3, 3, 0]])\nbonus_points = pd.DataFrame(bonus_array, columns=[\"oct\", \"nov\", \"dec\"], index=[\"bob\",\"colin\", \"darwin\", \"charles\"])\nbonus_points\n\n\n\n\n\n\n\n\n\noct\n\n\nnov\n\n\ndec\n\n\n\n\n\n\nbob\n\n\n0.0\n\n\nNaN\n\n\n2.0\n\n\n\n\ncolin\n\n\nNaN\n\n\n1.0\n\n\n0.0\n\n\n\n\ndarwin\n\n\n0.0\n\n\n1.0\n\n\n0.0\n\n\n\n\ncharles\n\n\n3.0\n\n\n3.0\n\n\n0.0\n\n\n\n\n\n\ngrades + bonus_points\n\n\n\n\n\n\n\n\n\ndec\n\n\nnov\n\n\noct\n\n\nsep\n\n\n\n\n\n\nalice\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\nbob\n\n\nNaN\n\n\nNaN\n\n\n9.0\n\n\nNaN\n\n\n\n\ncharles\n\n\nNaN\n\n\n5.0\n\n\n11.0\n\n\nNaN\n\n\n\n\ncolin\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\ndarwin\n\n\nNaN\n\n\n11.0\n\n\n10.0\n\n\nNaN\n\n\n\n\n\n\n덧셈 연산이 수행되었지만 너무 많은 원소가 NaN이 되었습니다. DataFrame을 정렬할 때 일부 열과 행이 한 쪽에만 있기 때문입니다. 다른 쪽에는 누란되었다고 간주합니다(NaN). NaN에 어떤 수를 더하면 NaN이 됩니다."
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#누락된-데이터-다루기",
    "href": "posts/Data_Mining_Pandas/pandas.html#누락된-데이터-다루기",
    "title": "Data_Mining CH2",
    "section": "34 누락된 데이터 다루기",
    "text": "34 누락된 데이터 다루기\n실제 데이터에서 누락된 데이터를 다루는 경우는 자주 발생합니다. 판다스는 누락된 데이터를 다룰 수 있는 몇 가지 방법을 제공합니다.\n위 데이터에 있는 문제를 해결해 보죠. 예를 들어, 누락된 데이터는 NaN이 아니라 0이 되어야 한다고 결정할 수 있습니다. fillna() 메서드를 사용해 모든 NaN 값을 어떤 값으로 바꿀 수 있습니다:\n(grades + bonus_points).fillna(0)\n\n\n\n\n\n\n\n\n\ndec\n\n\nnov\n\n\noct\n\n\nsep\n\n\n\n\n\n\nalice\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\nbob\n\n\n0.0\n\n\n0.0\n\n\n9.0\n\n\n0.0\n\n\n\n\ncharles\n\n\n0.0\n\n\n5.0\n\n\n11.0\n\n\n0.0\n\n\n\n\ncolin\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\ndarwin\n\n\n0.0\n\n\n11.0\n\n\n10.0\n\n\n0.0\n\n\n\n\n\n\n9월의 점수를 0으로 만드는 것은 공정하지 않습니다. 누락된 점수는 그대로 두고, 누락된 보너스 포인트는 0으로 바꿀 수 있습니다:\nfixed_bonus_points = bonus_points.fillna(0)\nfixed_bonus_points.insert(0, \"sep\", 0)\nfixed_bonus_points.loc[\"alice\"] = 0\ngrades + fixed_bonus_points\n\n\n\n\n\n\n\n\n\ndec\n\n\nnov\n\n\noct\n\n\nsep\n\n\n\n\n\n\nalice\n\n\nNaN\n\n\n9.0\n\n\n8.0\n\n\n8.0\n\n\n\n\nbob\n\n\nNaN\n\n\n9.0\n\n\n9.0\n\n\n10.0\n\n\n\n\ncharles\n\n\nNaN\n\n\n5.0\n\n\n11.0\n\n\n4.0\n\n\n\n\ncolin\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\ndarwin\n\n\nNaN\n\n\n11.0\n\n\n10.0\n\n\n9.0\n\n\n\n\n\n\n훨씬 낫네요: 일부 데이터를 꾸며냈지만 덜 불공정합니다.\n누락된 값을 다루는 또 다른 방법은 보간입니다. bonus_points DataFrame을 다시 보죠:\nbonus_points\n\n\n\n\n\n\n\n\n\noct\n\n\nnov\n\n\ndec\n\n\n\n\n\n\nbob\n\n\n0.0\n\n\nNaN\n\n\n2.0\n\n\n\n\ncolin\n\n\nNaN\n\n\n1.0\n\n\n0.0\n\n\n\n\ndarwin\n\n\n0.0\n\n\n1.0\n\n\n0.0\n\n\n\n\ncharles\n\n\n3.0\n\n\n3.0\n\n\n0.0\n\n\n\n\n\n\ninterpolate 메서드를 사용해 보죠. 기본적으로 수직 방향(axis=0)으로 보간합니다. 따라서 수평으로(axis=1)으로 보간하도록 지정합니다.\nbonus_points.interpolate(axis=1)\n\n\n\n\n\n\n\n\n\noct\n\n\nnov\n\n\ndec\n\n\n\n\n\n\nbob\n\n\n0.0\n\n\n1.0\n\n\n2.0\n\n\n\n\ncolin\n\n\nNaN\n\n\n1.0\n\n\n0.0\n\n\n\n\ndarwin\n\n\n0.0\n\n\n1.0\n\n\n0.0\n\n\n\n\ncharles\n\n\n3.0\n\n\n3.0\n\n\n0.0\n\n\n\n\n\n\nbob의 보너스 포인트는 10월에 0이고 12월에 2입니다. 11월을 보간하면 평균 보너스 포인트 1을 얻습니다. colin의 보너스 포인트는 11월에 1이지만 9월에 포인트는 얼마인지 모릅니다. 따라서 보간할 수 없고 10월의 포인트는 그대로 누락된 값으로 남아 있습니다. 이를 해결하려면 보간하기 전에 9월의 보너스 포인트를 0으로 설정해야 합니다.\nbetter_bonus_points = bonus_points.copy()\nbetter_bonus_points.insert(0, \"sep\", 0)\nbetter_bonus_points.loc[\"alice\"] = 0\nbetter_bonus_points = better_bonus_points.interpolate(axis=1)\nbetter_bonus_points\n\n\n\n\n\n\n\n\n\nsep\n\n\noct\n\n\nnov\n\n\ndec\n\n\n\n\n\n\nbob\n\n\n0.0\n\n\n0.0\n\n\n1.0\n\n\n2.0\n\n\n\n\ncolin\n\n\n0.0\n\n\n0.5\n\n\n1.0\n\n\n0.0\n\n\n\n\ndarwin\n\n\n0.0\n\n\n0.0\n\n\n1.0\n\n\n0.0\n\n\n\n\ncharles\n\n\n0.0\n\n\n3.0\n\n\n3.0\n\n\n0.0\n\n\n\n\nalice\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n0.0\n\n\n\n\n\n\n좋습니다. 이제 모든 보너스 포인트가 합리적으로 보간되었습니다. 최종 점수를 확인해 보죠:\ngrades + better_bonus_points\n\n\n\n\n\n\n\n\n\ndec\n\n\nnov\n\n\noct\n\n\nsep\n\n\n\n\n\n\nalice\n\n\nNaN\n\n\n9.0\n\n\n8.0\n\n\n8.0\n\n\n\n\nbob\n\n\nNaN\n\n\n10.0\n\n\n9.0\n\n\n10.0\n\n\n\n\ncharles\n\n\nNaN\n\n\n5.0\n\n\n11.0\n\n\n4.0\n\n\n\n\ncolin\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\ndarwin\n\n\nNaN\n\n\n11.0\n\n\n10.0\n\n\n9.0\n\n\n\n\n\n\n9월 열이 오른쪽에 추가되었는데 좀 이상합니다. 이는 더하려는 DataFrame이 정확히 같은 열을 가지고 있지 않기 때문입니다(grade DataFrame에는 \"dec\" 열이 없습니다). 따라서 판다스는 알파벳 순서로 최종 열을 정렬합니다. 이를 해결하려면 덧셈을 하기 전에 누락된 열을 추가하면 됩니다:\ngrades[\"dec\"] = np.nan\nfinal_grades = grades + better_bonus_points\nfinal_grades\n\n\n\n\n\n\n\n\n\nsep\n\n\noct\n\n\nnov\n\n\ndec\n\n\n\n\n\n\nalice\n\n\n8.0\n\n\n8.0\n\n\n9.0\n\n\nNaN\n\n\n\n\nbob\n\n\n10.0\n\n\n9.0\n\n\n10.0\n\n\nNaN\n\n\n\n\ncharles\n\n\n4.0\n\n\n11.0\n\n\n5.0\n\n\nNaN\n\n\n\n\ncolin\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\ndarwin\n\n\n9.0\n\n\n10.0\n\n\n11.0\n\n\nNaN\n\n\n\n\n\n\n12월과 colin에 대해 할 수 있는 것이 많지 않습니다. 보너스 포인트를 만드는 것이 나쁘지만 점수를 합리적으로 올릴 수는 없습니다(어떤 선생님들은 그럴 수 있지만). dropna() 메서드를 사용해 모두 NaN인 행을 삭제합니다:\nfinal_grades_clean = final_grades.dropna(how=\"all\")\nfinal_grades_clean\n\n\n\n\n\n\n\n\n\nsep\n\n\noct\n\n\nnov\n\n\ndec\n\n\n\n\n\n\nalice\n\n\n8.0\n\n\n8.0\n\n\n9.0\n\n\nNaN\n\n\n\n\nbob\n\n\n10.0\n\n\n9.0\n\n\n10.0\n\n\nNaN\n\n\n\n\ncharles\n\n\n4.0\n\n\n11.0\n\n\n5.0\n\n\nNaN\n\n\n\n\ndarwin\n\n\n9.0\n\n\n10.0\n\n\n11.0\n\n\nNaN\n\n\n\n\n\n\n그다음 axis 매개변수를 1로 지정하여 모두 NaN인 열을 삭제합니다:\nfinal_grades_clean = final_grades_clean.dropna(axis=1, how=\"all\")\nfinal_grades_clean\n\n\n\n\n\n\n\n\n\nsep\n\n\noct\n\n\nnov\n\n\n\n\n\n\nalice\n\n\n8.0\n\n\n8.0\n\n\n9.0\n\n\n\n\nbob\n\n\n10.0\n\n\n9.0\n\n\n10.0\n\n\n\n\ncharles\n\n\n4.0\n\n\n11.0\n\n\n5.0\n\n\n\n\ndarwin\n\n\n9.0\n\n\n10.0\n\n\n11.0"
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#groupby로-집계하기",
    "href": "posts/Data_Mining_Pandas/pandas.html#groupby로-집계하기",
    "title": "Data_Mining CH2",
    "section": "35 groupby로 집계하기",
    "text": "35 groupby로 집계하기\nSQL과 비슷하게 판다스는 데이터를 그룹핑하고 각 그룹에 대해 연산을 수행할 수 있습니다.\n먼저 그루핑을 위해 각 사람의 데이터를 추가로 만들겠습니다. NaN 값을 어떻게 다루는지 보기 위해 final_grades DataFrame을 다시 사용하겠습니다:\nfinal_grades[\"hobby\"] = [\"Biking\", \"Dancing\", np.nan, \"Dancing\", \"Biking\"]\nfinal_grades\n\n\n\n\n\n\n\n\n\nsep\n\n\noct\n\n\nnov\n\n\ndec\n\n\nhobby\n\n\n\n\n\n\nalice\n\n\n8.0\n\n\n8.0\n\n\n9.0\n\n\nNaN\n\n\nBiking\n\n\n\n\nbob\n\n\n10.0\n\n\n9.0\n\n\n10.0\n\n\nNaN\n\n\nDancing\n\n\n\n\ncharles\n\n\n4.0\n\n\n11.0\n\n\n5.0\n\n\nNaN\n\n\nNaN\n\n\n\n\ncolin\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nDancing\n\n\n\n\ndarwin\n\n\n9.0\n\n\n10.0\n\n\n11.0\n\n\nNaN\n\n\nBiking\n\n\n\n\n\n\nhobby로 이 DataFrame을 그룹핑해 보죠:\ngrouped_grades = final_grades.groupby(\"hobby\")\ngrouped_grades\n<pandas.core.groupby.generic.DataFrameGroupBy object at 0x000001F6917A6250>\n이제 hobby마다 평균 점수를 계산할 수 있습니다:\ngrouped_grades.mean()\n\n\n\n\n\n\n\n\n\nsep\n\n\noct\n\n\nnov\n\n\ndec\n\n\n\n\nhobby\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBiking\n\n\n8.5\n\n\n9.0\n\n\n10.0\n\n\nNaN\n\n\n\n\nDancing\n\n\n10.0\n\n\n9.0\n\n\n10.0\n\n\nNaN\n\n\n\n\n\n\n아주 쉽네요! 평균을 계산할 때 NaN 값은 그냥 무시됩니다."
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#피봇-테이블",
    "href": "posts/Data_Mining_Pandas/pandas.html#피봇-테이블",
    "title": "Data_Mining CH2",
    "section": "36 피봇 테이블",
    "text": "36 피봇 테이블\n판다스는 스프레드시트와 비슷하 피봇 테이블을 지원하여 데이터를 빠르게 요약할 수 있습니다. 어떻게 동작하는 알아 보기 위해 간단한 DataFrame을 만들어 보죠:\nbonus_points\n\n\n\n\n\n\n\n\n\noct\n\n\nnov\n\n\ndec\n\n\n\n\n\n\nbob\n\n\n0.0\n\n\nNaN\n\n\n2.0\n\n\n\n\ncolin\n\n\nNaN\n\n\n1.0\n\n\n0.0\n\n\n\n\ndarwin\n\n\n0.0\n\n\n1.0\n\n\n0.0\n\n\n\n\ncharles\n\n\n3.0\n\n\n3.0\n\n\n0.0\n\n\n\n\n\n\nmore_grades = final_grades_clean.stack().reset_index()\nmore_grades.columns = [\"name\", \"month\", \"grade\"]\nmore_grades[\"bonus\"] = [np.nan, np.nan, np.nan, 0, np.nan, 2, 3, 3, 0, 0, 1, 0]\nmore_grades\n\n\n\n\n\n\n\n\n\nname\n\n\nmonth\n\n\ngrade\n\n\nbonus\n\n\n\n\n\n\n0\n\n\nalice\n\n\nsep\n\n\n8.0\n\n\nNaN\n\n\n\n\n1\n\n\nalice\n\n\noct\n\n\n8.0\n\n\nNaN\n\n\n\n\n2\n\n\nalice\n\n\nnov\n\n\n9.0\n\n\nNaN\n\n\n\n\n3\n\n\nbob\n\n\nsep\n\n\n10.0\n\n\n0.0\n\n\n\n\n4\n\n\nbob\n\n\noct\n\n\n9.0\n\n\nNaN\n\n\n\n\n5\n\n\nbob\n\n\nnov\n\n\n10.0\n\n\n2.0\n\n\n\n\n6\n\n\ncharles\n\n\nsep\n\n\n4.0\n\n\n3.0\n\n\n\n\n7\n\n\ncharles\n\n\noct\n\n\n11.0\n\n\n3.0\n\n\n\n\n8\n\n\ncharles\n\n\nnov\n\n\n5.0\n\n\n0.0\n\n\n\n\n9\n\n\ndarwin\n\n\nsep\n\n\n9.0\n\n\n0.0\n\n\n\n\n10\n\n\ndarwin\n\n\noct\n\n\n10.0\n\n\n1.0\n\n\n\n\n11\n\n\ndarwin\n\n\nnov\n\n\n11.0\n\n\n0.0\n\n\n\n\n\n\n이제 이 DataFrame에 대해 pd.pivot_table() 함수를 호출하고 name 열로 그룹핑합니다. 기본적으로 pivot_table()은 수치 열의 평균을 계산합니다:\npd.pivot_table(more_grades, index=\"name\")\nC:\\Users\\seong taek\\AppData\\Local\\Temp\\ipykernel_9428\\584684106.py:1: FutureWarning: pivot_table dropped a column because it failed to aggregate. This behavior is deprecated and will raise in a future version of pandas. Select only the columns that can be aggregated.\n  pd.pivot_table(more_grades, index=\"name\")\n\n\n\n\n\n\n\n\n\nbonus\n\n\ngrade\n\n\n\n\nname\n\n\n\n\n\n\n\n\n\n\nalice\n\n\nNaN\n\n\n8.333333\n\n\n\n\nbob\n\n\n1.000000\n\n\n9.666667\n\n\n\n\ncharles\n\n\n2.000000\n\n\n6.666667\n\n\n\n\ndarwin\n\n\n0.333333\n\n\n10.000000\n\n\n\n\n\n\n집계 함수를 aggfunc 매개변수로 바꿀 수 있습니다. 또한 집계 대상의 열을 리스트로 지정할 수 있습니다:\npd.pivot_table(more_grades, index=\"name\", values=[\"grade\",\"bonus\"], aggfunc=np.max)\n\n\n\n\n\n\n\n\n\nbonus\n\n\ngrade\n\n\n\n\nname\n\n\n\n\n\n\n\n\n\n\nalice\n\n\nNaN\n\n\n9.0\n\n\n\n\nbob\n\n\n2.0\n\n\n10.0\n\n\n\n\ncharles\n\n\n3.0\n\n\n11.0\n\n\n\n\ndarwin\n\n\n1.0\n\n\n11.0\n\n\n\n\n\n\ncolumns 매개변수를 지정하여 수평으로 집계할 수 있고 margins=True로 설정해 각 행과 열에 대해 전체 합을 계산할 수 있습니다:\npd.pivot_table(more_grades, index=\"name\", values=\"grade\", columns=\"month\", margins=True)\n\n\n\n\n\n\n\nmonth\n\n\nnov\n\n\noct\n\n\nsep\n\n\nAll\n\n\n\n\nname\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nalice\n\n\n9.00\n\n\n8.0\n\n\n8.00\n\n\n8.333333\n\n\n\n\nbob\n\n\n10.00\n\n\n9.0\n\n\n10.00\n\n\n9.666667\n\n\n\n\ncharles\n\n\n5.00\n\n\n11.0\n\n\n4.00\n\n\n6.666667\n\n\n\n\ndarwin\n\n\n11.00\n\n\n10.0\n\n\n9.00\n\n\n10.000000\n\n\n\n\nAll\n\n\n8.75\n\n\n9.5\n\n\n7.75\n\n\n8.666667\n\n\n\n\n\n\n마지막으로 여러 개의 인덱스나 열 이름을 지정하면 판다스가 다중 레벨 인덱스를 만듭니다:\npd.pivot_table(more_grades, index=(\"name\", \"month\"), margins=True)\n\n\n\n\n\n\n\n\n\n\n\nbonus\n\n\ngrade\n\n\n\n\nname\n\n\nmonth\n\n\n\n\n\n\n\n\n\n\nalice\n\n\nnov\n\n\nNaN\n\n\n9.00\n\n\n\n\noct\n\n\nNaN\n\n\n8.00\n\n\n\n\nsep\n\n\nNaN\n\n\n8.00\n\n\n\n\nbob\n\n\nnov\n\n\n2.000\n\n\n10.00\n\n\n\n\noct\n\n\nNaN\n\n\n9.00\n\n\n\n\nsep\n\n\n0.000\n\n\n10.00\n\n\n\n\ncharles\n\n\nnov\n\n\n0.000\n\n\n5.00\n\n\n\n\noct\n\n\n3.000\n\n\n11.00\n\n\n\n\nsep\n\n\n3.000\n\n\n4.00\n\n\n\n\ndarwin\n\n\nnov\n\n\n0.000\n\n\n11.00\n\n\n\n\noct\n\n\n1.000\n\n\n10.00\n\n\n\n\nsep\n\n\n0.000\n\n\n9.00\n\n\n\n\nAll\n\n\n\n\n1.125\n\n\n8.75"
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#함수",
    "href": "posts/Data_Mining_Pandas/pandas.html#함수",
    "title": "Data_Mining CH2",
    "section": "37 함수",
    "text": "37 함수\n큰 DataFrame을 다룰 때 내용을 간단히 요약하는 것이 도움이 됩니다. 판다스는 이를 위한 몇 가지 함수를 제공합니다. 먼저 수치 값, 누락된 값, 텍스트 값이 섞인 큰 DataFrame을 만들어 보죠. 주피터 노트북은 이 DataFrame의 일부만 보여줍니다:\nmuch_data = np.fromfunction(lambda x,y: (x+y*y)%17*11, (10000, 26))\nlarge_df = pd.DataFrame(much_data, columns=list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"))\nlarge_df[large_df % 16 == 0] = np.nan\nlarge_df.insert(3,\"some_text\", \"Blabla\")\nlarge_df\n\n\n\n\n\n\n\n\n\nA\n\n\nB\n\n\nC\n\n\nsome_text\n\n\nD\n\n\nE\n\n\nF\n\n\nG\n\n\nH\n\n\nI\n\n\n…\n\n\nQ\n\n\nR\n\n\nS\n\n\nT\n\n\nU\n\n\nV\n\n\nW\n\n\nX\n\n\nY\n\n\nZ\n\n\n\n\n\n\n0\n\n\nNaN\n\n\n11.0\n\n\n44.0\n\n\nBlabla\n\n\n99.0\n\n\nNaN\n\n\n88.0\n\n\n22.0\n\n\n165.0\n\n\n143.0\n\n\n…\n\n\n11.0\n\n\nNaN\n\n\n11.0\n\n\n44.0\n\n\n99.0\n\n\nNaN\n\n\n88.0\n\n\n22.0\n\n\n165.0\n\n\n143.0\n\n\n\n\n1\n\n\n11.0\n\n\n22.0\n\n\n55.0\n\n\nBlabla\n\n\n110.0\n\n\nNaN\n\n\n99.0\n\n\n33.0\n\n\nNaN\n\n\n154.0\n\n\n…\n\n\n22.0\n\n\n11.0\n\n\n22.0\n\n\n55.0\n\n\n110.0\n\n\nNaN\n\n\n99.0\n\n\n33.0\n\n\nNaN\n\n\n154.0\n\n\n\n\n2\n\n\n22.0\n\n\n33.0\n\n\n66.0\n\n\nBlabla\n\n\n121.0\n\n\n11.0\n\n\n110.0\n\n\n44.0\n\n\nNaN\n\n\n165.0\n\n\n…\n\n\n33.0\n\n\n22.0\n\n\n33.0\n\n\n66.0\n\n\n121.0\n\n\n11.0\n\n\n110.0\n\n\n44.0\n\n\nNaN\n\n\n165.0\n\n\n\n\n3\n\n\n33.0\n\n\n44.0\n\n\n77.0\n\n\nBlabla\n\n\n132.0\n\n\n22.0\n\n\n121.0\n\n\n55.0\n\n\n11.0\n\n\nNaN\n\n\n…\n\n\n44.0\n\n\n33.0\n\n\n44.0\n\n\n77.0\n\n\n132.0\n\n\n22.0\n\n\n121.0\n\n\n55.0\n\n\n11.0\n\n\nNaN\n\n\n\n\n4\n\n\n44.0\n\n\n55.0\n\n\n88.0\n\n\nBlabla\n\n\n143.0\n\n\n33.0\n\n\n132.0\n\n\n66.0\n\n\n22.0\n\n\nNaN\n\n\n…\n\n\n55.0\n\n\n44.0\n\n\n55.0\n\n\n88.0\n\n\n143.0\n\n\n33.0\n\n\n132.0\n\n\n66.0\n\n\n22.0\n\n\nNaN\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n9995\n\n\nNaN\n\n\nNaN\n\n\n33.0\n\n\nBlabla\n\n\n88.0\n\n\n165.0\n\n\n77.0\n\n\n11.0\n\n\n154.0\n\n\n132.0\n\n\n…\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n33.0\n\n\n88.0\n\n\n165.0\n\n\n77.0\n\n\n11.0\n\n\n154.0\n\n\n132.0\n\n\n\n\n9996\n\n\nNaN\n\n\n11.0\n\n\n44.0\n\n\nBlabla\n\n\n99.0\n\n\nNaN\n\n\n88.0\n\n\n22.0\n\n\n165.0\n\n\n143.0\n\n\n…\n\n\n11.0\n\n\nNaN\n\n\n11.0\n\n\n44.0\n\n\n99.0\n\n\nNaN\n\n\n88.0\n\n\n22.0\n\n\n165.0\n\n\n143.0\n\n\n\n\n9997\n\n\n11.0\n\n\n22.0\n\n\n55.0\n\n\nBlabla\n\n\n110.0\n\n\nNaN\n\n\n99.0\n\n\n33.0\n\n\nNaN\n\n\n154.0\n\n\n…\n\n\n22.0\n\n\n11.0\n\n\n22.0\n\n\n55.0\n\n\n110.0\n\n\nNaN\n\n\n99.0\n\n\n33.0\n\n\nNaN\n\n\n154.0\n\n\n\n\n9998\n\n\n22.0\n\n\n33.0\n\n\n66.0\n\n\nBlabla\n\n\n121.0\n\n\n11.0\n\n\n110.0\n\n\n44.0\n\n\nNaN\n\n\n165.0\n\n\n…\n\n\n33.0\n\n\n22.0\n\n\n33.0\n\n\n66.0\n\n\n121.0\n\n\n11.0\n\n\n110.0\n\n\n44.0\n\n\nNaN\n\n\n165.0\n\n\n\n\n9999\n\n\n33.0\n\n\n44.0\n\n\n77.0\n\n\nBlabla\n\n\n132.0\n\n\n22.0\n\n\n121.0\n\n\n55.0\n\n\n11.0\n\n\nNaN\n\n\n…\n\n\n44.0\n\n\n33.0\n\n\n44.0\n\n\n77.0\n\n\n132.0\n\n\n22.0\n\n\n121.0\n\n\n55.0\n\n\n11.0\n\n\nNaN\n\n\n\n\n\n\n10000 rows × 27 columns\n\n\nhead() 메서드는 처음 5개 행을 반환합니다:\nlarge_df.head()\n\n\n\n\n\n\n\n\n\nA\n\n\nB\n\n\nC\n\n\nsome_text\n\n\nD\n\n\nE\n\n\nF\n\n\nG\n\n\nH\n\n\nI\n\n\n…\n\n\nQ\n\n\nR\n\n\nS\n\n\nT\n\n\nU\n\n\nV\n\n\nW\n\n\nX\n\n\nY\n\n\nZ\n\n\n\n\n\n\n0\n\n\nNaN\n\n\n11.0\n\n\n44.0\n\n\nBlabla\n\n\n99.0\n\n\nNaN\n\n\n88.0\n\n\n22.0\n\n\n165.0\n\n\n143.0\n\n\n…\n\n\n11.0\n\n\nNaN\n\n\n11.0\n\n\n44.0\n\n\n99.0\n\n\nNaN\n\n\n88.0\n\n\n22.0\n\n\n165.0\n\n\n143.0\n\n\n\n\n1\n\n\n11.0\n\n\n22.0\n\n\n55.0\n\n\nBlabla\n\n\n110.0\n\n\nNaN\n\n\n99.0\n\n\n33.0\n\n\nNaN\n\n\n154.0\n\n\n…\n\n\n22.0\n\n\n11.0\n\n\n22.0\n\n\n55.0\n\n\n110.0\n\n\nNaN\n\n\n99.0\n\n\n33.0\n\n\nNaN\n\n\n154.0\n\n\n\n\n2\n\n\n22.0\n\n\n33.0\n\n\n66.0\n\n\nBlabla\n\n\n121.0\n\n\n11.0\n\n\n110.0\n\n\n44.0\n\n\nNaN\n\n\n165.0\n\n\n…\n\n\n33.0\n\n\n22.0\n\n\n33.0\n\n\n66.0\n\n\n121.0\n\n\n11.0\n\n\n110.0\n\n\n44.0\n\n\nNaN\n\n\n165.0\n\n\n\n\n3\n\n\n33.0\n\n\n44.0\n\n\n77.0\n\n\nBlabla\n\n\n132.0\n\n\n22.0\n\n\n121.0\n\n\n55.0\n\n\n11.0\n\n\nNaN\n\n\n…\n\n\n44.0\n\n\n33.0\n\n\n44.0\n\n\n77.0\n\n\n132.0\n\n\n22.0\n\n\n121.0\n\n\n55.0\n\n\n11.0\n\n\nNaN\n\n\n\n\n4\n\n\n44.0\n\n\n55.0\n\n\n88.0\n\n\nBlabla\n\n\n143.0\n\n\n33.0\n\n\n132.0\n\n\n66.0\n\n\n22.0\n\n\nNaN\n\n\n…\n\n\n55.0\n\n\n44.0\n\n\n55.0\n\n\n88.0\n\n\n143.0\n\n\n33.0\n\n\n132.0\n\n\n66.0\n\n\n22.0\n\n\nNaN\n\n\n\n\n\n\n5 rows × 27 columns\n\n\n마지막 5개 행을 반환하는 tail() 함수도 있습니다. 원하는 행 개수를 전달할 수도 있습니다:\nlarge_df.tail(n=2)\n\n\n\n\n\n\n\n\n\nA\n\n\nB\n\n\nC\n\n\nsome_text\n\n\nD\n\n\nE\n\n\nF\n\n\nG\n\n\nH\n\n\nI\n\n\n…\n\n\nQ\n\n\nR\n\n\nS\n\n\nT\n\n\nU\n\n\nV\n\n\nW\n\n\nX\n\n\nY\n\n\nZ\n\n\n\n\n\n\n9998\n\n\n22.0\n\n\n33.0\n\n\n66.0\n\n\nBlabla\n\n\n121.0\n\n\n11.0\n\n\n110.0\n\n\n44.0\n\n\nNaN\n\n\n165.0\n\n\n…\n\n\n33.0\n\n\n22.0\n\n\n33.0\n\n\n66.0\n\n\n121.0\n\n\n11.0\n\n\n110.0\n\n\n44.0\n\n\nNaN\n\n\n165.0\n\n\n\n\n9999\n\n\n33.0\n\n\n44.0\n\n\n77.0\n\n\nBlabla\n\n\n132.0\n\n\n22.0\n\n\n121.0\n\n\n55.0\n\n\n11.0\n\n\nNaN\n\n\n…\n\n\n44.0\n\n\n33.0\n\n\n44.0\n\n\n77.0\n\n\n132.0\n\n\n22.0\n\n\n121.0\n\n\n55.0\n\n\n11.0\n\n\nNaN\n\n\n\n\n\n\n2 rows × 27 columns\n\n\ninfo() 메서드는 각 열의 내용을 요약하여 출력합니다:\nlarge_df.info()\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10000 entries, 0 to 9999\nData columns (total 27 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   A          8823 non-null   float64\n 1   B          8824 non-null   float64\n 2   C          8824 non-null   float64\n 3   some_text  10000 non-null  object \n 4   D          8824 non-null   float64\n 5   E          8822 non-null   float64\n 6   F          8824 non-null   float64\n 7   G          8824 non-null   float64\n 8   H          8822 non-null   float64\n 9   I          8823 non-null   float64\n 10  J          8823 non-null   float64\n 11  K          8822 non-null   float64\n 12  L          8824 non-null   float64\n 13  M          8824 non-null   float64\n 14  N          8822 non-null   float64\n 15  O          8824 non-null   float64\n 16  P          8824 non-null   float64\n 17  Q          8824 non-null   float64\n 18  R          8823 non-null   float64\n 19  S          8824 non-null   float64\n 20  T          8824 non-null   float64\n 21  U          8824 non-null   float64\n 22  V          8822 non-null   float64\n 23  W          8824 non-null   float64\n 24  X          8824 non-null   float64\n 25  Y          8822 non-null   float64\n 26  Z          8823 non-null   float64\ndtypes: float64(26), object(1)\nmemory usage: 2.1+ MB\n마지막으로 describe() 메서드는 각 열에 대한 주요 집계 연산을 수행한 결과를 보여줍니다:\nFinally, the describe() method gives a nice overview of the main aggregated values over each column: * count: null(NaN)이 아닌 값의 개수 * mean: null이 아닌 값의 평균 * std: null이 아닌 값의 표준 편차 * min: null이 아닌 값의 최솟값 * 25%, 50%, 75%: null이 아닌 값의 25번째, 50번째, 75번째 백분위수 * max: null이 아닌 값의 최댓값\nlarge_df.describe()\n\n\n\n\n\n\n\n\n\nA\n\n\nB\n\n\nC\n\n\nD\n\n\nE\n\n\nF\n\n\nG\n\n\nH\n\n\nI\n\n\nJ\n\n\n…\n\n\nQ\n\n\nR\n\n\nS\n\n\nT\n\n\nU\n\n\nV\n\n\nW\n\n\nX\n\n\nY\n\n\nZ\n\n\n\n\n\n\ncount\n\n\n8823.000000\n\n\n8824.000000\n\n\n8824.000000\n\n\n8824.000000\n\n\n8822.000000\n\n\n8824.000000\n\n\n8824.000000\n\n\n8822.000000\n\n\n8823.000000\n\n\n8823.000000\n\n\n…\n\n\n8824.000000\n\n\n8823.000000\n\n\n8824.000000\n\n\n8824.000000\n\n\n8824.000000\n\n\n8822.000000\n\n\n8824.000000\n\n\n8824.000000\n\n\n8822.000000\n\n\n8823.000000\n\n\n\n\nmean\n\n\n87.977559\n\n\n87.972575\n\n\n87.987534\n\n\n88.012466\n\n\n87.983791\n\n\n88.007480\n\n\n87.977561\n\n\n88.000000\n\n\n88.022441\n\n\n88.022441\n\n\n…\n\n\n87.972575\n\n\n87.977559\n\n\n87.972575\n\n\n87.987534\n\n\n88.012466\n\n\n87.983791\n\n\n88.007480\n\n\n87.977561\n\n\n88.000000\n\n\n88.022441\n\n\n\n\nstd\n\n\n47.535911\n\n\n47.535523\n\n\n47.521679\n\n\n47.521679\n\n\n47.535001\n\n\n47.519371\n\n\n47.529755\n\n\n47.536879\n\n\n47.535911\n\n\n47.535911\n\n\n…\n\n\n47.535523\n\n\n47.535911\n\n\n47.535523\n\n\n47.521679\n\n\n47.521679\n\n\n47.535001\n\n\n47.519371\n\n\n47.529755\n\n\n47.536879\n\n\n47.535911\n\n\n\n\nmin\n\n\n11.000000\n\n\n11.000000\n\n\n11.000000\n\n\n11.000000\n\n\n11.000000\n\n\n11.000000\n\n\n11.000000\n\n\n11.000000\n\n\n11.000000\n\n\n11.000000\n\n\n…\n\n\n11.000000\n\n\n11.000000\n\n\n11.000000\n\n\n11.000000\n\n\n11.000000\n\n\n11.000000\n\n\n11.000000\n\n\n11.000000\n\n\n11.000000\n\n\n11.000000\n\n\n\n\n25%\n\n\n44.000000\n\n\n44.000000\n\n\n44.000000\n\n\n44.000000\n\n\n44.000000\n\n\n44.000000\n\n\n44.000000\n\n\n44.000000\n\n\n44.000000\n\n\n44.000000\n\n\n…\n\n\n44.000000\n\n\n44.000000\n\n\n44.000000\n\n\n44.000000\n\n\n44.000000\n\n\n44.000000\n\n\n44.000000\n\n\n44.000000\n\n\n44.000000\n\n\n44.000000\n\n\n\n\n50%\n\n\n88.000000\n\n\n88.000000\n\n\n88.000000\n\n\n88.000000\n\n\n88.000000\n\n\n88.000000\n\n\n88.000000\n\n\n88.000000\n\n\n88.000000\n\n\n88.000000\n\n\n…\n\n\n88.000000\n\n\n88.000000\n\n\n88.000000\n\n\n88.000000\n\n\n88.000000\n\n\n88.000000\n\n\n88.000000\n\n\n88.000000\n\n\n88.000000\n\n\n88.000000\n\n\n\n\n75%\n\n\n132.000000\n\n\n132.000000\n\n\n132.000000\n\n\n132.000000\n\n\n132.000000\n\n\n132.000000\n\n\n132.000000\n\n\n132.000000\n\n\n132.000000\n\n\n132.000000\n\n\n…\n\n\n132.000000\n\n\n132.000000\n\n\n132.000000\n\n\n132.000000\n\n\n132.000000\n\n\n132.000000\n\n\n132.000000\n\n\n132.000000\n\n\n132.000000\n\n\n132.000000\n\n\n\n\nmax\n\n\n165.000000\n\n\n165.000000\n\n\n165.000000\n\n\n165.000000\n\n\n165.000000\n\n\n165.000000\n\n\n165.000000\n\n\n165.000000\n\n\n165.000000\n\n\n165.000000\n\n\n…\n\n\n165.000000\n\n\n165.000000\n\n\n165.000000\n\n\n165.000000\n\n\n165.000000\n\n\n165.000000\n\n\n165.000000\n\n\n165.000000\n\n\n165.000000\n\n\n165.000000\n\n\n\n\n\n\n8 rows × 26 columns"
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#저장-로딩",
    "href": "posts/Data_Mining_Pandas/pandas.html#저장-로딩",
    "title": "Data_Mining CH2",
    "section": "38 저장 & 로딩",
    "text": "38 저장 & 로딩\n판다스는 DataFrame를 여러 가지 포맷으로 저장할 수 있습니다. CSV, Excel, JSON, HTML, HDF5, SQL 데이터베이스 같은 포맷이 가능합니다. 예제를 위해 DataFrame을 하나 만들어 보겠습니다:\nmy_df = pd.DataFrame(\n    [[\"Biking\", 68.5, 1985, np.nan], [\"Dancing\", 83.1, 1984, 3]], \n    columns=[\"hobby\",\"weight\",\"birthyear\",\"children\"],\n    index=[\"alice\", \"bob\"]\n)\nmy_df\n\n\n\n\n\n\n\n\n\nhobby\n\n\nweight\n\n\nbirthyear\n\n\nchildren\n\n\n\n\n\n\nalice\n\n\nBiking\n\n\n68.5\n\n\n1985\n\n\nNaN\n\n\n\n\nbob\n\n\nDancing\n\n\n83.1\n\n\n1984\n\n\n3.0"
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#저장",
    "href": "posts/Data_Mining_Pandas/pandas.html#저장",
    "title": "Data_Mining CH2",
    "section": "39 저장",
    "text": "39 저장\nCSV, HTML, JSON로 저장해 보죠:\nmy_df.to_csv(\"my_df.csv\")\nmy_df.to_html(\"my_df.html\")\nmy_df.to_json(\"my_df.json\")\n저장된 내용을 확인해 보죠:\nfor filename in (\"my_df.csv\", \"my_df.html\", \"my_df.json\"):\n    print(\"#\", filename)\n    with open(filename, \"rt\") as f:\n        print(f.read())\n        print()\n# my_df.csv\n,hobby,weight,birthyear,children\nalice,Biking,68.5,1985,\nbob,Dancing,83.1,1984,3.0\n\n\n# my_df.html\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>hobby</th>\n      <th>weight</th>\n      <th>birthyear</th>\n      <th>children</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>alice</th>\n      <td>Biking</td>\n      <td>68.5</td>\n      <td>1985</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>bob</th>\n      <td>Dancing</td>\n      <td>83.1</td>\n      <td>1984</td>\n      <td>3.0</td>\n    </tr>\n  </tbody>\n</table>\n\n# my_df.json\n{\"hobby\":{\"alice\":\"Biking\",\"bob\":\"Dancing\"},\"weight\":{\"alice\":68.5,\"bob\":83.1},\"birthyear\":{\"alice\":1985,\"bob\":1984},\"children\":{\"alice\":null,\"bob\":3.0}}\n인덱스는 (이름 없이) CSV 파일의 첫 번째 열에 저장되었습니다. HTML에서는 <th> 태그와 JSON에서는 키로 저장되었습니다.\n다른 포맷으로 저장하는 것도 비슷합니다. 하지만 일부 포맷은 추가적인 라이브러리 설치가 필요합니다. 예를 들어, 엑셀로 저장하려면 openpyxl 라이브러리가 필요합니다:\ntry:\n    my_df.to_excel(\"my_df.xlsx\", sheet_name='People')\nexcept ImportError as e:\n    print(e)"
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#로딩",
    "href": "posts/Data_Mining_Pandas/pandas.html#로딩",
    "title": "Data_Mining CH2",
    "section": "40 로딩",
    "text": "40 로딩\nCSV 파일을 DataFrame으로 로드해 보죠:\nmy_df_loaded = pd.read_csv(\"my_df.csv\", index_col=0)\nmy_df_loaded\n\n\n\n\n\n\n\n\n\nhobby\n\n\nweight\n\n\nbirthyear\n\n\nchildren\n\n\n\n\n\n\nalice\n\n\nBiking\n\n\n68.5\n\n\n1985\n\n\nNaN\n\n\n\n\nbob\n\n\nDancing\n\n\n83.1\n\n\n1984\n\n\n3.0\n\n\n\n\n\n\n예상할 수 있듯이 read_json, read_html, read_excel 함수도 있습니다. 인터넷에서 데이터를 바로 읽을 수도 있습니다. 예를 들어 깃허브에서 1,000개의 U.S. 도시를 로드해 보죠:\nus_cities = None\ntry:\n    csv_url = \"https://raw.githubusercontent.com/plotly/datasets/master/us-cities-top-1k.csv\"\n    us_cities = pd.read_csv(csv_url, index_col=0)\n    us_cities = us_cities.head()\nexcept IOError as e:\n    print(e)\nus_cities\n\n\n\n\n\n\n\n\n\nState\n\n\nPopulation\n\n\nlat\n\n\nlon\n\n\n\n\nCity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarysville\n\n\nWashington\n\n\n63269\n\n\n48.051764\n\n\n-122.177082\n\n\n\n\nPerris\n\n\nCalifornia\n\n\n72326\n\n\n33.782519\n\n\n-117.228648\n\n\n\n\nCleveland\n\n\nOhio\n\n\n390113\n\n\n41.499320\n\n\n-81.694361\n\n\n\n\nWorcester\n\n\nMassachusetts\n\n\n182544\n\n\n42.262593\n\n\n-71.802293\n\n\n\n\nColumbia\n\n\nSouth Carolina\n\n\n133358\n\n\n34.000710\n\n\n-81.034814\n\n\n\n\n\n\n이외에도 많은 옵션이 있습니다. 특히 datetime 포맷에 관련된 옵션이 많습니다. 더 자세한 내용은 온라인 문서를 참고하세요."
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#dataframe-합치기",
    "href": "posts/Data_Mining_Pandas/pandas.html#dataframe-합치기",
    "title": "Data_Mining CH2",
    "section": "41 DataFrame 합치기",
    "text": "41 DataFrame 합치기"
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#sql-조인",
    "href": "posts/Data_Mining_Pandas/pandas.html#sql-조인",
    "title": "Data_Mining CH2",
    "section": "42 SQL 조인",
    "text": "42 SQL 조인\n판다스의 강력한 기능 중 하나는 DataFrame에 대해 SQL 같은 조인(join)을 수행할 수 있는 것입니다. 여러 종류의 조인이 지원됩니다. 이너 조인(inner join), 레프트/라이트 아우터 조인(left/right outer join), 풀 조인(full join)입니다. 이에 대해 알아 보기 위해 간단한 DataFrame을 만들어 보죠:\ncity_loc = pd.DataFrame(\n    [\n        [\"CA\", \"San Francisco\", 37.781334, -122.416728],\n        [\"NY\", \"New York\", 40.705649, -74.008344],\n        [\"FL\", \"Miami\", 25.791100, -80.320733],\n        [\"OH\", \"Cleveland\", 41.473508, -81.739791],\n        [\"UT\", \"Salt Lake City\", 40.755851, -111.896657]\n    ], columns=[\"state\", \"city\", \"lat\", \"lng\"])\ncity_loc\n\n\n\n\n\n\n\n\n\nstate\n\n\ncity\n\n\nlat\n\n\nlng\n\n\n\n\n\n\n0\n\n\nCA\n\n\nSan Francisco\n\n\n37.781334\n\n\n-122.416728\n\n\n\n\n1\n\n\nNY\n\n\nNew York\n\n\n40.705649\n\n\n-74.008344\n\n\n\n\n2\n\n\nFL\n\n\nMiami\n\n\n25.791100\n\n\n-80.320733\n\n\n\n\n3\n\n\nOH\n\n\nCleveland\n\n\n41.473508\n\n\n-81.739791\n\n\n\n\n4\n\n\nUT\n\n\nSalt Lake City\n\n\n40.755851\n\n\n-111.896657\n\n\n\n\n\n\ncity_pop = pd.DataFrame(\n    [\n        [808976, \"San Francisco\", \"California\"],\n        [8363710, \"New York\", \"New-York\"],\n        [413201, \"Miami\", \"Florida\"],\n        [2242193, \"Houston\", \"Texas\"]\n    ], index=[3,4,5,6], columns=[\"population\", \"city\", \"state\"])\ncity_pop\n\n\n\n\n\n\n\n\n\npopulation\n\n\ncity\n\n\nstate\n\n\n\n\n\n\n3\n\n\n808976\n\n\nSan Francisco\n\n\nCalifornia\n\n\n\n\n4\n\n\n8363710\n\n\nNew York\n\n\nNew-York\n\n\n\n\n5\n\n\n413201\n\n\nMiami\n\n\nFlorida\n\n\n\n\n6\n\n\n2242193\n\n\nHouston\n\n\nTexas\n\n\n\n\n\n\n이제 merge() 함수를 사용해 이 DataFrame을 조인해 보죠:\npd.merge(left=city_loc, right=city_pop, on=\"city\")\n\n\n\n\n\n\n\n\n\nstate_x\n\n\ncity\n\n\nlat\n\n\nlng\n\n\npopulation\n\n\nstate_y\n\n\n\n\n\n\n0\n\n\nCA\n\n\nSan Francisco\n\n\n37.781334\n\n\n-122.416728\n\n\n808976\n\n\nCalifornia\n\n\n\n\n1\n\n\nNY\n\n\nNew York\n\n\n40.705649\n\n\n-74.008344\n\n\n8363710\n\n\nNew-York\n\n\n\n\n2\n\n\nFL\n\n\nMiami\n\n\n25.791100\n\n\n-80.320733\n\n\n413201\n\n\nFlorida\n\n\n\n\n\n\n두 DataFrame은 state란 이름의 열을 가지고 있으므로 state_x와 state_y로 이름이 바뀌었습니다.\n또한 Cleveland, Salt Lake City, Houston은 두 DataFrame에 모두 존재하지 않기 때문에 삭제되었습니다. SQL의 INNER JOIN과 동일합니다. 도시를 삭제하지 않고 NaN으로 채우는 FULL OUTER JOIN을 원하면 how=\"outer\"로 지정합니다:\nall_cities = pd.merge(left=city_loc, right=city_pop, on=\"city\", how=\"outer\")\nall_cities\n\n\n\n\n\n\n\n\n\nstate_x\n\n\ncity\n\n\nlat\n\n\nlng\n\n\npopulation\n\n\nstate_y\n\n\n\n\n\n\n0\n\n\nCA\n\n\nSan Francisco\n\n\n37.781334\n\n\n-122.416728\n\n\n808976.0\n\n\nCalifornia\n\n\n\n\n1\n\n\nNY\n\n\nNew York\n\n\n40.705649\n\n\n-74.008344\n\n\n8363710.0\n\n\nNew-York\n\n\n\n\n2\n\n\nFL\n\n\nMiami\n\n\n25.791100\n\n\n-80.320733\n\n\n413201.0\n\n\nFlorida\n\n\n\n\n3\n\n\nOH\n\n\nCleveland\n\n\n41.473508\n\n\n-81.739791\n\n\nNaN\n\n\nNaN\n\n\n\n\n4\n\n\nUT\n\n\nSalt Lake City\n\n\n40.755851\n\n\n-111.896657\n\n\nNaN\n\n\nNaN\n\n\n\n\n5\n\n\nNaN\n\n\nHouston\n\n\nNaN\n\n\nNaN\n\n\n2242193.0\n\n\nTexas\n\n\n\n\n\n\n물론 LEFT OUTER JOIN은 how=\"left\"로 지정할 수 있습니다. 왼쪽의 DataFrame에 있는 도시만 남습니다. 비슷하게 how=\"right\"는 오른쪽 DataFrame에 있는 도시만 결과에 남습니다. 예를 들면:\npd.merge(left=city_loc, right=city_pop, on=\"city\", how=\"right\")\n\n\n\n\n\n\n\n\n\nstate_x\n\n\ncity\n\n\nlat\n\n\nlng\n\n\npopulation\n\n\nstate_y\n\n\n\n\n\n\n0\n\n\nCA\n\n\nSan Francisco\n\n\n37.781334\n\n\n-122.416728\n\n\n808976\n\n\nCalifornia\n\n\n\n\n1\n\n\nNY\n\n\nNew York\n\n\n40.705649\n\n\n-74.008344\n\n\n8363710\n\n\nNew-York\n\n\n\n\n2\n\n\nFL\n\n\nMiami\n\n\n25.791100\n\n\n-80.320733\n\n\n413201\n\n\nFlorida\n\n\n\n\n3\n\n\nNaN\n\n\nHouston\n\n\nNaN\n\n\nNaN\n\n\n2242193\n\n\nTexas\n\n\n\n\n\n\n조인할 키가 DataFrame 인덱스라면 left_index=True나 right_index=True로 지정해야 합니다. 키 열의 이름이 다르면 left_on과 right_on을 사용합니다. 예를 들어:\ncity_pop2 = city_pop.copy()\ncity_pop2.columns = [\"population\", \"name\", \"state\"]\npd.merge(left=city_loc, right=city_pop2, left_on=\"city\", right_on=\"name\")\n\n\n\n\n\n\n\n\n\nstate_x\n\n\ncity\n\n\nlat\n\n\nlng\n\n\npopulation\n\n\nname\n\n\nstate_y\n\n\n\n\n\n\n0\n\n\nCA\n\n\nSan Francisco\n\n\n37.781334\n\n\n-122.416728\n\n\n808976\n\n\nSan Francisco\n\n\nCalifornia\n\n\n\n\n1\n\n\nNY\n\n\nNew York\n\n\n40.705649\n\n\n-74.008344\n\n\n8363710\n\n\nNew York\n\n\nNew-York\n\n\n\n\n2\n\n\nFL\n\n\nMiami\n\n\n25.791100\n\n\n-80.320733\n\n\n413201\n\n\nMiami\n\n\nFlorida"
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#연결",
    "href": "posts/Data_Mining_Pandas/pandas.html#연결",
    "title": "Data_Mining CH2",
    "section": "43 연결",
    "text": "43 연결\nDataFrame을 조인하는 대신 그냥 연결할 수도 있습니다. concat() 함수가 하는 일입니다:\nresult_concat = pd.concat([city_loc, city_pop])\nresult_concat\n\n\n\n\n\n\n\n\n\nstate\n\n\ncity\n\n\nlat\n\n\nlng\n\n\npopulation\n\n\n\n\n\n\n0\n\n\nCA\n\n\nSan Francisco\n\n\n37.781334\n\n\n-122.416728\n\n\nNaN\n\n\n\n\n1\n\n\nNY\n\n\nNew York\n\n\n40.705649\n\n\n-74.008344\n\n\nNaN\n\n\n\n\n2\n\n\nFL\n\n\nMiami\n\n\n25.791100\n\n\n-80.320733\n\n\nNaN\n\n\n\n\n3\n\n\nOH\n\n\nCleveland\n\n\n41.473508\n\n\n-81.739791\n\n\nNaN\n\n\n\n\n4\n\n\nUT\n\n\nSalt Lake City\n\n\n40.755851\n\n\n-111.896657\n\n\nNaN\n\n\n\n\n3\n\n\nCalifornia\n\n\nSan Francisco\n\n\nNaN\n\n\nNaN\n\n\n808976.0\n\n\n\n\n4\n\n\nNew-York\n\n\nNew York\n\n\nNaN\n\n\nNaN\n\n\n8363710.0\n\n\n\n\n5\n\n\nFlorida\n\n\nMiami\n\n\nNaN\n\n\nNaN\n\n\n413201.0\n\n\n\n\n6\n\n\nTexas\n\n\nHouston\n\n\nNaN\n\n\nNaN\n\n\n2242193.0\n\n\n\n\n\n\n이 연산은 (행을 따라) 수직적으로 데이터를 연결하고 (열을 따라) 수평으로 연결하지 않습니다. 이 예에서 동일한 인덱스를 가진 행이 있습니다(예를 들면 3). 판다스는 이를 우아하게 처리합니다:\nresult_concat.loc[3]\n\n\n\n\n\n\n\n\n\nstate\n\n\ncity\n\n\nlat\n\n\nlng\n\n\npopulation\n\n\n\n\n\n\n3\n\n\nOH\n\n\nCleveland\n\n\n41.473508\n\n\n-81.739791\n\n\nNaN\n\n\n\n\n3\n\n\nCalifornia\n\n\nSan Francisco\n\n\nNaN\n\n\nNaN\n\n\n808976.0\n\n\n\n\n\n\n또는 인덱스를 무시하도록 설정할 수 있습니다:\npd.concat([city_loc, city_pop], ignore_index=True)\n\n\n\n\n\n\n\n\n\nstate\n\n\ncity\n\n\nlat\n\n\nlng\n\n\npopulation\n\n\n\n\n\n\n0\n\n\nCA\n\n\nSan Francisco\n\n\n37.781334\n\n\n-122.416728\n\n\nNaN\n\n\n\n\n1\n\n\nNY\n\n\nNew York\n\n\n40.705649\n\n\n-74.008344\n\n\nNaN\n\n\n\n\n2\n\n\nFL\n\n\nMiami\n\n\n25.791100\n\n\n-80.320733\n\n\nNaN\n\n\n\n\n3\n\n\nOH\n\n\nCleveland\n\n\n41.473508\n\n\n-81.739791\n\n\nNaN\n\n\n\n\n4\n\n\nUT\n\n\nSalt Lake City\n\n\n40.755851\n\n\n-111.896657\n\n\nNaN\n\n\n\n\n5\n\n\nCalifornia\n\n\nSan Francisco\n\n\nNaN\n\n\nNaN\n\n\n808976.0\n\n\n\n\n6\n\n\nNew-York\n\n\nNew York\n\n\nNaN\n\n\nNaN\n\n\n8363710.0\n\n\n\n\n7\n\n\nFlorida\n\n\nMiami\n\n\nNaN\n\n\nNaN\n\n\n413201.0\n\n\n\n\n8\n\n\nTexas\n\n\nHouston\n\n\nNaN\n\n\nNaN\n\n\n2242193.0\n\n\n\n\n\n\n한 DataFrame에 열이 없을 때 NaN이 채워져 있는 것처럼 동작합니다. join=\"inner\"로 설정하면 양쪽의 DataFrame에 존재하는 열만 반환됩니다:\npd.concat([city_loc, city_pop], join=\"inner\")\n\n\n\n\n\n\n\n\n\nstate\n\n\ncity\n\n\n\n\n\n\n0\n\n\nCA\n\n\nSan Francisco\n\n\n\n\n1\n\n\nNY\n\n\nNew York\n\n\n\n\n2\n\n\nFL\n\n\nMiami\n\n\n\n\n3\n\n\nOH\n\n\nCleveland\n\n\n\n\n4\n\n\nUT\n\n\nSalt Lake City\n\n\n\n\n3\n\n\nCalifornia\n\n\nSan Francisco\n\n\n\n\n4\n\n\nNew-York\n\n\nNew York\n\n\n\n\n5\n\n\nFlorida\n\n\nMiami\n\n\n\n\n6\n\n\nTexas\n\n\nHouston\n\n\n\n\n\n\naxis=1로 설정하면 DataFrame을 수직이 아니라 수평으로 연결할 수 있습니다:\npd.concat([city_loc, city_pop], axis=1)\n\n\n\n\n\n\n\n\n\nstate\n\n\ncity\n\n\nlat\n\n\nlng\n\n\npopulation\n\n\ncity\n\n\nstate\n\n\n\n\n\n\n0\n\n\nCA\n\n\nSan Francisco\n\n\n37.781334\n\n\n-122.416728\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n1\n\n\nNY\n\n\nNew York\n\n\n40.705649\n\n\n-74.008344\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n2\n\n\nFL\n\n\nMiami\n\n\n25.791100\n\n\n-80.320733\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n3\n\n\nOH\n\n\nCleveland\n\n\n41.473508\n\n\n-81.739791\n\n\n808976.0\n\n\nSan Francisco\n\n\nCalifornia\n\n\n\n\n4\n\n\nUT\n\n\nSalt Lake City\n\n\n40.755851\n\n\n-111.896657\n\n\n8363710.0\n\n\nNew York\n\n\nNew-York\n\n\n\n\n5\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n413201.0\n\n\nMiami\n\n\nFlorida\n\n\n\n\n6\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n2242193.0\n\n\nHouston\n\n\nTexas\n\n\n\n\n\n\n이 경우 인덱스가 잘 정렬되지 않기 때문에 의미가 없습니다(예를 들어 Cleveland와 San Francisco의 인덱스 레이블이 3이기 때문에 동일한 행에 놓여 있습니다). 이 DataFrame을 연결하기 전에 도시로 인덱스를 재설정해 보죠:\npd.concat([city_loc.set_index(\"city\"), city_pop.set_index(\"city\")], axis=1)\n\n\n\n\n\n\n\n\n\nstate\n\n\nlat\n\n\nlng\n\n\npopulation\n\n\nstate\n\n\n\n\ncity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSan Francisco\n\n\nCA\n\n\n37.781334\n\n\n-122.416728\n\n\n808976.0\n\n\nCalifornia\n\n\n\n\nNew York\n\n\nNY\n\n\n40.705649\n\n\n-74.008344\n\n\n8363710.0\n\n\nNew-York\n\n\n\n\nMiami\n\n\nFL\n\n\n25.791100\n\n\n-80.320733\n\n\n413201.0\n\n\nFlorida\n\n\n\n\nCleveland\n\n\nOH\n\n\n41.473508\n\n\n-81.739791\n\n\nNaN\n\n\nNaN\n\n\n\n\nSalt Lake City\n\n\nUT\n\n\n40.755851\n\n\n-111.896657\n\n\nNaN\n\n\nNaN\n\n\n\n\nHouston\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n2242193.0\n\n\nTexas\n\n\n\n\n\n\nFULL OUTER JOIN을 수행한 것과 비슷합니다. 하지만 state 열이 state_x와 state_y로 바뀌지 않았고 city 열이 인덱스가 되었습니다.\nappend() 메서드는 DataFrame을 수직으로 연결하는 단축 메서드입니다:\ncity_loc.append(city_pop)\nC:\\Users\\seong taek\\AppData\\Local\\Temp\\ipykernel_9428\\360768564.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  city_loc.append(city_pop)\n\n\n\n\n\n\n\n\n\nstate\n\n\ncity\n\n\nlat\n\n\nlng\n\n\npopulation\n\n\n\n\n\n\n0\n\n\nCA\n\n\nSan Francisco\n\n\n37.781334\n\n\n-122.416728\n\n\nNaN\n\n\n\n\n1\n\n\nNY\n\n\nNew York\n\n\n40.705649\n\n\n-74.008344\n\n\nNaN\n\n\n\n\n2\n\n\nFL\n\n\nMiami\n\n\n25.791100\n\n\n-80.320733\n\n\nNaN\n\n\n\n\n3\n\n\nOH\n\n\nCleveland\n\n\n41.473508\n\n\n-81.739791\n\n\nNaN\n\n\n\n\n4\n\n\nUT\n\n\nSalt Lake City\n\n\n40.755851\n\n\n-111.896657\n\n\nNaN\n\n\n\n\n3\n\n\nCalifornia\n\n\nSan Francisco\n\n\nNaN\n\n\nNaN\n\n\n808976.0\n\n\n\n\n4\n\n\nNew-York\n\n\nNew York\n\n\nNaN\n\n\nNaN\n\n\n8363710.0\n\n\n\n\n5\n\n\nFlorida\n\n\nMiami\n\n\nNaN\n\n\nNaN\n\n\n413201.0\n\n\n\n\n6\n\n\nTexas\n\n\nHouston\n\n\nNaN\n\n\nNaN\n\n\n2242193.0\n\n\n\n\n\n\n판다스의 다른 메서드와 마찬가지로 append() 메서드는 실제 city_loc을 수정하지 않습니다. 복사본을 만들어 수정한 다음 반환합니다."
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#범주",
    "href": "posts/Data_Mining_Pandas/pandas.html#범주",
    "title": "Data_Mining CH2",
    "section": "44 범주",
    "text": "44 범주\n범주로 표현된 값을 가진 경우가 흔합니다. 예를 들어 1은 여성, 2는 남성이거나 \"A\"는 좋은 것, \"B\"는 평균, \"C\"는 나쁜 것 등입니다. 범주형 값을 읽기 힘들고 다루기 번거롭습니다. 하지만 판다스에서는 간단합니다. 예를 들기 위해 앞서 만든 city_pop DataFrame에 범주를 표현하는 열을 추가해 보겠습니다:\ncity_eco = city_pop.copy()\ncity_eco[\"eco_code\"] = [17, 17, 34, 20]\ncity_eco\n\n\n\n\n\n\n\n\n\npopulation\n\n\ncity\n\n\nstate\n\n\neco_code\n\n\n\n\n\n\n3\n\n\n808976\n\n\nSan Francisco\n\n\nCalifornia\n\n\n17\n\n\n\n\n4\n\n\n8363710\n\n\nNew York\n\n\nNew-York\n\n\n17\n\n\n\n\n5\n\n\n413201\n\n\nMiami\n\n\nFlorida\n\n\n34\n\n\n\n\n6\n\n\n2242193\n\n\nHouston\n\n\nTexas\n\n\n20\n\n\n\n\n\n\n이제 eco_code열은 의미없는 코드입니다. 이를 바꿔 보죠. 먼저 eco_code를 기반으로 새로운 범주형 열을 만듭니다:\ncity_eco[\"economy\"] = city_eco[\"eco_code\"].astype('category')\ncity_eco[\"economy\"].cat.categories\nInt64Index([17, 20, 34], dtype='int64')\n의미있는 이름을 가진 범주를 지정할 수 있습니다:\ncity_eco[\"economy\"].cat.categories = [\"Finance\", \"Energy\", \"Tourism\"]\ncity_eco\nC:\\Users\\seong taek\\AppData\\Local\\Temp\\ipykernel_9428\\2586490918.py:1: FutureWarning: Setting categories in-place is deprecated and will raise in a future version. Use rename_categories instead.\n  city_eco[\"economy\"].cat.categories = [\"Finance\", \"Energy\", \"Tourism\"]\n\n\n\n\n\n\n\n\n\npopulation\n\n\ncity\n\n\nstate\n\n\neco_code\n\n\neconomy\n\n\n\n\n\n\n3\n\n\n808976\n\n\nSan Francisco\n\n\nCalifornia\n\n\n17\n\n\nFinance\n\n\n\n\n4\n\n\n8363710\n\n\nNew York\n\n\nNew-York\n\n\n17\n\n\nFinance\n\n\n\n\n5\n\n\n413201\n\n\nMiami\n\n\nFlorida\n\n\n34\n\n\nTourism\n\n\n\n\n6\n\n\n2242193\n\n\nHouston\n\n\nTexas\n\n\n20\n\n\nEnergy\n\n\n\n\n\n\n범주형 값은 알파벳 순서가 아니라 범주형 순서로 정렬합니다:\ncity_eco.sort_values(by=\"economy\", ascending=False)\n\n\n\n\n\n\n\n\n\npopulation\n\n\ncity\n\n\nstate\n\n\neco_code\n\n\neconomy\n\n\n\n\n\n\n5\n\n\n413201\n\n\nMiami\n\n\nFlorida\n\n\n34\n\n\nTourism\n\n\n\n\n6\n\n\n2242193\n\n\nHouston\n\n\nTexas\n\n\n20\n\n\nEnergy\n\n\n\n\n3\n\n\n808976\n\n\nSan Francisco\n\n\nCalifornia\n\n\n17\n\n\nFinance\n\n\n\n\n4\n\n\n8363710\n\n\nNew York\n\n\nNew-York\n\n\n17\n\n\nFinance"
  },
  {
    "objectID": "posts/Data_Mining_Pandas/pandas.html#그-다음엔",
    "href": "posts/Data_Mining_Pandas/pandas.html#그-다음엔",
    "title": "Data_Mining CH2",
    "section": "45 그 다음엔?",
    "text": "45 그 다음엔?\n이제 알았겠지만 판다스는 매우 커다란 라이브러리이고 기능이 많습니다. 가장 중요한 기능들을 둘러 보았지만 빙산의 일각일 뿐입니다. 더 많은 것을 익히려면 실전 데이터로 직접 실습해 보는 것이 제일 좋습니다. 판다스의 훌륭한 문서와 쿡북을 보는 것도 좋습니다."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Profile",
    "section": "",
    "text": "Name\nSeongtaek Jo\n\n\nBirth\n1999.01.15\n\n\nPhone\n+82 10-9823-3115\n\n\nEmail\ntjdxor3115@naver.com\n\n\ncertificate\nADsP (데이터분석 준전문가)\n\n\nstarted from the Gwangju"
  },
  {
    "objectID": "coding.html",
    "href": "coding.html",
    "title": "Code Repository",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nData_Mining CH2\n\n\n\n\n\n\n\ncode\n\n\ndata_mining\n\n\njupyter\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nData_Visualize CH2\n\n\n\n\n\n\n\ncode\n\n\ndata_visualize\n\n\nrstudio\n\n\n\n\n\n\n\n\n\n\n\nMar 30, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nData_Visualize CH1\n\n\n\n\n\n\n\ncode\n\n\ndata_visualize\n\n\nrstudio\n\n\n\n\n\n\n\n\n\n\n\nMar 29, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nadvanced_python CH1\n\n\n\n\n\n\n\ncode\n\n\nadvanced_python\n\n\njupyter\n\n\n\n\n\n\n\n\n\n\n\nMar 28, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nData_Mining CH1\n\n\n\n\n\n\n\ncode\n\n\ndata_mining\n\n\njupyter\n\n\n\n\n\n\n\n\n\n\n\nMar 28, 2023\n\n\nSeongtaek\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Data_Visualize_Ch3/figure 3.2.html",
    "href": "posts/Data_Visualize_Ch3/figure 3.2.html",
    "title": "Data_Visualization CH3",
    "section": "",
    "text": "HTML파일로 보기\nFigure 3.2"
  },
  {
    "objectID": "posts/Data_Visualize_Ch3/figure 3.2.html#데이터-시각화-실습-figure-3.2",
    "href": "posts/Data_Visualize_Ch3/figure 3.2.html#데이터-시각화-실습-figure-3.2",
    "title": "Data_Visualize CH3",
    "section": "1 데이터 시각화 실습 : Figure 3.2",
    "text": "1 데이터 시각화 실습 : Figure 3.2\n\n1.1 패키지 불러오기\n\nlibrary(ggplot2)\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\nlibrary(ggrepel)\nlibrary(cowplot) # plot_grid\n\n\n\n1.2 데이터 불러오기, 파악\n\nncdc_normals.csv (날짜별 온도 등 데이터셋)\n\n\nncdc_normals <- read.csv('C:/Users/seong taek/Desktop/3-1 DataVisualize/data_visualize/ncdc_normals.csv')\n\n### 차원 파악\nncdc_normals %>% dim()\n#> [1] 2745366       6\n\n### 앞부분 훑어보기\nncdc_normals %>% head()\n#>    station_id month day temperature flag       date\n#> 1 AQW00061705     1   1        82.4    C 0000-01-01\n#> 2 AQW00061705     1   2        82.4    C 0000-01-02\n#> 3 AQW00061705     1   3        82.4    C 0000-01-03\n#> 4 AQW00061705     1   4        82.4    C 0000-01-04\n#> 5 AQW00061705     1   5        82.4    C 0000-01-05\n#> 6 AQW00061705     1   6        82.4    C 0000-01-06\n\n### 통계 요약 정보\nncdc_normals %>% summary()\n#>   station_id            month             day         temperature    \n#>  Length:2745366     Min.   : 1.000   Min.   : 1.00   Min.   :-21.80  \n#>  Class :character   1st Qu.: 4.000   1st Qu.: 8.00   1st Qu.: 39.70  \n#>  Mode  :character   Median : 7.000   Median :16.00   Median : 54.60  \n#>                     Mean   : 6.514   Mean   :15.76   Mean   : 53.17  \n#>                     3rd Qu.:10.000   3rd Qu.:23.00   3rd Qu.: 68.10  \n#>                     Max.   :12.000   Max.   :31.00   Max.   :103.20  \n#>      flag               date          \n#>  Length:2745366     Length:2745366    \n#>  Class :character   Class :character  \n#>  Mode  :character   Mode  :character  \n#>                                       \n#>                                       \n#> \n\n### 각 컬럼 클래스(타입) 확인\nncdc_normals %>% sapply(class)\n#>  station_id       month         day temperature        flag        date \n#> \"character\"   \"integer\"   \"integer\"   \"numeric\" \"character\" \"character\"\n\n### 각 컬럼 자료형 확인\nncdc_normals %>% sapply(typeof)\n#>  station_id       month         day temperature        flag        date \n#> \"character\"   \"integer\"   \"integer\"    \"double\" \"character\" \"character\"\n\n\n\n1.3 전처리\n\n### staion id와 location 지정 선택한 d.f 만들기\nstation_loc <- data.frame(station_id = c(\"USW00014819\",\"USC00042319\",\"USW00093107\",\"USW00012918\"),\n                          location = c(\"Chicago\",\"Death Valley\",\"San Diego\",\"Houston\"))\n\nstation_loc %>% head()\n#>    station_id     location\n#> 1 USW00014819      Chicago\n#> 2 USC00042319 Death Valley\n#> 3 USW00093107    San Diego\n#> 4 USW00012918      Houston\n\n\n### station_id로 ncdc_normals와 station_loc 두 컬럼을 `inner_join`\ntemps_long <- ncdc_normals %>% inner_join(station_loc, by=\"station_id\")\ntemps_long %>% head()\n#>    station_id month day temperature flag       date     location\n#> 1 USC00042319     1   1        51.0    S 0000-01-01 Death Valley\n#> 2 USC00042319     1   2        51.2    S 0000-01-02 Death Valley\n#> 3 USC00042319     1   3        51.3    S 0000-01-03 Death Valley\n#> 4 USC00042319     1   4        51.4    S 0000-01-04 Death Valley\n#> 5 USC00042319     1   5        51.6    S 0000-01-05 Death Valley\n#> 6 USC00042319     1   6        51.7    S 0000-01-06 Death Valley\ntemps_long %>% sapply(class)\n#>  station_id       month         day temperature        flag        date \n#> \"character\"   \"integer\"   \"integer\"   \"numeric\" \"character\" \"character\" \n#>    location \n#> \"character\"\n\n### temps_long의 `date`의 타입을 Cha → Date로 변환\ntemps_long$date <- temps_long$date %>% as.Date('%Y-%m-%d')\ntemps_long %>% sapply(class)\n#>  station_id       month         day temperature        flag        date \n#> \"character\"   \"integer\"   \"integer\"   \"numeric\" \"character\"      \"Date\" \n#>    location \n#> \"character\"\n\n\n### Houston 필터 \ndata_Houston <- temps_long %>% filter(location=='Houston')\ndata_Houston %>% head()\n#>    station_id month day temperature flag       date location\n#> 1 USW00012918     1   1        53.9    S 0000-01-01  Houston\n#> 2 USW00012918     1   2        53.8    S 0000-01-02  Houston\n#> 3 USW00012918     1   3        53.8    S 0000-01-03  Houston\n#> 4 USW00012918     1   4        53.8    S 0000-01-04  Houston\n#> 5 USW00012918     1   5        53.8    S 0000-01-05  Houston\n#> 6 USW00012918     1   6        53.7    S 0000-01-06  Houston\ndata_Houston %>% tail()\n#>      station_id month day temperature flag       date location\n#> 361 USW00012918    12  26        54.2    C 0000-12-26  Houston\n#> 362 USW00012918    12  27        54.1    C 0000-12-27  Houston\n#> 363 USW00012918    12  28        54.1    C 0000-12-28  Houston\n#> 364 USW00012918    12  29        54.0    C 0000-12-29  Houston\n#> 365 USW00012918    12  30        53.9    C 0000-12-30  Houston\n#> 366 USW00012918    12  31        53.9    C 0000-12-31  Houston\n\n\n\n1.4 x축 눈금 설정\n\ndate_s <- '0000-01-01' %>% as.Date('%Y-%m-%d')\ndate_e <- '0001-01-01' %>% as.Date('%Y-%m-%d')\nbreak_date <- seq(date_s, date_e, by = '3 month')"
  },
  {
    "objectID": "posts/Data_Visualize_Ch3/figure 3.2.html#ggplot-축-설정",
    "href": "posts/Data_Visualize_Ch3/figure 3.2.html#ggplot-축-설정",
    "title": "Data_Visualization CH3",
    "section": "2 ggplot + 축 설정",
    "text": "2 ggplot + 축 설정\n\n사용 데이터셋 : data_Houston\nx=date, y=temperature\nscale_x_date\n\n이름 : month\n간격 : break_date (3개월)\n간격 라벨 (Jan ~ Jan)\n\nscale_y_continuous\n\n이름 : temp\n\n테마 : 밝게\n\n\ntemp_plot <- ggplot(data_Houston, aes(x=date, y=temperature)) +\n  geom_line(linewidth=1, color='royalblue') +\n  scale_x_date(name= 'month',\n               breaks= break_date,\n               labels= c('Jan','Apr','Jul','Oct','Jan')) +\n  scale_y_continuous(name= 'temp') +\n  theme_light()\n\ntemp_plot\n\n\n\n\n\n2.1 plot_grid\n\n여러 개의 그래프를 그리드로 결합하여 하나의 그래프로 만들어주는 함수\n\n\n### 2개의 temp_long 그래프\nplot_ab <- plot_grid(temp_plot,\n                     temp_plot,\n                     nrow= 1,            # 행의 개수\n                     rel_widths= c(1,2), # 각각의 너비\n                     labels= c('a','b')) # 라벨 a,b\n\nplot_ab\n\n\n\n### plot_ab 그래프 + templong 그래프\nplot_abc <- plot_grid(plot_ab,\n                      temp_plot,\n                      ncol= 1,               # 열의 개수\n                      rel_heights= c(1.5, 1),# 각각의 높이\n                      labels= c('','c'))     # 라벨 '그대로', 'c'\n\nplot_abc"
  },
  {
    "objectID": "posts/Data_Visualize_Ch4/figure 3.5 ~ 3.6.html",
    "href": "posts/Data_Visualize_Ch4/figure 3.5 ~ 3.6.html",
    "title": "Data_Visualization CH4",
    "section": "",
    "text": "HTML파일로 보기\nFigure 3.5, Figure 3.6"
  },
  {
    "objectID": "posts/Data_Visualize_Ch4/figure 3.5 ~ 3.6.html#데이터-시각화-실습-figure-3.5-figure-3.6",
    "href": "posts/Data_Visualize_Ch4/figure 3.5 ~ 3.6.html#데이터-시각화-실습-figure-3.5-figure-3.6",
    "title": "Data_Visualize CH4",
    "section": "1 데이터 시각화 실습 : Figure 3.5, Figure 3.6",
    "text": "1 데이터 시각화 실습 : Figure 3.5, Figure 3.6\n\n1.1 패키지 불러오기\n\nlibrary(ggplot2)\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\nlibrary(ggrepel) # geom_text_repel()\n\n\n\n1.2 데이터 불러오기, 파악\n\nUS_census.csv (미국 인구 통계정보 데이터셋)\n\n\nUS_census <- read.csv('C:/Users/seong taek/Desktop/3-1 DataVisualize/data_visualize/US_census.csv')\n\n### 차원 파악\nUS_census %>% dim()\n#> [1] 3143   53\n\n### 앞부분 훑어보기\nUS_census %>% head(2)\n#>     state           name FIPS pop2010 pop2000 age_under_5 age_under_18\n#> 1 Alabama Autauga County 1001   54571   43671         6.6         26.8\n#> 2 Alabama Baldwin County 1003  182265  140415         6.1         23.0\n#>   age_over_65 female white black native asian pac_isl two_plus_races hispanic\n#> 1        12.0   51.3  78.5  17.7    0.4   0.9      NA            1.6      2.4\n#> 2        16.8   51.1  85.7   9.4    0.7   0.7      NA            1.5      4.4\n#>   white_not_hispanic no_move_in_one_plus_year foreign_born\n#> 1               77.2                     86.3          2.0\n#> 2               83.5                     83.0          3.6\n#>   foreign_spoken_at_home hs_grad bachelors veterans mean_work_travel\n#> 1                    3.7    85.3      21.7     5817             25.1\n#> 2                    5.5    87.6      26.8    20396             25.8\n#>   housing_units home_ownership housing_multi_unit median_val_owner_occupied\n#> 1         22135           77.5                7.2                    133900\n#> 2        104061           76.7               22.6                    177200\n#>   households persons_per_household per_capita_income median_household_income\n#> 1      19718                   2.7             24568                   53255\n#> 2      69476                   2.5             26469                   50147\n#>   poverty private_nonfarm_establishments private_nonfarm_employment\n#> 1    10.6                            877                      10628\n#> 2    12.2                           4812                      52233\n#>   percent_change_private_nonfarm_employment nonemployment_establishments firms\n#> 1                                      16.6                         2971  4067\n#> 2                                      17.4                        14175 19035\n#>   black_owned_firms native_owned_firms asian_owned_firms pac_isl_owned_firms\n#> 1              15.2                 NA               1.3                  NA\n#> 2               2.7                0.4               1.0                  NA\n#>   hispanic_owned_firms women_owned_firms manufacturer_shipments_2007\n#> 1                  0.7              31.7                          NA\n#> 2                  1.3              27.3                     1410273\n#>   mercent_whole_sales_2007   sales sales_per_capita accommodation_food_service\n#> 1                       NA  598175            12003                      88157\n#> 2                       NA 2966489            17166                     436955\n#>   building_permits fed_spending    area density\n#> 1              191       331142  594.44    91.8\n#> 2              696      1119082 1589.78   114.6\n\n### 일부 통계 요약 정보\nUS_census %>% summary() %>% head(2)\n#>     state               name                FIPS          pop2010       \n#>  Length:3143        Length:3143        Min.   : 1001   Min.   :     82  \n#>  Class :character   Class :character   1st Qu.:18178   1st Qu.:  11104  \n#>     pop2000         age_under_5      age_under_18    age_over_65   \n#>  Min.   :     67   Min.   : 0.000   Min.   : 0.00   Min.   : 3.50  \n#>  1st Qu.:  11210   1st Qu.: 5.500   1st Qu.:21.40   1st Qu.:13.10  \n#>      female          white           black            native      \n#>  Min.   :27.90   Min.   : 2.70   Min.   : 0.000   Min.   : 0.000  \n#>  1st Qu.:49.60   1st Qu.:75.25   1st Qu.: 0.500   1st Qu.: 0.200  \n#>      asian           pac_isl       two_plus_races      hispanic     \n#>  Min.   : 0.000   Min.   : 0.000   Min.   : 0.100   Min.   : 0.000  \n#>  1st Qu.: 0.300   1st Qu.: 0.000   1st Qu.: 1.100   1st Qu.: 1.600  \n#>  white_not_hispanic no_move_in_one_plus_year  foreign_born   \n#>  Min.   : 2.70      Min.   : 51.6            Min.   : 0.000  \n#>  1st Qu.:66.95      1st Qu.: 83.2            1st Qu.: 1.200  \n#>  foreign_spoken_at_home    hs_grad        bachelors        veterans     \n#>  Min.   : 0.000         Min.   :47.90   Min.   : 3.70   Min.   :     0  \n#>  1st Qu.: 2.800         1st Qu.:78.40   1st Qu.:13.10   1st Qu.:   958  \n#>  mean_work_travel housing_units     home_ownership  housing_multi_unit\n#>  Min.   : 4.30    Min.   :     50   Min.   : 0.00   Min.   : 0.00     \n#>  1st Qu.:19.00    1st Qu.:   5416   1st Qu.:69.50   1st Qu.: 6.10     \n#>  median_val_owner_occupied   households      persons_per_household\n#>  Min.   :      0           Min.   :     22   Min.   :1.100        \n#>  1st Qu.:  80200           1st Qu.:   4260   1st Qu.:2.370        \n#>  per_capita_income median_household_income    poverty    \n#>  Min.   : 7772     Min.   : 19351          Min.   : 0.0  \n#>  1st Qu.:19030     1st Qu.: 36952          1st Qu.:11.0  \n#>  private_nonfarm_establishments private_nonfarm_employment\n#>  Min.   :     0                 Min.   :      0           \n#>  1st Qu.:   229                 1st Qu.:   2109           \n#>  percent_change_private_nonfarm_employment nonemployment_establishments\n#>  Min.   :-83.2000                          Min.   :    21              \n#>  1st Qu.:-12.0000                          1st Qu.:   729              \n#>      firms         black_owned_firms native_owned_firms asian_owned_firms\n#>  Min.   :     27   Min.   : 0.200    Min.   : 0.200     Min.   : 0.300   \n#>  1st Qu.:   1074   1st Qu.: 2.100    1st Qu.: 0.525     1st Qu.: 1.400   \n#>  pac_isl_owned_firms hispanic_owned_firms women_owned_firms\n#>  Min.   : 0.0000     Min.   : 0.300       Min.   : 6.50    \n#>  1st Qu.: 0.1000     1st Qu.: 1.400       1st Qu.:22.70    \n#>  manufacturer_shipments_2007 mercent_whole_sales_2007     sales          \n#>  Min.   :        0           Min.   :        0        Min.   :        0  \n#>  1st Qu.:        0           1st Qu.:    42125        1st Qu.:    79988  \n#>  sales_per_capita accommodation_food_service building_permits\n#>  Min.   :    0    Min.   :       0           Min.   :    0   \n#>  1st Qu.: 6993    1st Qu.:    9349           1st Qu.:    5   \n#>   fed_spending           area             density       \n#>  Min.   :       0   Min.   :     2.0   Min.   :    0.0  \n#>  1st Qu.:  102922   1st Qu.:   430.7   1st Qu.:   16.9\n\n### 일부 컬럼 클래스(타입) 확인\nUS_census %>% sapply(class) %>% head()\n#>       state        name        FIPS     pop2010     pop2000 age_under_5 \n#> \"character\" \"character\"   \"integer\"   \"integer\"   \"integer\"   \"numeric\"\n\n### 일부 컬럼 자료형 확인\nUS_census %>% sapply(typeof) %>% head()\n#>       state        name        FIPS     pop2010     pop2000 age_under_5 \n#> \"character\" \"character\"   \"integer\"   \"integer\"   \"integer\"    \"double\"\n\n\n\n1.3 전처리\n\n### Texas주의 인구 밀도가 높은 상위3개, 하위3개, 나머지의 랜덤한 지역 d.f 만들기\ntx_counties <- US_census %>%\n  filter(state == 'Texas') %>%             # Texas지역만 필터\n  select(name, pop2010) %>%                # name,pop2010 열만 선택\n  mutate(county = gsub(' County','',name), # name 열에서 County문자열을 공백으로 대체하고 county열 생성\n         popratio = pop2010/median(pop2010)) %>% # pop2010 값에서 pop2010의 중앙값을 나눈 비율을 popratio열로 생성\n  arrange(desc(popratio)) %>%              # popratio 열을 내림차순 정렬\n  mutate(index = 1:n(),\n         label = ifelse(index<=3 | index > n()-3 | runif(n()) < .04, county,'')) \n# index값이 3이하, 행의 수에서 3을 뺀 값보다 크거나, 0,1사이의 값을 뽑아 0.04보다 작으면 'county'값을 가지고 그렇지 않으면 ''(빈문자열) 값 가짐\n\ntx_counties %>% head()\n#>             name pop2010  county  popratio index   label\n#> 1  Harris County 4092459  Harris 222.64616     1  Harris\n#> 2  Dallas County 2368139  Dallas 128.83624     2  Dallas\n#> 3 Tarrant County 1809034 Tarrant  98.41869     3 Tarrant\n#> 4   Bexar County 1714773   Bexar  93.29052     4        \n#> 5  Travis County 1024266  Travis  55.72417     5        \n#> 6 El Paso County  800647 El Paso  43.55840     6"
  },
  {
    "objectID": "posts/Data_Visualize_Ch4/figure 3.5 ~ 3.6.html#figure-3.5",
    "href": "posts/Data_Visualize_Ch4/figure 3.5 ~ 3.6.html#figure-3.5",
    "title": "Data_Visualization CH4",
    "section": "2 Figure 3.5",
    "text": "2 Figure 3.5\n\n사용 데이터셋 : tx_counties\nx = index, y = popratio\ngeom_hline\n\n수평선 위치 : 0\n라인 종류 : 2 (눈금)\n색상 ‘grey40’\n\ngeom_point\n\n점 크기 : 1\n점 색상 : ‘royalblue’\n\ngeom_text_repel\n\n라벨 : ‘label’\n라벨과 지점 사이 선의 최소 길이 : 0\n겹치는 라벨 제어의 최대 수 : 100\n\n테마 : 밝게\n플롯 영역의 테두리(border) 제거\n\n\nggplot(tx_counties, aes(x=index, y=popratio)) +\n  geom_hline(yintercept = 0,linetype=2, color = 'grey40') +\n  geom_point(size=1, color='royalblue')  +\n  geom_text_repel(aes(label=label),\n                  min.segment.length = 0,\n                  max.overlaps = 100) +\n  theme_light() +\n  theme(panel.border = element_blank())"
  },
  {
    "objectID": "posts/Data_Visualize_Ch4/figure 3.5 ~ 3.6.html#fugiure-3.6",
    "href": "posts/Data_Visualize_Ch4/figure 3.5 ~ 3.6.html#fugiure-3.6",
    "title": "Data_Visualization CH4",
    "section": "3 Fugiure 3.6",
    "text": "3 Fugiure 3.6\n\n사용 데이터셋 : tx_counties\nx = index, y = popratio\ngeom_hline\n\n수평선 위치 : 1\n라인 종류 : 2 (눈금)\n색상 ‘grey40’\n\ngeom_point\n\n점 크기 : 1\n점 색상 : ‘royalblue’\n\ngeom_text_repel\n\n라벨 : ‘label’\n라벨과 지점 사이 선의 최소 길이 : 0\n겹치는 라벨 제어의 최대 수 : 100\n\nscale_y_log10\n\ny축 이름\ny축 간격\ny축 간격 라벨 : label_log10\n\nscale_x_continuous\n\nx축 이름\nx축 간격 : NULL (자동)\n\n테마 : 밝게\n플롯 영역의 테두리(border) 제거\n\n\n### -2부터 2까지의 지수값을 포함하는 문자열을 표현식으로 생성\nlabel_log10 <- sapply(-2:2,function(i) as.expression(bquote(10^ .(i))))\nlabel_log10\n#> expression(10^-2L, 10^-1L, 10^0L, 10^1L, 10^2L)\n\nggplot(tx_counties, aes(x = index, y = popratio)) +\n  geom_hline(yintercept = 1,linetype = 2, color = 'grey40') +\n  geom_point(size = 1, color = 'royalblue')  +\n  geom_text_repel(aes(label=label),\n                  min.segment.length = 0,\n                  max.overlaps = 100) +\n  scale_y_log10(name='population number / median',\n                breaks = 10^(-2:2),\n                labels= label_log10) +\n  scale_x_continuous(name = 'Texas counties , from most to least populous',\n                     breaks = NULL) +\n  theme_light() +\n  theme(panel.border = element_blank())"
  },
  {
    "objectID": "posts/Data_Visualize_Ch4/figure 3.5 ~ 3.6.html#예제",
    "href": "posts/Data_Visualize_Ch4/figure 3.5 ~ 3.6.html#예제",
    "title": "Data_Visualization CH4",
    "section": "4 예제",
    "text": "4 예제\n\n2023년 시군구 인구수 Figure 3.6\n\n\n4.1 데이터 불러오기, 파악\n\n행정구역 시군구 별 주민등록세대수.csv (2022.11 ~ 2023.02)\n\n\ndata_202302 <- read.csv('C:/Users/seong taek/Desktop/3-1 DataVisualize/data_visualize/행정구역_시군구_별_주민등록세대수_202302.csv')\n\n### 차원 파악\ndata_202302 %>% dim()\n#> [1] 274   5\n\n### 앞부분 훑어보기\ndata_202302 %>% head()\n#>   행정구역.시군구.별 X2022.11 X2022.12 X2023.01 X2023.02\n#> 1             종로구    72666    72524    72479    72773\n#> 2               중구    63167    63139    63123    63492\n#> 3             용산구   109905   109805   109734   109778\n#> 4             성동구   133435   133305   133293   133517\n#> 5             광진구   169376   169291   169416   169648\n#> 6           동대문구   170154   169873   169716   170766\n\n### 통계 요약 정보\ndata_202302 %>% summary()\n#>  행정구역.시군구.별    X2022.11         X2022.12         X2023.01     \n#>  Length:274         Min.   :     0   Min.   :     0   Min.   :     0  \n#>  Class :character   1st Qu.: 26391   1st Qu.: 26590   1st Qu.: 26546  \n#>  Mode  :character   Median : 81122   Median : 81158   Median : 81177  \n#>                     Mean   :101197   Mean   :101227   Mean   :101274  \n#>                     3rd Qu.:145260   3rd Qu.:145426   3rd Qu.:145706  \n#>                     Max.   :528097   Max.   :528482   Max.   :529082  \n#>     X2023.02     \n#>  Min.   :     0  \n#>  1st Qu.: 26522  \n#>  Median : 81243  \n#>  Mean   :101442  \n#>  3rd Qu.:146308  \n#>  Max.   :530462\n\n### 컬럼 클래스(타입) 확인\ndata_202302 %>% sapply(class) \n#> 행정구역.시군구.별           X2022.11           X2022.12           X2023.01 \n#>        \"character\"          \"integer\"          \"integer\"          \"integer\" \n#>           X2023.02 \n#>          \"integer\"\n\n### 컬럼 자료형 확인\ndata_202302 %>% sapply(typeof)\n#> 행정구역.시군구.별           X2022.11           X2022.12           X2023.01 \n#>        \"character\"          \"integer\"          \"integer\"          \"integer\" \n#>           X2023.02 \n#>          \"integer\"\n\n\n\n4.2 전처리\n\ndata_202302_second <- data_202302 %>%\n  filter(X2023.02 != 0) %>%                        # 0값이 아닌것만 필터링\n  select(행정구역.시군구.별, X2023.02) %>%         # 열 지정 선택        \n  mutate(popratio = X2023.02/median(X2023.02)) %>% # 새로운 컬럼 'popratio' \n  arrange(desc(popratio)) %>%                      # 내림차순 정렬\n  mutate(index = 1:n(),\n         label = ifelse(index<=5 | index > n()-5 | index==median(index), 행정구역.시군구.별,''))\n# index값이 5이하, 행의 수에서 5를 뺀 값보다 크거나, index가 중위수인 index이면 '행정구역.시군구.별' 값을 가지고 그렇지 않으면 ''(빈문자열) 값 가짐\n\ndata_202302_second %>% head()\n#>   행정구역.시군구.별 X2023.02 popratio index  label\n#> 1             수원시   530462 6.043498     1 수원시\n#> 2             고양시   462873 5.273464     2 고양시\n#> 3             창원시   456357 5.199228     3 창원시\n#> 4             용인시   432476 4.927154     4 용인시\n#> 5             성남시   409466 4.665003     5 성남시\n#> 6             청주시   394735 4.497175     6\n\n\n\n4.3 Fugiure 3.6\n\n사용 데이터셋 : data_202302_second\nx = index, y = popratio\ngeom_hline\n\n수평선 위치 : 1\n라인 종류 : 2 (눈금)\n색상 ‘grey40’\n\ngeom_point\n\n점 크기 : 1\n점 색상 : ‘royalblue’\n\ngeom_text_repel\n\n라벨 : ‘label’\n라벨과 지점 사이 선의 최소 길이 : 0\n겹치는 라벨 제어의 최대 수 : 100\n\nscale_y_log10\n\ny축 이름\ny축 간격\ny축 간격 라벨 : label_log10\ny축 범위\n\nscale_x_continuous\n\nx축 이름\nx축 간격 : NULL (자동)\n\n테마 : 밝게\n플롯 영역의 테두리(border) 제거\n\n\n### -2부터 2까지의 지수값을 포함하는 문자열을 표현식으로 생성\nlabel_log10 <- sapply(-2:2,function(i) as.expression(bquote(10^ .(i))))\nlabel_log10\n#> expression(10^-2L, 10^-1L, 10^0L, 10^1L, 10^2L)\n\nggplot(data_202302_second, aes(x = index, y = popratio)) +\n  geom_hline(yintercept = 1,linetype = 2, color = 'grey40') +\n  geom_point(size = 1, color = 'royalblue')  +\n  geom_text_repel(aes(label=label),\n                  min.segment.length = 0,\n                  max.overlaps = 100) +\n  scale_y_log10(name='인구 수/중위수',\n                breaks = 10^(-2:2),\n                labels= label_log10,\n                limits = c(10^-1.3, 10^1.3)) +\n  scale_x_discrete(name = '행정구역(시군구)별 주민등록세대수',\n                     breaks = NULL) +\n  theme_light() +\n  theme(panel.border = element_blank())"
  },
  {
    "objectID": "posts/Data_Visualize_Ch4/figure 3.5 ~ 3.6.html#데이터-시각화-실습-상위-하위지역-그래프-figure-3.5-figure-3.6",
    "href": "posts/Data_Visualize_Ch4/figure 3.5 ~ 3.6.html#데이터-시각화-실습-상위-하위지역-그래프-figure-3.5-figure-3.6",
    "title": "Data_Visualization CH4",
    "section": "1 데이터 시각화 실습 : 상위, 하위지역 그래프 Figure 3.5, Figure 3.6",
    "text": "1 데이터 시각화 실습 : 상위, 하위지역 그래프 Figure 3.5, Figure 3.6\n\n1.1 패키지 불러오기\n\nlibrary(ggplot2)\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\nlibrary(ggrepel) # geom_text_repel()\n\n\n\n1.2 데이터 불러오기, 파악\n\nUS_census.csv (미국 인구 통계정보 데이터셋)\n\n\nUS_census <- read.csv('C:/Users/seong taek/Desktop/3-1 DataVisualize/data_visualize/US_census.csv')\n\n### 차원 파악\nUS_census %>% dim()\n#> [1] 3143   53\n\n### 앞부분 훑어보기\nUS_census %>% head(2)\n#>     state           name FIPS pop2010 pop2000 age_under_5 age_under_18\n#> 1 Alabama Autauga County 1001   54571   43671         6.6         26.8\n#> 2 Alabama Baldwin County 1003  182265  140415         6.1         23.0\n#>   age_over_65 female white black native asian pac_isl two_plus_races hispanic\n#> 1        12.0   51.3  78.5  17.7    0.4   0.9      NA            1.6      2.4\n#> 2        16.8   51.1  85.7   9.4    0.7   0.7      NA            1.5      4.4\n#>   white_not_hispanic no_move_in_one_plus_year foreign_born\n#> 1               77.2                     86.3          2.0\n#> 2               83.5                     83.0          3.6\n#>   foreign_spoken_at_home hs_grad bachelors veterans mean_work_travel\n#> 1                    3.7    85.3      21.7     5817             25.1\n#> 2                    5.5    87.6      26.8    20396             25.8\n#>   housing_units home_ownership housing_multi_unit median_val_owner_occupied\n#> 1         22135           77.5                7.2                    133900\n#> 2        104061           76.7               22.6                    177200\n#>   households persons_per_household per_capita_income median_household_income\n#> 1      19718                   2.7             24568                   53255\n#> 2      69476                   2.5             26469                   50147\n#>   poverty private_nonfarm_establishments private_nonfarm_employment\n#> 1    10.6                            877                      10628\n#> 2    12.2                           4812                      52233\n#>   percent_change_private_nonfarm_employment nonemployment_establishments firms\n#> 1                                      16.6                         2971  4067\n#> 2                                      17.4                        14175 19035\n#>   black_owned_firms native_owned_firms asian_owned_firms pac_isl_owned_firms\n#> 1              15.2                 NA               1.3                  NA\n#> 2               2.7                0.4               1.0                  NA\n#>   hispanic_owned_firms women_owned_firms manufacturer_shipments_2007\n#> 1                  0.7              31.7                          NA\n#> 2                  1.3              27.3                     1410273\n#>   mercent_whole_sales_2007   sales sales_per_capita accommodation_food_service\n#> 1                       NA  598175            12003                      88157\n#> 2                       NA 2966489            17166                     436955\n#>   building_permits fed_spending    area density\n#> 1              191       331142  594.44    91.8\n#> 2              696      1119082 1589.78   114.6\n\n### 일부 통계 요약 정보\nUS_census %>% summary() %>% head(2)\n#>     state               name                FIPS          pop2010       \n#>  Length:3143        Length:3143        Min.   : 1001   Min.   :     82  \n#>  Class :character   Class :character   1st Qu.:18178   1st Qu.:  11104  \n#>     pop2000         age_under_5      age_under_18    age_over_65   \n#>  Min.   :     67   Min.   : 0.000   Min.   : 0.00   Min.   : 3.50  \n#>  1st Qu.:  11210   1st Qu.: 5.500   1st Qu.:21.40   1st Qu.:13.10  \n#>      female          white           black            native      \n#>  Min.   :27.90   Min.   : 2.70   Min.   : 0.000   Min.   : 0.000  \n#>  1st Qu.:49.60   1st Qu.:75.25   1st Qu.: 0.500   1st Qu.: 0.200  \n#>      asian           pac_isl       two_plus_races      hispanic     \n#>  Min.   : 0.000   Min.   : 0.000   Min.   : 0.100   Min.   : 0.000  \n#>  1st Qu.: 0.300   1st Qu.: 0.000   1st Qu.: 1.100   1st Qu.: 1.600  \n#>  white_not_hispanic no_move_in_one_plus_year  foreign_born   \n#>  Min.   : 2.70      Min.   : 51.6            Min.   : 0.000  \n#>  1st Qu.:66.95      1st Qu.: 83.2            1st Qu.: 1.200  \n#>  foreign_spoken_at_home    hs_grad        bachelors        veterans     \n#>  Min.   : 0.000         Min.   :47.90   Min.   : 3.70   Min.   :     0  \n#>  1st Qu.: 2.800         1st Qu.:78.40   1st Qu.:13.10   1st Qu.:   958  \n#>  mean_work_travel housing_units     home_ownership  housing_multi_unit\n#>  Min.   : 4.30    Min.   :     50   Min.   : 0.00   Min.   : 0.00     \n#>  1st Qu.:19.00    1st Qu.:   5416   1st Qu.:69.50   1st Qu.: 6.10     \n#>  median_val_owner_occupied   households      persons_per_household\n#>  Min.   :      0           Min.   :     22   Min.   :1.100        \n#>  1st Qu.:  80200           1st Qu.:   4260   1st Qu.:2.370        \n#>  per_capita_income median_household_income    poverty    \n#>  Min.   : 7772     Min.   : 19351          Min.   : 0.0  \n#>  1st Qu.:19030     1st Qu.: 36952          1st Qu.:11.0  \n#>  private_nonfarm_establishments private_nonfarm_employment\n#>  Min.   :     0                 Min.   :      0           \n#>  1st Qu.:   229                 1st Qu.:   2109           \n#>  percent_change_private_nonfarm_employment nonemployment_establishments\n#>  Min.   :-83.2000                          Min.   :    21              \n#>  1st Qu.:-12.0000                          1st Qu.:   729              \n#>      firms         black_owned_firms native_owned_firms asian_owned_firms\n#>  Min.   :     27   Min.   : 0.200    Min.   : 0.200     Min.   : 0.300   \n#>  1st Qu.:   1074   1st Qu.: 2.100    1st Qu.: 0.525     1st Qu.: 1.400   \n#>  pac_isl_owned_firms hispanic_owned_firms women_owned_firms\n#>  Min.   : 0.0000     Min.   : 0.300       Min.   : 6.50    \n#>  1st Qu.: 0.1000     1st Qu.: 1.400       1st Qu.:22.70    \n#>  manufacturer_shipments_2007 mercent_whole_sales_2007     sales          \n#>  Min.   :        0           Min.   :        0        Min.   :        0  \n#>  1st Qu.:        0           1st Qu.:    42125        1st Qu.:    79988  \n#>  sales_per_capita accommodation_food_service building_permits\n#>  Min.   :    0    Min.   :       0           Min.   :    0   \n#>  1st Qu.: 6993    1st Qu.:    9349           1st Qu.:    5   \n#>   fed_spending           area             density       \n#>  Min.   :       0   Min.   :     2.0   Min.   :    0.0  \n#>  1st Qu.:  102922   1st Qu.:   430.7   1st Qu.:   16.9\n\n### 일부 컬럼 클래스(타입) 확인\nUS_census %>% sapply(class) %>% head()\n#>       state        name        FIPS     pop2010     pop2000 age_under_5 \n#> \"character\" \"character\"   \"integer\"   \"integer\"   \"integer\"   \"numeric\"\n\n### 일부 컬럼 자료형 확인\nUS_census %>% sapply(typeof) %>% head()\n#>       state        name        FIPS     pop2010     pop2000 age_under_5 \n#> \"character\" \"character\"   \"integer\"   \"integer\"   \"integer\"    \"double\"\n\n\n\n1.3 전처리\n\n### Texas주의 인구 밀도가 높은 상위3개, 하위3개, 나머지의 랜덤한 지역 d.f 만들기\ntx_counties <- US_census %>%\n  filter(state == 'Texas') %>%             # Texas지역만 필터\n  select(name, pop2010) %>%                # name,pop2010 열만 선택\n  mutate(county = gsub(' County','',name), # name 열에서 County문자열을 공백으로 대체하고 county열 생성\n         popratio = pop2010/median(pop2010)) %>% # pop2010 값에서 pop2010의 중앙값을 나눈 비율을 popratio열로 생성\n  arrange(desc(popratio)) %>%              # popratio 열을 내림차순 정렬\n  mutate(index = 1:n(),\n         label = ifelse(index<=3 | index > n()-3 | runif(n()) < .04, county,'')) \n# index값이 3이하, 행의 수에서 3을 뺀 값보다 크거나, 0,1사이의 값을 뽑아 0.04보다 작으면 'county'값을 가지고 그렇지 않으면 ''(빈문자열) 값 가짐\n\ntx_counties %>% head()\n#>             name pop2010  county  popratio index   label\n#> 1  Harris County 4092459  Harris 222.64616     1  Harris\n#> 2  Dallas County 2368139  Dallas 128.83624     2  Dallas\n#> 3 Tarrant County 1809034 Tarrant  98.41869     3 Tarrant\n#> 4   Bexar County 1714773   Bexar  93.29052     4        \n#> 5  Travis County 1024266  Travis  55.72417     5        \n#> 6 El Paso County  800647 El Paso  43.55840     6"
  },
  {
    "objectID": "posts/Data_Visualize_Ch3/figure 3.2.html#데이터-시각화-실습-그래프-합치기-figure-3.2",
    "href": "posts/Data_Visualize_Ch3/figure 3.2.html#데이터-시각화-실습-그래프-합치기-figure-3.2",
    "title": "Data_Visualization CH3",
    "section": "1 데이터 시각화 실습 : 그래프 합치기 Figure 3.2",
    "text": "1 데이터 시각화 실습 : 그래프 합치기 Figure 3.2\n\n1.1 패키지 불러오기\n\nlibrary(ggplot2)\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\nlibrary(ggrepel)\nlibrary(cowplot) # plot_grid\n\n\n\n1.2 데이터 불러오기, 파악\n\nncdc_normals.csv (날짜별 온도 등 데이터셋)\n\n\nncdc_normals <- read.csv('C:/Users/seong taek/Desktop/3-1 DataVisualize/data_visualize/ncdc_normals.csv')\n\n### 차원 파악\nncdc_normals %>% dim()\n#> [1] 2745366       6\n\n### 앞부분 훑어보기\nncdc_normals %>% head()\n#>    station_id month day temperature flag       date\n#> 1 AQW00061705     1   1        82.4    C 0000-01-01\n#> 2 AQW00061705     1   2        82.4    C 0000-01-02\n#> 3 AQW00061705     1   3        82.4    C 0000-01-03\n#> 4 AQW00061705     1   4        82.4    C 0000-01-04\n#> 5 AQW00061705     1   5        82.4    C 0000-01-05\n#> 6 AQW00061705     1   6        82.4    C 0000-01-06\n\n### 통계 요약 정보\nncdc_normals %>% summary()\n#>   station_id            month             day         temperature    \n#>  Length:2745366     Min.   : 1.000   Min.   : 1.00   Min.   :-21.80  \n#>  Class :character   1st Qu.: 4.000   1st Qu.: 8.00   1st Qu.: 39.70  \n#>  Mode  :character   Median : 7.000   Median :16.00   Median : 54.60  \n#>                     Mean   : 6.514   Mean   :15.76   Mean   : 53.17  \n#>                     3rd Qu.:10.000   3rd Qu.:23.00   3rd Qu.: 68.10  \n#>                     Max.   :12.000   Max.   :31.00   Max.   :103.20  \n#>      flag               date          \n#>  Length:2745366     Length:2745366    \n#>  Class :character   Class :character  \n#>  Mode  :character   Mode  :character  \n#>                                       \n#>                                       \n#> \n\n### 각 컬럼 클래스(타입) 확인\nncdc_normals %>% sapply(class)\n#>  station_id       month         day temperature        flag        date \n#> \"character\"   \"integer\"   \"integer\"   \"numeric\" \"character\" \"character\"\n\n### 각 컬럼 자료형 확인\nncdc_normals %>% sapply(typeof)\n#>  station_id       month         day temperature        flag        date \n#> \"character\"   \"integer\"   \"integer\"    \"double\" \"character\" \"character\"\n\n\n\n1.3 전처리\n\n### staion id와 location 지정 선택한 d.f 만들기\nstation_loc <- data.frame(station_id = c(\"USW00014819\",\"USC00042319\",\"USW00093107\",\"USW00012918\"),\n                          location = c(\"Chicago\",\"Death Valley\",\"San Diego\",\"Houston\"))\n\nstation_loc %>% head()\n#>    station_id     location\n#> 1 USW00014819      Chicago\n#> 2 USC00042319 Death Valley\n#> 3 USW00093107    San Diego\n#> 4 USW00012918      Houston\n\n\n### station_id로 ncdc_normals와 station_loc 두 컬럼을 `inner_join`\ntemps_long <- ncdc_normals %>% inner_join(station_loc, by=\"station_id\")\ntemps_long %>% head()\n#>    station_id month day temperature flag       date     location\n#> 1 USC00042319     1   1        51.0    S 0000-01-01 Death Valley\n#> 2 USC00042319     1   2        51.2    S 0000-01-02 Death Valley\n#> 3 USC00042319     1   3        51.3    S 0000-01-03 Death Valley\n#> 4 USC00042319     1   4        51.4    S 0000-01-04 Death Valley\n#> 5 USC00042319     1   5        51.6    S 0000-01-05 Death Valley\n#> 6 USC00042319     1   6        51.7    S 0000-01-06 Death Valley\ntemps_long %>% sapply(class)\n#>  station_id       month         day temperature        flag        date \n#> \"character\"   \"integer\"   \"integer\"   \"numeric\" \"character\" \"character\" \n#>    location \n#> \"character\"\n\n### temps_long의 `date`의 타입을 Cha → Date로 변환\ntemps_long$date <- temps_long$date %>% as.Date('%Y-%m-%d')\ntemps_long %>% sapply(class)\n#>  station_id       month         day temperature        flag        date \n#> \"character\"   \"integer\"   \"integer\"   \"numeric\" \"character\"      \"Date\" \n#>    location \n#> \"character\"\n\n\n### Houston 필터 \ndata_Houston <- temps_long %>% filter(location=='Houston')\ndata_Houston %>% head()\n#>    station_id month day temperature flag       date location\n#> 1 USW00012918     1   1        53.9    S 0000-01-01  Houston\n#> 2 USW00012918     1   2        53.8    S 0000-01-02  Houston\n#> 3 USW00012918     1   3        53.8    S 0000-01-03  Houston\n#> 4 USW00012918     1   4        53.8    S 0000-01-04  Houston\n#> 5 USW00012918     1   5        53.8    S 0000-01-05  Houston\n#> 6 USW00012918     1   6        53.7    S 0000-01-06  Houston\ndata_Houston %>% tail()\n#>      station_id month day temperature flag       date location\n#> 361 USW00012918    12  26        54.2    C 0000-12-26  Houston\n#> 362 USW00012918    12  27        54.1    C 0000-12-27  Houston\n#> 363 USW00012918    12  28        54.1    C 0000-12-28  Houston\n#> 364 USW00012918    12  29        54.0    C 0000-12-29  Houston\n#> 365 USW00012918    12  30        53.9    C 0000-12-30  Houston\n#> 366 USW00012918    12  31        53.9    C 0000-12-31  Houston\n\n\n\n1.4 x축 눈금 설정\n\ndate_s <- '0000-01-01' %>% as.Date('%Y-%m-%d')\ndate_e <- '0001-01-01' %>% as.Date('%Y-%m-%d')\nbreak_date <- seq(date_s, date_e, by = '3 month')"
  },
  {
    "objectID": "posts/Data_Visualize_Ch5/figure 3.10.html",
    "href": "posts/Data_Visualize_Ch5/figure 3.10.html",
    "title": "Data_Visualization CH5",
    "section": "",
    "text": "HTML파일로 보기\nFigure 3.10"
  },
  {
    "objectID": "posts/Data_Visualize_Ch5/figure 3.10.html#데이터-시각화-실습-원형-그래프-figure-3.10",
    "href": "posts/Data_Visualize_Ch5/figure 3.10.html#데이터-시각화-실습-원형-그래프-figure-3.10",
    "title": "Data_Visualization CH5",
    "section": "1 데이터 시각화 실습 : 원형 그래프 Figure 3.10",
    "text": "1 데이터 시각화 실습 : 원형 그래프 Figure 3.10\n\n1.1 패키지 불러오기\n\nlibrary(ggplot2)\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\n\n\n\n1.2 데이터 불러오기, 파악\n\nncdc_normals.csv (날짜별 온도 등 데이터셋)\n\n\nncdc_normals <- read.csv('C:/Users/seong taek/Desktop/3-1 DataVisualize/data_visualize/ncdc_normals.csv')\n\n### 차원 파악\nncdc_normals %>% dim()\n#> [1] 2745366       6\n\n### 앞부분 훑어보기\nncdc_normals %>% head()\n#>    station_id month day temperature flag       date\n#> 1 AQW00061705     1   1        82.4    C 0000-01-01\n#> 2 AQW00061705     1   2        82.4    C 0000-01-02\n#> 3 AQW00061705     1   3        82.4    C 0000-01-03\n#> 4 AQW00061705     1   4        82.4    C 0000-01-04\n#> 5 AQW00061705     1   5        82.4    C 0000-01-05\n#> 6 AQW00061705     1   6        82.4    C 0000-01-06\n\n### 통계 요약 정보\nncdc_normals %>% summary()\n#>   station_id            month             day         temperature    \n#>  Length:2745366     Min.   : 1.000   Min.   : 1.00   Min.   :-21.80  \n#>  Class :character   1st Qu.: 4.000   1st Qu.: 8.00   1st Qu.: 39.70  \n#>  Mode  :character   Median : 7.000   Median :16.00   Median : 54.60  \n#>                     Mean   : 6.514   Mean   :15.76   Mean   : 53.17  \n#>                     3rd Qu.:10.000   3rd Qu.:23.00   3rd Qu.: 68.10  \n#>                     Max.   :12.000   Max.   :31.00   Max.   :103.20  \n#>      flag               date          \n#>  Length:2745366     Length:2745366    \n#>  Class :character   Class :character  \n#>  Mode  :character   Mode  :character  \n#>                                       \n#>                                       \n#> \n\n### 각 컬럼 클래스(타입) 확인\nncdc_normals %>% sapply(class)\n#>  station_id       month         day temperature        flag        date \n#> \"character\"   \"integer\"   \"integer\"   \"numeric\" \"character\" \"character\"\n\n### 각 컬럼 자료형 확인\nncdc_normals %>% sapply(typeof)\n#>  station_id       month         day temperature        flag        date \n#> \"character\"   \"integer\"   \"integer\"    \"double\" \"character\" \"character\"\n\n\n\n1.3 전처리\n\n### staion id와 location 지정 선택한 d.f 만들기\nstation_loc <- data.frame(station_id = c(\"USW00014819\",\"USC00042319\",\"USW00093107\",\"USW00012918\"),\n                          location = c(\"Chicago\",\"Death Valley\",\"San Diego\",\"Houston\"))\n\nstation_loc %>% head()\n#>    station_id     location\n#> 1 USW00014819      Chicago\n#> 2 USC00042319 Death Valley\n#> 3 USW00093107    San Diego\n#> 4 USW00012918      Houston\n\n\n### station_id로 ncdc_normals와 station_loc 두 컬럼을 `inner_join`\ntemps_long <- ncdc_normals %>% inner_join(station_loc, by=\"station_id\")\ntemps_long %>% head()\n#>    station_id month day temperature flag       date     location\n#> 1 USC00042319     1   1        51.0    S 0000-01-01 Death Valley\n#> 2 USC00042319     1   2        51.2    S 0000-01-02 Death Valley\n#> 3 USC00042319     1   3        51.3    S 0000-01-03 Death Valley\n#> 4 USC00042319     1   4        51.4    S 0000-01-04 Death Valley\n#> 5 USC00042319     1   5        51.6    S 0000-01-05 Death Valley\n#> 6 USC00042319     1   6        51.7    S 0000-01-06 Death Valley\ntemps_long %>% sapply(class)\n#>  station_id       month         day temperature        flag        date \n#> \"character\"   \"integer\"   \"integer\"   \"numeric\" \"character\" \"character\" \n#>    location \n#> \"character\"\n\n### temps_long의 `date`의 타입을 Cha → Date로 변환\ntemps_long$date <- temps_long$date %>% as.Date('%Y-%m-%d')\ntemps_long %>% sapply(class)\n#>  station_id       month         day temperature        flag        date \n#> \"character\"   \"integer\"   \"integer\"   \"numeric\" \"character\"      \"Date\" \n#>    location \n#> \"character\"\n\n\n\n1.4 x축 눈금 설정\n\ndate_s <- '0000-01-01' %>% as.Date('%Y-%m-%d')\ndate_e <- '0001-01-01' %>% as.Date('%Y-%m-%d')\nbreak_date <- seq.Date(date_s, date_e, by = '3 month')\ndate_lab <- format(break_date, '%B')\n\n\n\n1.5 figure 3.10\n\n사용 데이터셋 : temps_long\nx = date, y = temperature\ncolor : location별\ngeom_line\n\n선 굵기 : 1.2\n\nscale_x_date\n\n이름 : ‘month’\n간격 : break_date (3개월)\n간격 라벨 (1월 ~ 1월)\n\nscale_y_continuous\n\n이름 : ‘temperature’\n범위 : 0 ~ 105\n\ncoord_polar\n\n사용 각도 변수 : x축\n시작 각도 : 180도\n반시계 방향\n\n테마 : 밝게\n플롯 영역의 테두리(border) 제거\n\n\nggplot(temps_long, aes(x = date, y = temperature, color = location)) +\n  geom_line(linewidth = 1.2) +\n  scale_x_date(name='month',\n                breaks =break_date,\n                labels= date_lab) +\n  scale_y_continuous(name = 'temperature',\n                     limits = c(0, 105)) +\n  coord_polar(theta = 'x',start = pi, direction = -1) + # 6시 위치에서 반시계 방향 (0,1이면 12시 위치에서 시계방향)\n  theme_light() +\n  theme(panel.border = element_blank())"
  },
  {
    "objectID": "posts/Data_Visualize_Ch5/figure 3.10.html#예제",
    "href": "posts/Data_Visualize_Ch5/figure 3.10.html#예제",
    "title": "Data_Visualization CH5",
    "section": "2 예제",
    "text": "2 예제\n\n2022년 기상청 자료 Figure 3.10\n\n\n2.1 데이터 불러오기, 파악\n\nOBS_ASOS_DD_20230322080932.csv (2022년 기상청 자료)\n\n\ndata_2022 <- read.csv('C:/Users/seong taek/Desktop/3-1 DataVisualize/data_visualize/OBS_ASOS_DD_20230322080932.csv', fileEncoding = 'cp949')\n\n### 차원 파악\ndata_2022 %>% dim()\n#> [1] 2555    6\n\n### 앞부분 훑어보기\ndata_2022 %>% head()\n#>   지점 지점명       일시 평균기온..C. 최저기온..C. 최고기온..C.\n#> 1  108   서울 2022-01-01         -4.3        -10.2          2.3\n#> 2  108   서울 2022-01-02         -1.3         -5.2          3.0\n#> 3  108   서울 2022-01-03         -1.9         -8.0          2.5\n#> 4  108   서울 2022-01-04         -2.5         -5.6          1.0\n#> 5  108   서울 2022-01-05         -2.8         -7.8          1.9\n#> 6  108   서울 2022-01-06         -2.2         -5.9          3.3\n\n### 통계 요약 정보\ndata_2022 %>% summary()\n#>       지점          지점명              일시            평균기온..C.   \n#>  Min.   :108.0   Length:2555        Length:2555        Min.   :-11.80  \n#>  1st Qu.:133.0   Class :character   Class :character   1st Qu.:  8.20  \n#>  Median :185.0   Mode  :character   Mode  :character   Median : 16.40  \n#>  Mean   :175.1                                         Mean   : 15.27  \n#>  3rd Qu.:189.0                                         3rd Qu.: 23.00  \n#>  Max.   :239.0                                         Max.   : 32.20  \n#>                                                        NA's   :2       \n#>   최저기온..C.      최고기온..C.  \n#>  Min.   :-13.800   Min.   :-8.60  \n#>  1st Qu.:  4.225   1st Qu.:12.30  \n#>  Median : 12.600   Median :20.75  \n#>  Mean   : 11.626   Mean   :19.49  \n#>  3rd Qu.: 19.800   3rd Qu.:27.20  \n#>  Max.   : 28.900   Max.   :37.50  \n#>  NA's   :1         NA's   :1\n\n### 컬럼 클래스(타입) 확인\ndata_2022 %>% sapply(class)\n#>         지점       지점명         일시 평균기온..C. 최저기온..C. 최고기온..C. \n#>    \"integer\"  \"character\"  \"character\"    \"numeric\"    \"numeric\"    \"numeric\"\n\n### 컬럼 자료형 확인\ndata_2022 %>% sapply(typeof)\n#>         지점       지점명         일시 평균기온..C. 최저기온..C. 최고기온..C. \n#>    \"integer\"  \"character\"  \"character\"     \"double\"     \"double\"     \"double\"\n\n### 결측값 있는지 확인\nsum(is.na(data_2022))\n#> [1] 4\n\n### 결측값 제거\ndata_2022 <- data_2022 %>% na.omit()\n\n\n\n2.2 전처리\n\n###  `일시`를 character → date형식으로 변환\ndata_2022$일시 <- data_2022$일시 %>% as.Date('%Y-%m-%d')\n\ndata_2022 %>% sapply(class)\n#>         지점       지점명         일시 평균기온..C. 최저기온..C. 최고기온..C. \n#>    \"integer\"  \"character\"       \"Date\"    \"numeric\"    \"numeric\"    \"numeric\"\n\n### 대전, 서울, 세종, 제주 지역만 추출\ndata_2022 <- data_2022 %>% filter(data_2022$지점명 %in% c('대전','서울','세종','제주'))\n\ndata_2022$지점명 %>% unique()\n#> [1] \"서울\" \"대전\" \"제주\" \"세종\"\n\n\n\n2.3 x축에 표시할 눈금\n\ndata_2022$일시 %>% head()\n#> [1] \"2022-01-01\" \"2022-01-02\" \"2022-01-03\" \"2022-01-04\" \"2022-01-05\"\n#> [6] \"2022-01-06\"\n\ndata_2022$일시 %>% tail()\n#> [1] \"2022-12-26\" \"2022-12-27\" \"2022-12-28\" \"2022-12-29\" \"2022-12-30\"\n#> [6] \"2022-12-31\"\n\ndate_s <- '2022-01-01' %>% as.Date('%Y-%m-%d')\ndate_e <- '2023-01-01' %>% as.Date('%Y-%m-%d')\n\nbreak_date <- seq.Date(date_s, date_e, by='2 month')\nbreak_date\n#> [1] \"2022-01-01\" \"2022-03-01\" \"2022-05-01\" \"2022-07-01\" \"2022-09-01\"\n#> [6] \"2022-11-01\" \"2023-01-01\"\n\n# 월만 문자 값으로 뽑아내기\ndate_lab <- format(break_date, '%B')\ndate_lab\n#> [1] \"1월\"  \"3월\"  \"5월\"  \"7월\"  \"9월\"  \"11월\" \"1월\"\n\n\n\n2.4 figure 3.10\n\n사용 데이터셋 : data_2022\nx=일시, y=평균기온..C.\ncolor : 지점명별\ngeom_line\n\n선 굵기 : 1.2\n\nscale_x_date\n\n이름 : ‘월’\n간격 : break_date (2개월)\n간격 라벨 (1월 ~ 1월)\n\nscale_y_continuous\n\n이름 : ‘평균기온’\n범위 : -20 ~ 35\n\ncoord_polar\n\n사용 각도 변수 : x축\n시작 각도 : 180도\n반시계 방향\n\n테마 : 밝게\n플롯 영역의 테두리(border) 제거\n\n\nggplot(data_2022, aes(x=일시, y=평균기온..C., color=지점명)) +\n  geom_line(linewidth = 1.2) +\n  scale_x_date(name='월',\n                breaks =break_date,\n                labels= date_lab) +\n  scale_y_continuous(name = '평균기온',\n                     limits = c(-20, 35)) +\n  coord_polar(theta = 'x',start = pi, direction = -1) + # 6시 위치에서 반시계 방향 0,1이면 12시 위치에서 시계방향\n  theme_light() +\n  theme(panel.border = element_blank())"
  },
  {
    "objectID": "posts/Data_Visualize_Ch6/figure 4.2.html",
    "href": "posts/Data_Visualize_Ch6/figure 4.2.html",
    "title": "Data_Visualization CH6",
    "section": "",
    "text": "HTML파일로 보기\nFigure 4.2"
  },
  {
    "objectID": "posts/Data_Visualize_Ch6/figure 4.2.html#데이터-시각화-실습-막대-그래프-figure-4.2",
    "href": "posts/Data_Visualize_Ch6/figure 4.2.html#데이터-시각화-실습-막대-그래프-figure-4.2",
    "title": "Data_Visualization CH6",
    "section": "1 데이터 시각화 실습 : 막대 그래프 Figure 4.2",
    "text": "1 데이터 시각화 실습 : 막대 그래프 Figure 4.2\n\n1.1 패키지 불러오기\n\nlibrary(ggplot2)\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\nlibrary(colorspace)\n\n\n\n1.2 RColorBrewer의 모든 컬러 맵 보기\n\n# install.packages(\"RColorBrewer\")\nRColorBrewer::display.brewer.all()\n\n\n\n\n\n\n1.3 데이터 불러오기, 파악\n\nUS_cescus.csv (미국 인구조사 데이터셋)\nUS_regions.csv (미국 지역 분류 정보 데이터셋)\n\n\nUS_census <- read.csv('C:/Users/seong taek/Desktop/3-1 DataVisualize/data_visualize/US_census.csv')\nUS_regions <- read.csv('C:/Users/seong taek/Desktop/3-1 DataVisualize/data_visualize/US_regions.csv')\n\n### 차원 파악\nUS_census %>% dim()\n#> [1] 3143   53\nUS_regions %>% dim()\n#> [1] 51  4\n\n### 앞부분 훑어보기\nUS_census %>% head(2)\n#>     state           name FIPS pop2010 pop2000 age_under_5 age_under_18\n#> 1 Alabama Autauga County 1001   54571   43671         6.6         26.8\n#> 2 Alabama Baldwin County 1003  182265  140415         6.1         23.0\n#>   age_over_65 female white black native asian pac_isl two_plus_races hispanic\n#> 1        12.0   51.3  78.5  17.7    0.4   0.9      NA            1.6      2.4\n#> 2        16.8   51.1  85.7   9.4    0.7   0.7      NA            1.5      4.4\n#>   white_not_hispanic no_move_in_one_plus_year foreign_born\n#> 1               77.2                     86.3          2.0\n#> 2               83.5                     83.0          3.6\n#>   foreign_spoken_at_home hs_grad bachelors veterans mean_work_travel\n#> 1                    3.7    85.3      21.7     5817             25.1\n#> 2                    5.5    87.6      26.8    20396             25.8\n#>   housing_units home_ownership housing_multi_unit median_val_owner_occupied\n#> 1         22135           77.5                7.2                    133900\n#> 2        104061           76.7               22.6                    177200\n#>   households persons_per_household per_capita_income median_household_income\n#> 1      19718                   2.7             24568                   53255\n#> 2      69476                   2.5             26469                   50147\n#>   poverty private_nonfarm_establishments private_nonfarm_employment\n#> 1    10.6                            877                      10628\n#> 2    12.2                           4812                      52233\n#>   percent_change_private_nonfarm_employment nonemployment_establishments firms\n#> 1                                      16.6                         2971  4067\n#> 2                                      17.4                        14175 19035\n#>   black_owned_firms native_owned_firms asian_owned_firms pac_isl_owned_firms\n#> 1              15.2                 NA               1.3                  NA\n#> 2               2.7                0.4               1.0                  NA\n#>   hispanic_owned_firms women_owned_firms manufacturer_shipments_2007\n#> 1                  0.7              31.7                          NA\n#> 2                  1.3              27.3                     1410273\n#>   mercent_whole_sales_2007   sales sales_per_capita accommodation_food_service\n#> 1                       NA  598175            12003                      88157\n#> 2                       NA 2966489            17166                     436955\n#>   building_permits fed_spending    area density\n#> 1              191       331142  594.44    91.8\n#> 2              696      1119082 1589.78   114.6\nUS_regions %>% head()\n#>        state state_abr region           division\n#> 1    Alabama        AL  South East South Central\n#> 2     Alaska        AK   West            Pacific\n#> 3    Arizona        AZ   West           Mountain\n#> 4   Arkansas        AR  South West South Central\n#> 5 California        CA   West            Pacific\n#> 6   Colorado        CO   West           Mountain\n\n\n\n1.4 전처리\n\npopgrowth_df <- US_census %>%                         \n  left_join(US_regions) %>%                         # US_census 기준으로 left_join\n  group_by(region, division, state) %>%             # 그룹화할 컬럼 선택  \n  summarise(pop2000 = sum(pop2000, na.rm = T),      # 그룹화된 컬럼들의 집계값 설정\n            pop2010 = sum(pop2010, na.rm = T),\n            popgrowth = (pop2010 - pop2000)/pop2000,\n            area = sum(area)) %>%\n  arrange(popgrowth) %>%                            # 해당 컬럼 기준으로 오름차순 정렬\n  ungroup() %>%                                     # 그룹화 해제\n  mutate(state = factor(state, levels=state),       # 팩터로 변환, level 지정 \n         region = factor(region, levels=c('West','South','Midwest','Northeast')))\n#> Joining with `by = join_by(state)`\n#> `summarise()` has grouped output by 'region', 'division'. You can override\n#> using the `.groups` argument.\n\npopgrowth_df\n#> # A tibble: 51 × 7\n#>    region    division           state          pop2000  pop2010 popgrowth   area\n#>    <fct>     <chr>              <fct>            <int>    <int>     <dbl>  <dbl>\n#>  1 Midwest   East North Central Michigan       9938444  9883640  -0.00551 56539.\n#>  2 Northeast New England        Rhode Island   1048319  1052567   0.00405  1034.\n#>  3 South     West South Central Louisiana      4468976  4533372   0.0144  43204.\n#>  4 Midwest   East North Central Ohio          11353140 11536504   0.0162  40861.\n#>  5 Northeast Middle Atlantic    New York      18976457 19378102   0.0212  47126.\n#>  6 South     South Atlantic     West Virginia  1808344  1852994   0.0247  24038.\n#>  7 Northeast New England        Vermont         608827   625741   0.0278   9217.\n#>  8 Northeast New England        Massachusetts  6349097  6547629   0.0313   7800.\n#>  9 Midwest   East North Central Illinois      12419293 12830632   0.0331  55519.\n#> 10 Northeast Middle Atlantic    Pennsylvania  12281054 12702379   0.0343  44743.\n#> # … with 41 more rows\n\n### 4개 지방의 색 지정\nregion_colors <- c('#E69F00','#56B4E9','#009E73','#F0E442')\nregion_colors\n#> [1] \"#E69F00\" \"#56B4E9\" \"#009E73\" \"#F0E442\"\n\n\n\n1.5 Figure 4.2\n\n사용 데이터셋 : popgrowth_df\nx=state, y=100*popgrowth\nfill : region별\ngeom_col (막대 그래프)\nscale_y_continuous\n\n이름 : ‘population growth, 2000 to 2010’\n범위 : 0 ~ 0 : 더욱 직관적인 그래프 표현 가능\n라벨 : 백분율 형식\n\nscale_fill_manual\n\nfill 옵션 색상 : region_colors\n\ncoord_flip() : x축, y축 체인지 → 세로방향 긴 막대를 가로방향으로 짧게 표시\n테마 : 밝게\n테마 옵션\n\n그래프 패널의 테두리 제거\ny축 그리드 제거\ny축 레이블 제거\ny축 눈금선 제거\ny축 폰트 크기 설정\n그래프상의 범례 위치 조정\n범례 배경 색상 설정\n\n\n\nggplot(popgrowth_df, aes(x=state, y=100*popgrowth, fill=region)) +\n  geom_col() +\n  scale_y_continuous(name = 'population growth, 2000 to 2010',\n                     labels = scales::percent_format(scale=1),\n                     expand = c(0,0)) +\n  scale_fill_manual(values = region_colors) +\n  coord_flip() +\n  theme_light() +\n  theme(panel.border = element_blank(),\n        panel.grid.major.y = element_blank(),\n        axis.title.y = element_blank(),\n        axis.ticks.length = unit(0, 'pt'),\n        axis.text.y = element_text(size = 6),\n        legend.position = c(.7, .68),\n        legend.background = element_rect(fill = '#ffffffb0'))"
  },
  {
    "objectID": "posts/Data_Visualize_Ch6/figure 4.2.html#예제",
    "href": "posts/Data_Visualize_Ch6/figure 4.2.html#예제",
    "title": "Data_Visualization CH6",
    "section": "2 예제",
    "text": "2 예제\n\n주민등록 인구 및 세대현황 Figure 4.2\n\n\n2.1 데이터 불러오기, 파악\n\n202202_주민등록인구및세대현황.csv\n\n\nkor_202202 <- read.csv('C:/Users/seong taek/Desktop/3-1 DataVisualize/data_visualize/202202_주민등록인구및세대현황.csv')\n\n### 앞부분 훑어보기\nkor_202202 %>%  head()\n#>            행정구역 행정구역_코드 총인구수  세대수 세대당_인구 남자_인구수\n#> 1       서울특별시     1100000000  9508451 4442586        2.14     4615823\n#> 2 서울특별시 종로구    1111000000   144575   73763        1.96       70092\n#> 3   서울특별시 중구    1114000000   122167   63644        1.92       59446\n#> 4 서울특별시 용산구    1117000000   222413  111134        2.00      106881\n#> 5 서울특별시 성동구    1120000000   285137  134286        2.12      138866\n#> 6 서울특별시 광진구    1121500000   340494  168975        2.02      164226\n#>   여자_인구수 남여_비율\n#> 1     4892628      0.94\n#> 2       74483      0.94\n#> 3       62721      0.95\n#> 4      115532      0.93\n#> 5      146271      0.95\n#> 6      176268      0.93\n\n### 차원 파악\nkor_202202 %>% dim()\n#> [1] 291   8\n\n### 컬럼 클래스(타입) 확인\nkor_202202 %>% sapply(class)\n#>      행정구역 행정구역_코드      총인구수        세대수   세대당_인구 \n#>   \"character\"     \"numeric\"     \"numeric\"     \"numeric\"     \"numeric\" \n#>   남자_인구수   여자_인구수     남여_비율 \n#>     \"numeric\"     \"numeric\"     \"numeric\"\n\n### `행정구역_코드`를 numeric → character형식으로 변환\nkor_202202$행정구역_코드 <- kor_202202$행정구역_코드 %>% format()\n\n\n\n2.2 전처리\n\n# 행정구역 컬럼의 1,2번째 자리의 값을 지정해서 추출\n# 행정구역_코드 컬럼의 3,4번째 자리의 값이 0이 아닌것만 추출\n# 해당 열만 선택\n# 해당 열 기준으로 오름차순 정렬\nkor_202202_use <- kor_202202 %>% \n  filter(substr(행정구역,1,2) %in% c('서울','대전','대구','부산')) %>% \n  filter(substr(행정구역_코드,3,4) != '00') %>%                        \n  select(행정구역, 총인구수) %>%                                        \n  arrange(총인구수)                                                    \n\nkor_202202_use %>% head()\n#>            행정구역 총인구수\n#> 1   부산광역시 중구    40582\n#> 2   대구광역시 중구    74710\n#> 3   부산광역시 동구    88245\n#> 4   부산광역시 서구   104618\n#> 5 부산광역시 영도구   109991\n#> 6   서울특별시 중구   122167\n\n### '시도' 컬럼 생성\nkor_202202_use$시도 <- sapply(kor_202202_use$행정구역,\n                            function(x) strsplit(x, \" \")[[1]][1])\n\nkor_202202_use$시도 %>% head()\n#> [1] \"부산광역시\" \"대구광역시\" \"부산광역시\" \"부산광역시\" \"부산광역시\"\n#> [6] \"서울특별시\"\n\n# factor 변환, level 지정\nkor_202202_use$시도 <- factor(kor_202202_use$시도,\n                            levels = c(\"서울특별시\",\n                                       \"대전광역시\",\n                                       \"대구광역시\",\n                                       \"부산광역시\"))\n\nkor_202202_use$시도 %>% head()\n#> [1] 부산광역시 대구광역시 부산광역시 부산광역시 부산광역시 서울특별시\n#> Levels: 서울특별시 대전광역시 대구광역시 부산광역시\n\nkor_202202_use$시도 %>% summary()\n#> 서울특별시 대전광역시 대구광역시 부산광역시 \n#>         25          5          8         16\n\n### 4개 지방의 색 지정\nregion_colors <- RColorBrewer::brewer.pal(4, \"Set2\")\n\n\n\n2.3 Figure 4.2\n\n사용 데이터셋 : kor_202202_use\nx=reorder(행정구역, 총인구수) : ‘총인구수’ 기준으로 ‘행정구역’ 정렬, y=총인구수\nfill : 시도별\ngeom_col (막대 그래프)\nscale_y_continuous\n\n이름 : ‘총인구수, 2022년 2월’\n범위 : 0 ~ 0 : 더욱 직관적인 그래프 표현 가능\n라벨 : 천 단위 구분 기호 사용\n\nscale_x_discrete\n\n색상 구분 : state_colors별\n\nscale_color_manual\n\n색상 : state_colors\n\nscale_fill_manual\n\nfill 옵션 색상 : region_colors\n\ncoord_flip() : x축, y축 체인지 → 세로방향 긴 막대를 가로방향으로 짧게 표시\n테마 : 밝게\n테마 옵션\n\n그래프 패널의 테두리 제거\ny축 그리드 제거\ny축 레이블 제거\ny축 눈금선 제거\ny축 폰트 크기 설정\n그래프상의 범례 위치 조정\n범례 배경 색상 설정\n\n\n\nggplot(kor_202202_use, aes(x=reorder(행정구역, 총인구수), y=총인구수, fill=시도)) +\n  geom_col() +\n  scale_y_continuous(name = '총인구수, 2022년 2월',\n                     labels = scales::comma,\n                     expand = c(0,0)) +\n  scale_fill_manual(values = region_colors) +\n  coord_flip() +\n  theme_light() +\n  theme(panel.border = element_blank(),\n        panel.grid.major.y = element_blank(),\n        axis.title.y = element_blank(),\n        axis.ticks.length = unit(0, 'pt'),\n        axis.text.y = element_text(size = 5),\n        legend.position = c(.78, .28),\n        legend.background = element_rect(fill = '#ffffffb0'))"
  },
  {
    "objectID": "posts/Data_Visualize_Ch6/figure 4.2.html#figure-4.8",
    "href": "posts/Data_Visualize_Ch6/figure 4.2.html#figure-4.8",
    "title": "Data_Visualization CH6",
    "section": "3 Figure 4.8",
    "text": "3 Figure 4.8\n\n3.1 전처리\n\n### 4개 지방의 색 지정 + 밝기조정 + 채도조정 \nregion_colors <- c('#E69F00','#56B4E9','#009E73','#F0E442') %>% lighten(0.4) %>% desaturate(0.8)\n\nregion_colors\n#> [1] \"#D9CBBE\" \"#C3CDD6\" \"#AABCB3\" \"#F0EDD6\"\n\n### 컬럼 추가 (region_highlight - NA)\npopgrowth_df <- popgrowth_df %>% \n  mutate(region_highlight = ifelse(state %in% c('Texas', 'Louisiana'),\n                                   NA, region %>% paste()))\n\npopgrowth_df %>% head()\n#> # A tibble: 6 × 8\n#>   region    division           state     pop2000 pop2010 popgro…¹   area regio…²\n#>   <fct>     <chr>              <fct>       <int>   <int>    <dbl>  <dbl> <chr>  \n#> 1 Midwest   East North Central Michigan   9.94e6  9.88e6 -0.00551 56539. Midwest\n#> 2 Northeast New England        Rhode Is…  1.05e6  1.05e6  0.00405  1034. Northe…\n#> 3 South     West South Central Louisiana  4.47e6  4.53e6  0.0144  43204. <NA>   \n#> 4 Midwest   East North Central Ohio       1.14e7  1.15e7  0.0162  40861. Midwest\n#> 5 Northeast Middle Atlantic    New York   1.90e7  1.94e7  0.0212  47126. Northe…\n#> 6 South     South Atlantic     West Vir…  1.81e6  1.85e6  0.0247  24038. South  \n#> # … with abbreviated variable names ¹​popgrowth, ²​region_highlight\n\n\n\n3.2 Figure 4.8\n\n사용 데이터셋 : popgrowth_df\nx=state, y=100*popgrowth\nfill : region_highlight별\ngeom_col (막대 그래프)\nscale_y_continuous\n\n이름 : ‘population growth, 2000 to 2010’\n범위 : 0 ~ 0 : 더욱 직관적인 그래프 표현 가능\n라벨 : 백분율 형식\n\nscale_fill_manual\n\nfill 옵션 색상 : region_colors\n그래프상 범례 레이블 지정\nNA 값일 때 색상 지정\n\ncoord_flip() : x축, y축 체인지 → 세로방향 긴 막대를 가로방향으로 짧게 표시\n테마 : 밝게\n테마 옵션\n\n그래프 패널의 테두리 제거\ny축 그리드 제거\ny축 레이블 제거\ny축 눈금선 제거\ny축 폰트 크기 설정\n그래프상의 범례 위치 조정\n범례 배경 색상 설정\n\n\n\nggplot(popgrowth_df, aes(x=state, y=100*popgrowth, fill=region_highlight)) +\n  geom_col() +\n  scale_y_continuous(name = 'population growth, 2000 to 2010',\n                     labels = scales::percent_format(scale=1),\n                     expand = c(0,0)) +\n  scale_fill_manual(values = region_colors,\n                    breaks = c(\"West\", \"South\", \"Midwest\", \"Northeast\"),\n                    na.value = \"#56B4E9\" %>% darken(0.3)) +\n  coord_flip() +\n  theme_light() +\n  theme(panel.border = element_blank(),\n        panel.grid.major.y = element_blank(),\n        axis.title.y = element_blank(),\n        axis.ticks.length = unit(0, 'pt'),\n        axis.text.y = element_text(size = 5),\n        legend.position = c(.58, .58),\n        legend.background = element_rect(fill = '#ffffffb0')) # color =테두리색"
  },
  {
    "objectID": "posts/Data_Visualize_Ch6/figure 4.2.html#예제2",
    "href": "posts/Data_Visualize_Ch6/figure 4.2.html#예제2",
    "title": "Data_Visualization CH6",
    "section": "4 예제2",
    "text": "4 예제2\n\n4.1 대전광역시 강조하기\n\n### 컬러 지정시 대전 따로 지정\nregion_colors <- c('#E69F00','#56B4E9','#009E73','#F0E442') %>% lighten(0.4) %>% desaturate(0.8)\n\nregion_colors[2] <- \"#56B4E9\" %>% darken(0.3)\n\nregion_colors \n#> [1] \"#D9CBBE\" \"#077DAA\" \"#AABCB3\" \"#F0EDD6\"\n\nggplot(kor_202202_use, aes(x=reorder(행정구역, 총인구수), y=총인구수, fill=시도)) +\n  geom_col() +\n  scale_y_continuous(name = '총인구수, 2022년 2월',\n                     labels = scales::comma,\n                     expand = c(0,0)) +\n  scale_fill_manual(values = region_colors) +\n  coord_flip() +\n  theme_light() +\n  theme(panel.border = element_blank(),\n        panel.grid.major.y = element_blank(),\n        axis.title.y = element_blank(),\n        axis.ticks.length = unit(0, 'pt'),\n        axis.text.y = element_text(size = 5),\n        legend.position = c(.78, .28),\n        legend.background = element_rect(fill = '#ffffffb0'))"
  },
  {
    "objectID": "posts/Data_Visualize_Ch6/figure 4.2, Figure 4.8.html",
    "href": "posts/Data_Visualize_Ch6/figure 4.2, Figure 4.8.html",
    "title": "Data_Visualization CH6",
    "section": "",
    "text": "HTML파일로 보기\nFigure 4.2, Figure 4.8"
  },
  {
    "objectID": "posts/Data_Visualize_Ch6/figure 4.2, Figure 4.8.html#데이터-시각화-실습-막대그래프-figure-4.2-figure-4.8",
    "href": "posts/Data_Visualize_Ch6/figure 4.2, Figure 4.8.html#데이터-시각화-실습-막대그래프-figure-4.2-figure-4.8",
    "title": "Data_Visualization CH6",
    "section": "1 데이터 시각화 실습 : 막대그래프 Figure 4.2, Figure 4.8",
    "text": "1 데이터 시각화 실습 : 막대그래프 Figure 4.2, Figure 4.8\n\n1.1 패키지 불러오기\n\nlibrary(ggplot2)\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\nlibrary(colorspace)\n\n\n\n1.2 RColorBrewer의 모든 컬러 맵 보기\n\n# install.packages(\"RColorBrewer\")\nRColorBrewer::display.brewer.all()\n\n\n\n\n\n\n1.3 데이터 불러오기, 파악\n\nUS_cescus.csv (미국 인구조사 데이터셋)\nUS_regions.csv (미국 지역 분류 정보 데이터셋)\n\n\nUS_census <- read.csv('C:/Users/seong taek/Desktop/3-1 DataVisualize/data_visualize/US_census.csv')\nUS_regions <- read.csv('C:/Users/seong taek/Desktop/3-1 DataVisualize/data_visualize/US_regions.csv')\n\n### 차원 파악\nUS_census %>% dim()\n#> [1] 3143   53\nUS_regions %>% dim()\n#> [1] 51  4\n\n### 앞부분 훑어보기\nUS_census %>% head(2)\n#>     state           name FIPS pop2010 pop2000 age_under_5 age_under_18\n#> 1 Alabama Autauga County 1001   54571   43671         6.6         26.8\n#> 2 Alabama Baldwin County 1003  182265  140415         6.1         23.0\n#>   age_over_65 female white black native asian pac_isl two_plus_races hispanic\n#> 1        12.0   51.3  78.5  17.7    0.4   0.9      NA            1.6      2.4\n#> 2        16.8   51.1  85.7   9.4    0.7   0.7      NA            1.5      4.4\n#>   white_not_hispanic no_move_in_one_plus_year foreign_born\n#> 1               77.2                     86.3          2.0\n#> 2               83.5                     83.0          3.6\n#>   foreign_spoken_at_home hs_grad bachelors veterans mean_work_travel\n#> 1                    3.7    85.3      21.7     5817             25.1\n#> 2                    5.5    87.6      26.8    20396             25.8\n#>   housing_units home_ownership housing_multi_unit median_val_owner_occupied\n#> 1         22135           77.5                7.2                    133900\n#> 2        104061           76.7               22.6                    177200\n#>   households persons_per_household per_capita_income median_household_income\n#> 1      19718                   2.7             24568                   53255\n#> 2      69476                   2.5             26469                   50147\n#>   poverty private_nonfarm_establishments private_nonfarm_employment\n#> 1    10.6                            877                      10628\n#> 2    12.2                           4812                      52233\n#>   percent_change_private_nonfarm_employment nonemployment_establishments firms\n#> 1                                      16.6                         2971  4067\n#> 2                                      17.4                        14175 19035\n#>   black_owned_firms native_owned_firms asian_owned_firms pac_isl_owned_firms\n#> 1              15.2                 NA               1.3                  NA\n#> 2               2.7                0.4               1.0                  NA\n#>   hispanic_owned_firms women_owned_firms manufacturer_shipments_2007\n#> 1                  0.7              31.7                          NA\n#> 2                  1.3              27.3                     1410273\n#>   mercent_whole_sales_2007   sales sales_per_capita accommodation_food_service\n#> 1                       NA  598175            12003                      88157\n#> 2                       NA 2966489            17166                     436955\n#>   building_permits fed_spending    area density\n#> 1              191       331142  594.44    91.8\n#> 2              696      1119082 1589.78   114.6\nUS_regions %>% head()\n#>        state state_abr region           division\n#> 1    Alabama        AL  South East South Central\n#> 2     Alaska        AK   West            Pacific\n#> 3    Arizona        AZ   West           Mountain\n#> 4   Arkansas        AR  South West South Central\n#> 5 California        CA   West            Pacific\n#> 6   Colorado        CO   West           Mountain\n\n\n\n1.4 전처리\n\npopgrowth_df <- US_census %>%                         \n  left_join(US_regions) %>%                         # US_census 기준으로 left_join\n  group_by(region, division, state) %>%             # 그룹화할 컬럼 선택  \n  summarise(pop2000 = sum(pop2000, na.rm = T),      # 그룹화된 컬럼들의 집계값 설정\n            pop2010 = sum(pop2010, na.rm = T),\n            popgrowth = (pop2010 - pop2000)/pop2000,\n            area = sum(area)) %>%\n  arrange(popgrowth) %>%                            # 해당 컬럼 기준으로 오름차순 정렬\n  ungroup() %>%                                     # 그룹화 해제\n  mutate(state = factor(state, levels=state),       # 팩터로 변환, level 지정 \n         region = factor(region, levels=c('West','South','Midwest','Northeast')))\n#> Joining with `by = join_by(state)`\n#> `summarise()` has grouped output by 'region', 'division'. You can override\n#> using the `.groups` argument.\n\npopgrowth_df\n#> # A tibble: 51 × 7\n#>    region    division           state          pop2000  pop2010 popgrowth   area\n#>    <fct>     <chr>              <fct>            <int>    <int>     <dbl>  <dbl>\n#>  1 Midwest   East North Central Michigan       9938444  9883640  -0.00551 56539.\n#>  2 Northeast New England        Rhode Island   1048319  1052567   0.00405  1034.\n#>  3 South     West South Central Louisiana      4468976  4533372   0.0144  43204.\n#>  4 Midwest   East North Central Ohio          11353140 11536504   0.0162  40861.\n#>  5 Northeast Middle Atlantic    New York      18976457 19378102   0.0212  47126.\n#>  6 South     South Atlantic     West Virginia  1808344  1852994   0.0247  24038.\n#>  7 Northeast New England        Vermont         608827   625741   0.0278   9217.\n#>  8 Northeast New England        Massachusetts  6349097  6547629   0.0313   7800.\n#>  9 Midwest   East North Central Illinois      12419293 12830632   0.0331  55519.\n#> 10 Northeast Middle Atlantic    Pennsylvania  12281054 12702379   0.0343  44743.\n#> # … with 41 more rows\n\n### 4개 지방의 색 지정\nregion_colors <- c('#E69F00','#56B4E9','#009E73','#F0E442')\nregion_colors\n#> [1] \"#E69F00\" \"#56B4E9\" \"#009E73\" \"#F0E442\"\n\n\n\n1.5 Figure 4.2\n\n사용 데이터셋 : popgrowth_df\nx=state, y=100*popgrowth\nfill : region별\ngeom_col (막대 그래프)\nscale_y_continuous\n\n이름 : ‘population growth, 2000 to 2010’\n범위 : 0 ~ 0 : 더욱 직관적인 그래프 표현 가능\n라벨 : 백분율 형식\n\nscale_fill_manual\n\nfill 옵션 색상 : region_colors\n\ncoord_flip() : x축, y축 체인지 → 세로방향 긴 막대를 가로방향으로 짧게 표시\n테마 : 밝게\n테마 옵션\n\n그래프 패널의 테두리 제거\ny축 그리드 제거\ny축 레이블 제거\ny축 눈금선 제거\ny축 폰트 크기 설정\n그래프상의 범례 위치 조정\n범례 배경 색상 설정\n\n\n\nggplot(popgrowth_df, aes(x=state, y=100*popgrowth, fill=region)) +\n  geom_col() +\n  scale_y_continuous(name = 'population growth, 2000 to 2010',\n                     labels = scales::percent_format(scale=1),\n                     expand = c(0,0)) +\n  scale_fill_manual(values = region_colors) +\n  coord_flip() +\n  theme_light() +\n  theme(panel.border = element_blank(),\n        panel.grid.major.y = element_blank(),\n        axis.title.y = element_blank(),\n        axis.ticks.length = unit(0, 'pt'),\n        axis.text.y = element_text(size = 6),\n        legend.position = c(.7, .68),\n        legend.background = element_rect(fill = '#ffffffb0'))"
  },
  {
    "objectID": "posts/Data_Visualize_Ch6/figure 4.2, Figure 4.8.html#예제",
    "href": "posts/Data_Visualize_Ch6/figure 4.2, Figure 4.8.html#예제",
    "title": "Data_Visualization CH6",
    "section": "2 예제",
    "text": "2 예제\n\n주민등록 인구 및 세대현황 Figure 4.2\n\n\n2.1 데이터 불러오기, 파악\n\n202202_주민등록인구및세대현황.csv\n\n\nkor_202202 <- read.csv('C:/Users/seong taek/Desktop/3-1 DataVisualize/data_visualize/202202_주민등록인구및세대현황.csv')\n\n### 앞부분 훑어보기\nkor_202202 %>%  head()\n#>            행정구역 행정구역_코드 총인구수  세대수 세대당_인구 남자_인구수\n#> 1       서울특별시     1100000000  9508451 4442586        2.14     4615823\n#> 2 서울특별시 종로구    1111000000   144575   73763        1.96       70092\n#> 3   서울특별시 중구    1114000000   122167   63644        1.92       59446\n#> 4 서울특별시 용산구    1117000000   222413  111134        2.00      106881\n#> 5 서울특별시 성동구    1120000000   285137  134286        2.12      138866\n#> 6 서울특별시 광진구    1121500000   340494  168975        2.02      164226\n#>   여자_인구수 남여_비율\n#> 1     4892628      0.94\n#> 2       74483      0.94\n#> 3       62721      0.95\n#> 4      115532      0.93\n#> 5      146271      0.95\n#> 6      176268      0.93\n\n### 차원 파악\nkor_202202 %>% dim()\n#> [1] 291   8\n\n### 컬럼 클래스(타입) 확인\nkor_202202 %>% sapply(class)\n#>      행정구역 행정구역_코드      총인구수        세대수   세대당_인구 \n#>   \"character\"     \"numeric\"     \"numeric\"     \"numeric\"     \"numeric\" \n#>   남자_인구수   여자_인구수     남여_비율 \n#>     \"numeric\"     \"numeric\"     \"numeric\"\n\n### `행정구역_코드`를 numeric → character형식으로 변환\nkor_202202$행정구역_코드 <- kor_202202$행정구역_코드 %>% format()\n\n\n\n2.2 전처리\n\n# 행정구역 컬럼의 1,2번째 자리의 값을 지정해서 추출\n# 행정구역_코드 컬럼의 3,4번째 자리의 값이 0이 아닌것만 추출\n# 해당 열만 선택\n# 해당 열 기준으로 오름차순 정렬\nkor_202202_use <- kor_202202 %>% \n  filter(substr(행정구역,1,2) %in% c('서울','대전','대구','부산')) %>% \n  filter(substr(행정구역_코드,3,4) != '00') %>%                        \n  select(행정구역, 총인구수) %>%                                        \n  arrange(총인구수)                                                    \n\nkor_202202_use %>% head()\n#>            행정구역 총인구수\n#> 1   부산광역시 중구    40582\n#> 2   대구광역시 중구    74710\n#> 3   부산광역시 동구    88245\n#> 4   부산광역시 서구   104618\n#> 5 부산광역시 영도구   109991\n#> 6   서울특별시 중구   122167\n\n### '시도' 컬럼 생성\nkor_202202_use$시도 <- sapply(kor_202202_use$행정구역,\n                            function(x) strsplit(x, \" \")[[1]][1])\n\nkor_202202_use %>% head()\n#>            행정구역 총인구수       시도\n#> 1   부산광역시 중구    40582 부산광역시\n#> 2   대구광역시 중구    74710 대구광역시\n#> 3   부산광역시 동구    88245 부산광역시\n#> 4   부산광역시 서구   104618 부산광역시\n#> 5 부산광역시 영도구   109991 부산광역시\n#> 6   서울특별시 중구   122167 서울특별시\n\nkor_202202_use %>% sapply(class)\n#>    행정구역    총인구수        시도 \n#> \"character\"   \"numeric\" \"character\"\n\n# factor 변환, level 지정\nkor_202202_use$시도 <- factor(kor_202202_use$시도,\n                            levels = c(\"서울특별시\",\n                                       \"대전광역시\",\n                                       \"대구광역시\",\n                                       \"부산광역시\"))\n\nkor_202202_use$시도 %>% head()\n#> [1] 부산광역시 대구광역시 부산광역시 부산광역시 부산광역시 서울특별시\n#> Levels: 서울특별시 대전광역시 대구광역시 부산광역시\n\nkor_202202_use$시도 %>% summary()\n#> 서울특별시 대전광역시 대구광역시 부산광역시 \n#>         25          5          8         16\n\n### 4개 지방의 색 지정\nregion_colors <- RColorBrewer::brewer.pal(4, \"Set2\")\n\n\n\n2.3 Figure 4.2\n\n사용 데이터셋 : kor_202202_use\nx=reorder(행정구역, 총인구수) : ‘총인구수’ 기준으로 ‘행정구역’ 정렬, y=총인구수\nfill : 시도별\ngeom_col (막대 그래프)\nscale_y_continuous\n\n이름 : ‘총인구수, 2022년 2월’\n범위 : 0 ~ 0 : 더욱 직관적인 그래프 표현 가능\n라벨 : 천 단위 구분 기호 사용\n\nscale_fill_manual\n\nfill 옵션 색상 : region_colors\n\ncoord_flip() : x축, y축 체인지 → 세로방향 긴 막대를 가로방향으로 짧게 표시\n테마 : 밝게\n테마 옵션\n\n그래프 패널의 테두리 제거\ny축 그리드 제거\ny축 레이블 제거\ny축 눈금선 제거\ny축 폰트 크기 설정\n그래프상의 범례 위치 조정\n범례 배경 색상 설정\n\n\n\nggplot(kor_202202_use, aes(x=reorder(행정구역, 총인구수), y=총인구수, fill=시도)) +\n  geom_col() +\n  scale_y_continuous(name = '총인구수, 2022년 2월',\n                     labels = scales::comma,\n                     expand = c(0,0)) +\n  scale_fill_manual(values = region_colors) +\n  coord_flip() +\n  theme_light() +\n  theme(panel.border = element_blank(),\n        panel.grid.major.y = element_blank(),\n        axis.title.y = element_blank(),\n        axis.ticks.length = unit(0, 'pt'),\n        axis.text.y = element_text(size = 5),\n        legend.position = c(.78, .28),\n        legend.background = element_rect(fill = '#ffffffb0'))"
  },
  {
    "objectID": "posts/Data_Visualize_Ch6/figure 4.2, Figure 4.8.html#figure-4.8",
    "href": "posts/Data_Visualize_Ch6/figure 4.2, Figure 4.8.html#figure-4.8",
    "title": "Data_Visualization CH6",
    "section": "3 Figure 4.8",
    "text": "3 Figure 4.8\n\n3.1 전처리\n\n### 4개 지방의 색 지정 + 밝기조정 + 채도조정 \nregion_colors <- c('#E69F00','#56B4E9','#009E73','#F0E442') %>% lighten(0.4) %>% desaturate(0.8)\n\nregion_colors\n#> [1] \"#D9CBBE\" \"#C3CDD6\" \"#AABCB3\" \"#F0EDD6\"\n\n### 컬럼 추가 (region_highlight - NA)\npopgrowth_df <- popgrowth_df %>% \n  mutate(region_highlight = ifelse(state %in% c('Texas', 'Louisiana'),\n                                   NA, region %>% paste()))\n\npopgrowth_df %>% head()\n#> # A tibble: 6 × 8\n#>   region    division           state     pop2000 pop2010 popgro…¹   area regio…²\n#>   <fct>     <chr>              <fct>       <int>   <int>    <dbl>  <dbl> <chr>  \n#> 1 Midwest   East North Central Michigan   9.94e6  9.88e6 -0.00551 56539. Midwest\n#> 2 Northeast New England        Rhode Is…  1.05e6  1.05e6  0.00405  1034. Northe…\n#> 3 South     West South Central Louisiana  4.47e6  4.53e6  0.0144  43204. <NA>   \n#> 4 Midwest   East North Central Ohio       1.14e7  1.15e7  0.0162  40861. Midwest\n#> 5 Northeast Middle Atlantic    New York   1.90e7  1.94e7  0.0212  47126. Northe…\n#> 6 South     South Atlantic     West Vir…  1.81e6  1.85e6  0.0247  24038. South  \n#> # … with abbreviated variable names ¹​popgrowth, ²​region_highlight\n\n\n\n3.2 Figure 4.8\n\n사용 데이터셋 : popgrowth_df\nx=state, y=100*popgrowth\nfill : region_highlight별\ngeom_col (막대 그래프)\nscale_y_continuous\n\n이름 : ‘population growth, 2000 to 2010’\n범위 : 0 ~ 0 : 더욱 직관적인 그래프 표현 가능\n라벨 : 백분율 형식\n\nscale_fill_manual\n\nfill 옵션 색상 : region_colors\n그래프상 범례 레이블 지정\nNA 값일 때 색상 지정\n\ncoord_flip() : x축, y축 체인지 → 세로방향 긴 막대를 가로방향으로 짧게 표시\n테마 : 밝게\n테마 옵션\n\n그래프 패널의 테두리 제거\ny축 그리드 제거\ny축 레이블 제거\ny축 눈금선 제거\ny축 폰트 크기 설정\n그래프상의 범례 위치 조정\n범례 배경 색상 설정\n\n\n\nggplot(popgrowth_df, aes(x=state, y=100*popgrowth, fill=region_highlight)) +\n  geom_col() +\n  scale_y_continuous(name = 'population growth, 2000 to 2010',\n                     labels = scales::percent_format(scale=1),\n                     expand = c(0,0)) +\n  scale_fill_manual(values = region_colors,\n                    breaks = c(\"West\", \"South\", \"Midwest\", \"Northeast\"),\n                    na.value = \"#56B4E9\" %>% darken(0.3)) +\n  coord_flip() +\n  theme_light() +\n  theme(panel.border = element_blank(),\n        panel.grid.major.y = element_blank(),\n        axis.title.y = element_blank(),\n        axis.ticks.length = unit(0, 'pt'),\n        axis.text.y = element_text(size = 5),\n        legend.position = c(.58, .58),\n        legend.background = element_rect(fill = '#ffffffb0')) # color =테두리색"
  },
  {
    "objectID": "posts/Data_Visualize_Ch6/figure 4.2, Figure 4.8.html#예제2",
    "href": "posts/Data_Visualize_Ch6/figure 4.2, Figure 4.8.html#예제2",
    "title": "Data_Visualization CH6",
    "section": "4 예제2",
    "text": "4 예제2\n-대전광역시 강조하기 Figure 4.8\n\n### 컬러 지정시 대전 따로 지정\nregion_colors <- c('#E69F00','#56B4E9','#009E73','#F0E442') %>% lighten(0.4) %>% desaturate(0.8)\n\nregion_colors[2] <- \"#56B4E9\" %>% darken(0.3)\n\nregion_colors \n#> [1] \"#D9CBBE\" \"#077DAA\" \"#AABCB3\" \"#F0EDD6\"\n\nggplot(kor_202202_use, aes(x=reorder(행정구역, 총인구수), y=총인구수, fill=시도)) +\n  geom_col() +\n  scale_y_continuous(name = '총인구수, 2022년 2월',\n                     labels = scales::comma,\n                     expand = c(0,0)) +\n  scale_fill_manual(values = region_colors) +\n  coord_flip() +\n  theme_light() +\n  theme(panel.border = element_blank(),\n        panel.grid.major.y = element_blank(),\n        axis.title.y = element_blank(),\n        axis.ticks.length = unit(0, 'pt'),\n        axis.text.y = element_text(size = 5),\n        legend.position = c(.78, .28),\n        legend.background = element_rect(fill = '#ffffffb0'))"
  },
  {
    "objectID": "posts/Data_Visualize_Ch7/figure 4.4 ,  figure 4.6.html",
    "href": "posts/Data_Visualize_Ch7/figure 4.4 ,  figure 4.6.html",
    "title": "Data_Visualization CH7",
    "section": "",
    "text": "HTML파일로 보기\nFigure 4.4, Figure 4.6"
  },
  {
    "objectID": "posts/Data_Visualize_Ch7/figure 4.4 ,  figure 4.6.html#데이터-시각화-실습-지역-정보-시각화-figure-4.4-4.6",
    "href": "posts/Data_Visualize_Ch7/figure 4.4 ,  figure 4.6.html#데이터-시각화-실습-지역-정보-시각화-figure-4.4-4.6",
    "title": "Data_Visualization CH7",
    "section": "1 데이터 시각화 실습 : 지역 정보 시각화 Figure 4.4, 4.6",
    "text": "1 데이터 시각화 실습 : 지역 정보 시각화 Figure 4.4, 4.6\n\n1.1 패키지 불러오기\n\n#install.packages(\"geojsonsf\")\nlibrary(geojsonsf)\nlibrary(sf)\n#> Linking to GEOS 3.9.3, GDAL 3.5.2, PROJ 8.2.1; sf_use_s2() is TRUE\nlibrary(ggplot2)\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\nlibrary(colorspace)\n\n\n\n1.2 데이터 불러오기\n\nKOR_SIDO.json : 대한민국 시,도 정보\nKOR_SIGU.json : 대한민국 시,군,구 정보\n202202_주민등록인구및세대현황.csv\n\n\nKOR_SIDO <- geojson_sf('C:/Users/seong taek/Desktop/3-1 DataVisualize/data_visualize/KOR_SIDO.json')\n\nKOR_SIGU <- geojson_sf('C:/Users/seong taek/Desktop/3-1 DataVisualize/data_visualize/KOR_SIGU.json')\n\nkor_202202 <- read.csv('C:/Users/seong taek/Desktop/3-1 DataVisualize/data_visualize/202202_주민등록인구및세대현황.csv')\n\n\n\n1.3 전처리\n\n### 컬럼 클래스(타입) 확인\nkor_202202 %>% sapply(class)\n#>      행정구역 행정구역_코드      총인구수        세대수   세대당_인구 \n#>   \"character\"     \"numeric\"     \"numeric\"     \"numeric\"     \"numeric\" \n#>   남자_인구수   여자_인구수     남여_비율 \n#>     \"numeric\"     \"numeric\"     \"numeric\"\n\n### `행정구역_코드`를 numeric → character형식으로 변환\nkor_202202$행정구역_코드 <- kor_202202$행정구역_코드 %>% format()\n\nuse_map <- KOR_SIGU\nuse_map %>% head()\n#> Simple feature collection with 6 features and 3 fields\n#> Geometry type: POLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: 127.5071 ymin: 37.06363 xmax: 129.1596 ymax: 38.23041\n#> Geodetic CRS:  WGS 84\n#>   SIG_CD   SIG_ENG_NM SIG_KOR_NM                       geometry\n#> 1  42110 Chuncheon-si     춘천시 POLYGON ((127.6047 38.07265...\n#> 2  42130     Wonju-si     원주시 POLYGON ((128.1086 37.2988,...\n#> 3  42150 Gangneung-si     강릉시 POLYGON ((129.0479 37.64961...\n#> 4  42170   Donghae-si     동해시 POLYGON ((129.1596 37.47574...\n#> 5  42190   Taebaek-si     태백시 POLYGON ((129.0121 37.31012...\n#> 6  42210    Sokcho-si     속초시 POLYGON ((128.6087 38.16128...\n\n### 병합할 컬럼 동일화\nuse_map$행정구역_코드 <- paste(use_map$SIG_CD, \"00000\", sep = \"\")\n\n### 공통된 '행정구역_코드'를 기준으로 병합 \nuse_map <- use_map %>% merge(kor_202202, by = \"행정구역_코드\", all.x=T)\n\n\n\n1.4 Figure 4.4 - 총 인구수\n\n사용 데이터셋 : use_map\nfill : 총인구수\ngeom_sf (지리적 객체 그래프)\n\n경계선 색상\n\ncoord_sf\n\n지도 좌표계 제거\n\nscale_fill_distiller\n\n이름 : “인구수”\n팔레트 색상 설정\n연속형 색상 척도 사용\n결측값 색상 설정\n색상 척도 방향 설정 : 작은 값 → 큰 값\n색상 척도 눈금 설정\n색상 척도 라벨 설정, 천 단위 옵션, 지수표기법 미사용\n\n테마 : 미니멀\n테마 옵션\n\n범례 제목 위치 지정\n범례 텍스트 위치 지정\n범례 위치 지정\n\n\n\nggplot(use_map, aes(fill = 총인구수)) +\ngeom_sf(color = \"gray90\") +\ncoord_sf(datum = NA) +\nscale_fill_distiller(name = \"인구수\",\n                      palette = \"Blues\",\n                      type = \"seq\",\n                      na.value = \"grey60\",\n                      direction = 1,\n                      breaks = seq(0,10,2) * 1e+5,\n                      labels = format(seq(0,10,2) * 1e+5,\n                                      big.mark = \",\", scientific = F),) +\ntheme_minimal() +\ntheme(legend.title.align = 0.5,\n      legend.text.align = 1.0,\n      legend.position = c(.85, .2))\n\n\n\n\n\n\n1.5 광주만 뽑기\n\n### filter 이용 광주지역만 추출\nuse_map %>%\n  filter(substr(행정구역_코드,1,2) == \"29\") %>% \n  ggplot(aes(fill = 총인구수)) +\n  geom_sf(color = \"gray90\") +\n  coord_sf(datum = NA) +\n  scale_fill_distiller(name = \"인구수\",\n                       palette = \"Blues\",\n                       type = \"seq\",\n                       na.value = \"grey60\",\n                       direction = 1,\n                       breaks = seq(0,10,2) * 1e+5,\n                       labels = format(seq(0,10,2) * 1e+5,\n                                       big.mark = \",\", scientific = F),) +\n  theme_minimal() +\n  theme(legend.title.align = 0.5,\n        legend.text.align = 1.0,\n        legend.position = c(1, .2))\n\n\n\n\n\n\n1.6 Figure 4.6 - 남여 비율\n\n사용 데이터셋 : use_map\nfill : 남여_비율\ngeom_sf (지리적 객체 그래프)\ncoord_sf\n\n지도 좌표계 제거\n\nscale_fill_continuous_diverging\n\n이름 : “남자/여자”\n팔레트 색상 설정\n색상 척도 : 중앙값 기준\n색상 척도 범위 설정\n색상 척도 순서 반전\n\n테마 : 미니멀\n테마 옵션\n\n범례 제목 위치 지정\n범례 텍스트 위치 지정\n범례 위치 지정\n\n\n\nggplot(use_map, aes(fill = 남여_비율)) +\ngeom_sf() +\ncoord_sf(datum = NA) +\nscale_fill_continuous_diverging(name = \"남자/여자\",\n                                palette = \"Blue-Red\",\n                                mid = 1,\n                                limits = 1 + c(-1, 1)*0.35,\n                                rev = T) +\ntheme_minimal() +\ntheme(legend.title.align = 0.5,\n      legend.text.align = 1.0,\n      legend.position = c(.95, .3))"
  },
  {
    "objectID": "posts/Data_Visualize_Ch7/figure 4.4 ,  figure 4.6.html#과제",
    "href": "posts/Data_Visualize_Ch7/figure 4.4 ,  figure 4.6.html#과제",
    "title": "Data_Visualization CH7",
    "section": "2 과제",
    "text": "2 과제\n\n2023년3월 총인구수 - Figure 4.4\n\n\n2.1 데이터 불러오기\n\n202303_202303_주민등록인구및세대현황_월간.csv\nKOR_SIGU.json : 대한민국 시,군,구 정보\n\n\nkor_202303 <- read.csv('C:/Users/seong taek/Desktop/3-1 DataVisualize/data_visualize/202303_202303_주민등록인구및세대현황_월간.csv',fileEncoding = \"CP949\")\nKOR_SIGU <- geojson_sf('C:/Users/seong taek/Desktop/3-1 DataVisualize/data_visualize/KOR_SIGU.json')\n\nkor_202303 %>% head()\n#>                         행정구역 X2023년03월_총인구수 X2023년03월_세대수\n#> 1       서울특별시  (1100000000)            9,426,404          4,463,385\n#> 2 서울특별시 종로구 (1111000000)              141,060             72,679\n#> 3   서울특별시 중구 (1114000000)              120,963             63,862\n#> 4 서울특별시 용산구 (1117000000)              217,756            109,735\n#> 5 서울특별시 성동구 (1120000000)              280,240            133,513\n#> 6 서울특별시 광진구 (1121500000)              336,801            169,787\n#>   X2023년03월_세대당.인구 X2023년03월_남자.인구수 X2023년03월_여자.인구수\n#> 1                    2.11               4,566,299               4,860,105\n#> 2                    1.94                  68,170                  72,890\n#> 3                    1.89                  58,699                  62,264\n#> 4                    1.98                 104,640                 113,116\n#> 5                    2.10                 136,233                 144,007\n#> 6                    1.98                 162,209                 174,592\n#>   X2023년03월_남여.비율\n#> 1                  0.94\n#> 2                  0.94\n#> 3                  0.94\n#> 4                  0.93\n#> 5                  0.95\n#> 6                  0.93\n\nKOR_SIGU %>% head()\n#> Simple feature collection with 6 features and 3 fields\n#> Geometry type: POLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: 127.5071 ymin: 37.06363 xmax: 129.1596 ymax: 38.23041\n#> Geodetic CRS:  WGS 84\n#>   SIG_CD   SIG_ENG_NM SIG_KOR_NM                       geometry\n#> 1  42110 Chuncheon-si     춘천시 POLYGON ((127.6047 38.07265...\n#> 2  42130     Wonju-si     원주시 POLYGON ((128.1086 37.2988,...\n#> 3  42150 Gangneung-si     강릉시 POLYGON ((129.0479 37.64961...\n#> 4  42170   Donghae-si     동해시 POLYGON ((129.1596 37.47574...\n#> 5  42190   Taebaek-si     태백시 POLYGON ((129.0121 37.31012...\n#> 6  42210    Sokcho-si     속초시 POLYGON ((128.6087 38.16128...\n\n\n\n2.2 전처리\n\n#install.packages(\"stringr\")\nlibrary(stringr) # 고급 문자 추출 패키지\n#> Warning: package 'stringr' was built under R version 4.2.2\n\n### 쉼표없는 총인구수 컬럼 생성\n### 행정구역 코드만 추출한 컬럼 생성\nkor_202303 <- kor_202303 %>% \n  mutate(총인구수_202303 = gsub(\",\",\"\", X2023년03월_총인구수),\n         행정구역_코드 = str_sub(kor_202303$행정구역, -11, -2))\n\n### 컬럼 클래스(타입) 확인\nkor_202303 %>% sapply(class)\n#>                행정구역    X2023년03월_총인구수      X2023년03월_세대수 \n#>             \"character\"             \"character\"             \"character\" \n#> X2023년03월_세대당.인구 X2023년03월_남자.인구수 X2023년03월_여자.인구수 \n#>               \"numeric\"             \"character\"             \"character\" \n#>   X2023년03월_남여.비율         총인구수_202303           행정구역_코드 \n#>               \"numeric\"             \"character\"             \"character\"\n\n### '총인구수_202303' 컬럼 character → numeric 변환\nkor_202303$총인구수_202303 <- kor_202303$총인구수_202303 %>% as.numeric()\n\n### 병합할 컬럼 동일화\nKOR_SIGU_use <- KOR_SIGU\nKOR_SIGU_use$행정구역_코드 <- paste(KOR_SIGU_use$SIG_CD, \"00000\", sep = \"\")\n\nkor_202303 %>% head()\n#>                         행정구역 X2023년03월_총인구수 X2023년03월_세대수\n#> 1       서울특별시  (1100000000)            9,426,404          4,463,385\n#> 2 서울특별시 종로구 (1111000000)              141,060             72,679\n#> 3   서울특별시 중구 (1114000000)              120,963             63,862\n#> 4 서울특별시 용산구 (1117000000)              217,756            109,735\n#> 5 서울특별시 성동구 (1120000000)              280,240            133,513\n#> 6 서울특별시 광진구 (1121500000)              336,801            169,787\n#>   X2023년03월_세대당.인구 X2023년03월_남자.인구수 X2023년03월_여자.인구수\n#> 1                    2.11               4,566,299               4,860,105\n#> 2                    1.94                  68,170                  72,890\n#> 3                    1.89                  58,699                  62,264\n#> 4                    1.98                 104,640                 113,116\n#> 5                    2.10                 136,233                 144,007\n#> 6                    1.98                 162,209                 174,592\n#>   X2023년03월_남여.비율 총인구수_202303 행정구역_코드\n#> 1                  0.94         9426404    1100000000\n#> 2                  0.94          141060    1111000000\n#> 3                  0.94          120963    1114000000\n#> 4                  0.93          217756    1117000000\n#> 5                  0.95          280240    1120000000\n#> 6                  0.93          336801    1121500000\nKOR_SIGU_use %>% head()\n#> Simple feature collection with 6 features and 4 fields\n#> Geometry type: POLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: 127.5071 ymin: 37.06363 xmax: 129.1596 ymax: 38.23041\n#> Geodetic CRS:  WGS 84\n#>   SIG_CD   SIG_ENG_NM SIG_KOR_NM                       geometry 행정구역_코드\n#> 1  42110 Chuncheon-si     춘천시 POLYGON ((127.6047 38.07265...    4211000000\n#> 2  42130     Wonju-si     원주시 POLYGON ((128.1086 37.2988,...    4213000000\n#> 3  42150 Gangneung-si     강릉시 POLYGON ((129.0479 37.64961...    4215000000\n#> 4  42170   Donghae-si     동해시 POLYGON ((129.1596 37.47574...    4217000000\n#> 5  42190   Taebaek-si     태백시 POLYGON ((129.0121 37.31012...    4219000000\n#> 6  42210    Sokcho-si     속초시 POLYGON ((128.6087 38.16128...    4221000000\n\n### 공통된 '행정구역_코드'를 기준으로 병합 \nKOR_SIGU_use <- KOR_SIGU_use %>% merge(kor_202303, by = \"행정구역_코드\", all.x=T)\nKOR_SIGU_use %>% head()\n#> Simple feature collection with 6 features and 12 fields\n#> Geometry type: POLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: 126.9446 ymin: 37.50654 xmax: 127.1153 ymax: 37.6317\n#> Geodetic CRS:  WGS 84\n#>   행정구역_코드 SIG_CD    SIG_ENG_NM SIG_KOR_NM\n#> 1    1111000000  11110     Jongno-gu     종로구\n#> 2    1114000000  11140       Jung-gu       중구\n#> 3    1117000000  11170    Yongsan-gu     용산구\n#> 4    1120000000  11200  Seongdong-gu     성동구\n#> 5    1121500000  11215   Gwangjin-gu     광진구\n#> 6    1123000000  11230 Dongdaemun-gu   동대문구\n#>                           행정구역 X2023년03월_총인구수 X2023년03월_세대수\n#> 1   서울특별시 종로구 (1111000000)              141,060             72,679\n#> 2     서울특별시 중구 (1114000000)              120,963             63,862\n#> 3   서울특별시 용산구 (1117000000)              217,756            109,735\n#> 4   서울특별시 성동구 (1120000000)              280,240            133,513\n#> 5   서울특별시 광진구 (1121500000)              336,801            169,787\n#> 6 서울특별시 동대문구 (1123000000)              337,574            171,140\n#>   X2023년03월_세대당.인구 X2023년03월_남자.인구수 X2023년03월_여자.인구수\n#> 1                    1.94                  68,170                  72,890\n#> 2                    1.89                  58,699                  62,264\n#> 3                    1.98                 104,640                 113,116\n#> 4                    2.10                 136,233                 144,007\n#> 5                    1.98                 162,209                 174,592\n#> 6                    1.97                 165,933                 171,641\n#>   X2023년03월_남여.비율 총인구수_202303                       geometry\n#> 1                  0.94          141060 POLYGON ((127.0118 37.58157...\n#> 2                  0.94          120963 POLYGON ((127.0234 37.57191...\n#> 3                  0.93          217756 POLYGON ((127.009 37.54413,...\n#> 4                  0.95          280240 POLYGON ((127.0724 37.55996...\n#> 5                  0.93          336801 POLYGON ((127.1153 37.55676...\n#> 6                  0.97          337574 POLYGON ((127.0711 37.60732...\n\n\n\n2.3 Figure 4.4 - 2023년3월 총인구수\n\n사용 데이터셋 : KOR_SIGU_use\nfill : 총인구수_202303\ngeom_sf (지리적 객체 그래프)\n\n경계선 색상\n\ncoord_sf\n\n지도 좌표계 제거\n\nscale_fill_distiller\n\n이름 : “2023년 3월 총인구수”\n팔레트 색상 설정\n연속형 색상 척도 사용\n결측값 색상 설정\n색상 척도 방향 설정 : 작은 값 → 큰 값\n색상 척도 눈금 설정\n색상 척도 라벨 설정, 천 단위 옵션, 지수표기법 미사용\n\n테마 : 미니멀\n테마 옵션\n\n범례 제목 위치 지정\n범례 텍스트 위치 지정\n범례 위치 지정\n\n\n\nggplot(KOR_SIGU_use, aes(fill = 총인구수_202303)) +\ngeom_sf(color = \"gray90\") +\ncoord_sf(datum = NA) +\nscale_fill_distiller(name = \"2023년 3월 총인구수\",\n                      palette = \"Blues\",\n                      type = \"seq\",\n                      na.value = \"grey60\",\n                      direction = 1,\n                      breaks = seq(0,10,2) * 1e+5,\n                      labels = format(seq(0,10,2) * 1e+5,\n                                      big.mark = \",\", scientific = F),) +\ntheme_minimal() +\ntheme(legend.title.align = 0.5,\n      legend.text.align = 1.0,\n      legend.position = c(.85, .2))"
  },
  {
    "objectID": "posts/Opendata_Analysis Ch1/tidyverse.html",
    "href": "posts/Opendata_Analysis Ch1/tidyverse.html",
    "title": "Opendata_Analysis CH1",
    "section": "",
    "text": "HTML파일로 보기\nTidyvers Package"
  },
  {
    "objectID": "posts/Opendata_Analysis Ch1/tidyverse.html#오픈데이터-분석-실습-tidyverse-패키지",
    "href": "posts/Opendata_Analysis Ch1/tidyverse.html#오픈데이터-분석-실습-tidyverse-패키지",
    "title": "Opendata_Analysis CH1",
    "section": "1 오픈데이터 분석 실습 : Tidyverse 패키지",
    "text": "1 오픈데이터 분석 실습 : Tidyverse 패키지\n\n1.1 설명\n\n6개의 핵심 패키지 포함 23개의 패키지로 이뤄진 메타 패키지\n\nggplot2\ndplyr\ntidyr\nreadr\npurrr\ntibble\nstringr\nforcats\n\n\n\n\n1.2 패키지 불러오기\n\n#install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.1.8\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors"
  },
  {
    "objectID": "posts/Opendata_Analysis Ch1/tidyverse.html#tidyverse-실습",
    "href": "posts/Opendata_Analysis Ch1/tidyverse.html#tidyverse-실습",
    "title": "Opendata_Analysis CH1",
    "section": "2 tidyverse 실습",
    "text": "2 tidyverse 실습\n\n항공편 데이터 nycflights13\n\n\n#install.packages(\"nycflights13\")\nlibrary(nycflights13)\n\n\n2.1 파이프 연산자를 통한 코드 직관화 예시\n\n순차적, 간결함\n\n\nrnorm(10000, mean=10, sd=1) %>%        # 평균10, 표준편차1인 정규분포에서 10000개 난수 생성\n  sample(size = 100, replace = F) %>%  # 샘플 100개를 랜덤으로 비복원 추출\n  log() %>%                            # 로그 함수 적용\n  diff() %>%                           # 차분 계산 \n  plot(col=\"red\", type=\"l\")            # 그래프 그리기\n\n\n\n\n\n\n2.2 행의 수\n\nflights %>% nrow()\n\n[1] 336776\n\n\n\n\n2.3 앞부분 훑어보기\n\nflights %>% head()\n\n# A tibble: 6 × 19\n   year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n  <int> <int> <int>    <int>       <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n1  2013     1     1      517         515       2     830     819      11 UA     \n2  2013     1     1      533         529       4     850     830      20 UA     \n3  2013     1     1      542         540       2     923     850      33 AA     \n4  2013     1     1      544         545      -1    1004    1022     -18 B6     \n5  2013     1     1      554         600      -6     812     837     -25 DL     \n6  2013     1     1      554         558      -4     740     728      12 UA     \n# … with 9 more variables: flight <int>, tailnum <chr>, origin <chr>,\n#   dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>, and abbreviated variable names ¹​sched_dep_time,\n#   ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n\n\n\n\n2.4 그룹화 + 집계값 요약\n\n### 날짜별 평균 출발지연 시간\nmean_delay_by_day <- flights %>% \n  group_by(year, month, day) %>% \n  summarise(delay = mean(dep_delay, na.rm = T))\n\n`summarise()` has grouped output by 'year', 'month'. You can override using the\n`.groups` argument.\n\nmean_delay_by_day\n\n# A tibble: 365 × 4\n# Groups:   year, month [12]\n    year month   day delay\n   <int> <int> <int> <dbl>\n 1  2013     1     1 11.5 \n 2  2013     1     2 13.9 \n 3  2013     1     3 11.0 \n 4  2013     1     4  8.95\n 5  2013     1     5  5.73\n 6  2013     1     6  7.15\n 7  2013     1     7  5.42\n 8  2013     1     8  2.55\n 9  2013     1     9  2.28\n10  2013     1    10  2.84\n# … with 355 more rows\n\n\n\n\n2.5 select()\n\n컬럼 선택 (순서 지정 가능)\n\n\n### p.s 컬럼이름 되도록 띄어쓰기 사용×\nflights %>% select(year,month,day)\n\n# A tibble: 336,776 × 3\n    year month   day\n   <int> <int> <int>\n 1  2013     1     1\n 2  2013     1     1\n 3  2013     1     1\n 4  2013     1     1\n 5  2013     1     1\n 6  2013     1     1\n 7  2013     1     1\n 8  2013     1     1\n 9  2013     1     1\n10  2013     1     1\n# … with 336,766 more rows\n\n\n\n\n2.6 컬럼 선택\n\n### year ~ day, delay로 끝나는 컬럼 외 2개 컬럼\nflights_sample <- flights %>% \n  select(year:day, ends_with(\"delay\"), distance, air_time)\n\nflights_sample\n\n# A tibble: 336,776 × 7\n    year month   day dep_delay arr_delay distance air_time\n   <int> <int> <int>     <dbl>     <dbl>    <dbl>    <dbl>\n 1  2013     1     1         2        11     1400      227\n 2  2013     1     1         4        20     1416      227\n 3  2013     1     1         2        33     1089      160\n 4  2013     1     1        -1       -18     1576      183\n 5  2013     1     1        -6       -25      762      116\n 6  2013     1     1        -4        12      719      150\n 7  2013     1     1        -5        19     1065      158\n 8  2013     1     1        -3       -14      229       53\n 9  2013     1     1        -3        -8      944      140\n10  2013     1     1        -2         8      733      138\n# … with 336,766 more rows\n\n\n\n\n2.7 mutate()\n\n새로운 컬럼 생성\n\n\nflights_sample %>% \n  mutate(net_delay = arr_delay - dep_delay,\n         speed = distance/air_time*60)\n\n# A tibble: 336,776 × 9\n    year month   day dep_delay arr_delay distance air_time net_delay speed\n   <int> <int> <int>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl> <dbl>\n 1  2013     1     1         2        11     1400      227         9  370.\n 2  2013     1     1         4        20     1416      227        16  374.\n 3  2013     1     1         2        33     1089      160        31  408.\n 4  2013     1     1        -1       -18     1576      183       -17  517.\n 5  2013     1     1        -6       -25      762      116       -19  394.\n 6  2013     1     1        -4        12      719      150        16  288.\n 7  2013     1     1        -5        19     1065      158        24  404.\n 8  2013     1     1        -3       -14      229       53       -11  259.\n 9  2013     1     1        -3        -8      944      140        -5  405.\n10  2013     1     1        -2         8      733      138        10  319.\n# … with 336,766 more rows\n\n\n\n\n2.8 filter()\n\n필터링 (조건 설정)\n\n\nflights %>% filter(month==1)\n\n# A tibble: 27,004 × 19\n    year month   day dep_time sched_de…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n   <int> <int> <int>    <int>      <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n 1  2013     1     1      517        515       2     830     819      11 UA     \n 2  2013     1     1      533        529       4     850     830      20 UA     \n 3  2013     1     1      542        540       2     923     850      33 AA     \n 4  2013     1     1      544        545      -1    1004    1022     -18 B6     \n 5  2013     1     1      554        600      -6     812     837     -25 DL     \n 6  2013     1     1      554        558      -4     740     728      12 UA     \n 7  2013     1     1      555        600      -5     913     854      19 B6     \n 8  2013     1     1      557        600      -3     709     723     -14 EV     \n 9  2013     1     1      557        600      -3     838     846      -8 B6     \n10  2013     1     1      558        600      -2     753     745       8 AA     \n# … with 26,994 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n\n\n\n\n2.9 bind_rows()\n\n두 데이터 묶기\n\n\njan <- flights %>% filter(month==1)\nfeb <- flights %>% filter(month==2)\njanfeb <- bind_rows(jan,feb)\n\njanfeb %>% head()\n\n# A tibble: 6 × 19\n   year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n  <int> <int> <int>    <int>       <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n1  2013     1     1      517         515       2     830     819      11 UA     \n2  2013     1     1      533         529       4     850     830      20 UA     \n3  2013     1     1      542         540       2     923     850      33 AA     \n4  2013     1     1      544         545      -1    1004    1022     -18 B6     \n5  2013     1     1      554         600      -6     812     837     -25 DL     \n6  2013     1     1      554         558      -4     740     728      12 UA     \n# … with 9 more variables: flight <int>, tailnum <chr>, origin <chr>,\n#   dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>, and abbreviated variable names ¹​sched_dep_time,\n#   ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n\njanfeb %>% tail()\n\n# A tibble: 6 × 19\n   year month   day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n  <int> <int> <int>    <int>       <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n1  2013     2    28       NA         605      NA      NA     805      NA MQ     \n2  2013     2    28       NA         850      NA      NA    1035      NA MQ     \n3  2013     2    28       NA         905      NA      NA    1115      NA MQ     \n4  2013     2    28       NA        1115      NA      NA    1310      NA MQ     \n5  2013     2    28       NA         830      NA      NA    1205      NA UA     \n6  2013     2    28       NA         840      NA      NA    1147      NA UA     \n# … with 9 more variables: flight <int>, tailnum <chr>, origin <chr>,\n#   dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>, and abbreviated variable names ¹​sched_dep_time,\n#   ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n\n### filter로 간결하게 사용\nflights %>% filter(month %in% c(1,2))\n\n# A tibble: 51,955 × 19\n    year month   day dep_time sched_de…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n   <int> <int> <int>    <int>      <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n 1  2013     1     1      517        515       2     830     819      11 UA     \n 2  2013     1     1      533        529       4     850     830      20 UA     \n 3  2013     1     1      542        540       2     923     850      33 AA     \n 4  2013     1     1      544        545      -1    1004    1022     -18 B6     \n 5  2013     1     1      554        600      -6     812     837     -25 DL     \n 6  2013     1     1      554        558      -4     740     728      12 UA     \n 7  2013     1     1      555        600      -5     913     854      19 B6     \n 8  2013     1     1      557        600      -3     709     723     -14 EV     \n 9  2013     1     1      557        600      -3     838     846      -8 B6     \n10  2013     1     1      558        600      -2     753     745       8 AA     \n# … with 51,945 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n\n\n\n\n2.10 arrange()\n\n데이터 정렬 (오름/내림차순)\n\n\nflights %>% arrange(dep_delay)\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_de…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n   <int> <int> <int>    <int>      <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n 1  2013    12     7     2040       2123     -43      40    2352      48 B6     \n 2  2013     2     3     2022       2055     -33    2240    2338     -58 DL     \n 3  2013    11    10     1408       1440     -32    1549    1559     -10 EV     \n 4  2013     1    11     1900       1930     -30    2233    2243     -10 DL     \n 5  2013     1    29     1703       1730     -27    1947    1957     -10 F9     \n 6  2013     8     9      729        755     -26    1002     955       7 MQ     \n 7  2013    10    23     1907       1932     -25    2143    2143       0 EV     \n 8  2013     3    30     2030       2055     -25    2213    2250     -37 MQ     \n 9  2013     3     2     1431       1455     -24    1601    1631     -30 9E     \n10  2013     5     5      934        958     -24    1225    1309     -44 B6     \n# … with 336,766 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n\nflights %>% arrange(-dep_delay)\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_de…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n   <int> <int> <int>    <int>      <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n 1  2013     1     9      641        900    1301    1242    1530    1272 HA     \n 2  2013     6    15     1432       1935    1137    1607    2120    1127 MQ     \n 3  2013     1    10     1121       1635    1126    1239    1810    1109 MQ     \n 4  2013     9    20     1139       1845    1014    1457    2210    1007 AA     \n 5  2013     7    22      845       1600    1005    1044    1815     989 MQ     \n 6  2013     4    10     1100       1900     960    1342    2211     931 DL     \n 7  2013     3    17     2321        810     911     135    1020     915 DL     \n 8  2013     6    27      959       1900     899    1236    2226     850 DL     \n 9  2013     7    22     2257        759     898     121    1026     895 DL     \n10  2013    12     5      756       1700     896    1058    2020     878 AA     \n# … with 336,766 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n\n\n\n\n2.11 그룹화 + 집계값 요약\n\n월별 평균 지연시간, 개수\n\n\nflights %>% \n  group_by(month) %>% \n  summarise(mean_dep_delay = mean(dep_delay, na.rm = T),\n            count = n())\n\n# A tibble: 12 × 3\n   month mean_dep_delay count\n   <int>          <dbl> <int>\n 1     1          10.0  27004\n 2     2          10.8  24951\n 3     3          13.2  28834\n 4     4          13.9  28330\n 5     5          13.0  28796\n 6     6          20.8  28243\n 7     7          21.7  29425\n 8     8          12.6  29327\n 9     9           6.72 27574\n10    10           6.24 28889\n11    11           5.44 27268\n12    12          16.6  28135\n\n\n\n\n2.12 left_join\n\n### 컬럼 선택\nflights_sample2 <- flights %>% \n  select(year:day, origin, carrier)\n\nflights_sample2\n\n# A tibble: 336,776 × 5\n    year month   day origin carrier\n   <int> <int> <int> <chr>  <chr>  \n 1  2013     1     1 EWR    UA     \n 2  2013     1     1 LGA    UA     \n 3  2013     1     1 JFK    AA     \n 4  2013     1     1 JFK    B6     \n 5  2013     1     1 LGA    DL     \n 6  2013     1     1 EWR    UA     \n 7  2013     1     1 EWR    B6     \n 8  2013     1     1 LGA    EV     \n 9  2013     1     1 JFK    B6     \n10  2013     1     1 LGA    AA     \n# … with 336,766 more rows\n\n### join 시킬 데이터셋 - 'airlines'\nairlines\n\n# A tibble: 16 × 2\n   carrier name                       \n   <chr>   <chr>                      \n 1 9E      Endeavor Air Inc.          \n 2 AA      American Airlines Inc.     \n 3 AS      Alaska Airlines Inc.       \n 4 B6      JetBlue Airways            \n 5 DL      Delta Air Lines Inc.       \n 6 EV      ExpressJet Airlines Inc.   \n 7 F9      Frontier Airlines Inc.     \n 8 FL      AirTran Airways Corporation\n 9 HA      Hawaiian Airlines Inc.     \n10 MQ      Envoy Air                  \n11 OO      SkyWest Airlines Inc.      \n12 UA      United Air Lines Inc.      \n13 US      US Airways Inc.            \n14 VX      Virgin America             \n15 WN      Southwest Airlines Co.     \n16 YV      Mesa Airlines Inc.         \n\n### key값 기준으로 왼쪽에 join\nflights_sample2 %>% \n  left_join(airlines, by=\"carrier\")\n\n# A tibble: 336,776 × 6\n    year month   day origin carrier name                    \n   <int> <int> <int> <chr>  <chr>   <chr>                   \n 1  2013     1     1 EWR    UA      United Air Lines Inc.   \n 2  2013     1     1 LGA    UA      United Air Lines Inc.   \n 3  2013     1     1 JFK    AA      American Airlines Inc.  \n 4  2013     1     1 JFK    B6      JetBlue Airways         \n 5  2013     1     1 LGA    DL      Delta Air Lines Inc.    \n 6  2013     1     1 EWR    UA      United Air Lines Inc.   \n 7  2013     1     1 EWR    B6      JetBlue Airways         \n 8  2013     1     1 LGA    EV      ExpressJet Airlines Inc.\n 9  2013     1     1 JFK    B6      JetBlue Airways         \n10  2013     1     1 LGA    AA      American Airlines Inc.  \n# … with 336,766 more rows\n\n### key값 컬럼 이름이 다를 때\ncolnames(airlines)[1] <- 'different'\n\nflights_sample2 %>% \n  left_join(airlines, by=c(\"carrier\" = \"different\"))\n\n# A tibble: 336,776 × 6\n    year month   day origin carrier name                    \n   <int> <int> <int> <chr>  <chr>   <chr>                   \n 1  2013     1     1 EWR    UA      United Air Lines Inc.   \n 2  2013     1     1 LGA    UA      United Air Lines Inc.   \n 3  2013     1     1 JFK    AA      American Airlines Inc.  \n 4  2013     1     1 JFK    B6      JetBlue Airways         \n 5  2013     1     1 LGA    DL      Delta Air Lines Inc.    \n 6  2013     1     1 EWR    UA      United Air Lines Inc.   \n 7  2013     1     1 EWR    B6      JetBlue Airways         \n 8  2013     1     1 LGA    EV      ExpressJet Airlines Inc.\n 9  2013     1     1 JFK    B6      JetBlue Airways         \n10  2013     1     1 LGA    AA      American Airlines Inc.  \n# … with 336,766 more rows"
  },
  {
    "objectID": "posts/Opendata_Analysis Ch1/tidyverse.html#quiz",
    "href": "posts/Opendata_Analysis Ch1/tidyverse.html#quiz",
    "title": "Opendata_Analysis CH1",
    "section": "3 Quiz",
    "text": "3 Quiz\n\n\n월 마다 가장 연착이 긴 시간 톺아보기\n\n\n월 마다 가장 연착이 많이된 비행기는 해당월의 평균보다 몇배가 높나?\n\n\n\n### 1\nflights %>%\n  group_by(month) %>% \n  summarise(max_dep_delay = max(dep_delay, na.rm = T)) %>% \n  arrange(-max_dep_delay)\n\n# A tibble: 12 × 2\n   month max_dep_delay\n   <int>         <dbl>\n 1     1          1301\n 2     6          1137\n 3     9          1014\n 4     7          1005\n 5     4           960\n 6     3           911\n 7    12           896\n 8     5           878\n 9     2           853\n10    11           798\n11    10           702\n12     8           520\n\n### 2\nflights %>%\n  group_by(month) %>% \n  summarise(mean_dep_delay = mean(dep_delay, na.rm = T),\n            max_dep_delay = max(dep_delay, na.rm = T),\n            compare = max_dep_delay/mean_dep_delay)\n\n# A tibble: 12 × 4\n   month mean_dep_delay max_dep_delay compare\n   <int>          <dbl>         <dbl>   <dbl>\n 1     1          10.0           1301   130. \n 2     2          10.8            853    78.9\n 3     3          13.2            911    68.9\n 4     4          13.9            960    68.9\n 5     5          13.0            878    67.6\n 6     6          20.8           1137    54.5\n 7     7          21.7           1005    46.3\n 8     8          12.6            520    41.2\n 9     9           6.72          1014   151. \n10    10           6.24           702   112. \n11    11           5.44           798   147. \n12    12          16.6            896    54.1"
  },
  {
    "objectID": "posts/Opendata_Analysis Ch2/data_transformation.html",
    "href": "posts/Opendata_Analysis Ch2/data_transformation.html",
    "title": "Opendata_Analysis CH2",
    "section": "",
    "text": "HTML파일로 보기\nData Transformaion"
  },
  {
    "objectID": "posts/Opendata_Analysis Ch2/data_transformation.html#오픈데이터-분석-실습-data-transformation",
    "href": "posts/Opendata_Analysis Ch2/data_transformation.html#오픈데이터-분석-실습-data-transformation",
    "title": "Opendata_Analysis CH2",
    "section": "1 오픈데이터 분석 실습 : Data Transformation",
    "text": "1 오픈데이터 분석 실습 : Data Transformation\n\n1.1 패키지 불러오기\n\nlibrary(tidyverse)\nlibrary(nycflights13)\n\n\n\n1.2 간단한 데이터셋 파악\n\nnycflights13\n\n\n### 도움말\n#?flights\n\n### 요약\nflights %>% str()\n\ntibble [336,776 × 19] (S3: tbl_df/tbl/data.frame)\n $ year          : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...\n $ month         : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n $ day           : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n $ dep_time      : int [1:336776] 517 533 542 544 554 554 555 557 557 558 ...\n $ sched_dep_time: int [1:336776] 515 529 540 545 600 558 600 600 600 600 ...\n $ dep_delay     : num [1:336776] 2 4 2 -1 -6 -4 -5 -3 -3 -2 ...\n $ arr_time      : int [1:336776] 830 850 923 1004 812 740 913 709 838 753 ...\n $ sched_arr_time: int [1:336776] 819 830 850 1022 837 728 854 723 846 745 ...\n $ arr_delay     : num [1:336776] 11 20 33 -18 -25 12 19 -14 -8 8 ...\n $ carrier       : chr [1:336776] \"UA\" \"UA\" \"AA\" \"B6\" ...\n $ flight        : int [1:336776] 1545 1714 1141 725 461 1696 507 5708 79 301 ...\n $ tailnum       : chr [1:336776] \"N14228\" \"N24211\" \"N619AA\" \"N804JB\" ...\n $ origin        : chr [1:336776] \"EWR\" \"LGA\" \"JFK\" \"JFK\" ...\n $ dest          : chr [1:336776] \"IAH\" \"IAH\" \"MIA\" \"BQN\" ...\n $ air_time      : num [1:336776] 227 227 160 183 116 150 158 53 140 138 ...\n $ distance      : num [1:336776] 1400 1416 1089 1576 762 ...\n $ hour          : num [1:336776] 5 5 5 5 6 5 6 6 6 6 ...\n $ minute        : num [1:336776] 15 29 40 45 0 58 0 0 0 0 ...\n $ time_hour     : POSIXct[1:336776], format: \"2013-01-01 05:00:00\" \"2013-01-01 05:00:00\" ...\n\n### 컬럼 이름\nflights %>% colnames()\n\n [1] \"year\"           \"month\"          \"day\"            \"dep_time\"      \n [5] \"sched_dep_time\" \"dep_delay\"      \"arr_time\"       \"sched_arr_time\"\n [9] \"arr_delay\"      \"carrier\"        \"flight\"         \"tailnum\"       \n[13] \"origin\"         \"dest\"           \"air_time\"       \"distance\"      \n[17] \"hour\"           \"minute\"         \"time_hour\""
  },
  {
    "objectID": "posts/Opendata_Analysis Ch2/data_transformation.html#dplyr-기초",
    "href": "posts/Opendata_Analysis Ch2/data_transformation.html#dplyr-기초",
    "title": "Opendata_Analysis CH2",
    "section": "2 dplyr 기초",
    "text": "2 dplyr 기초\n\n2.1 filter\n\n조건 필터링\n\n\n### table - 빈도수 파악\nflights$month %>% table()\n\n.\n    1     2     3     4     5     6     7     8     9    10    11    12 \n27004 24951 28834 28330 28796 28243 29425 29327 27574 28889 27268 28135 \n\n### 월별 필터링\nflights %>%\n  filter(month==12 | month==11)\n\n# A tibble: 55,403 × 19\n    year month   day dep_time sched_de…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n   <int> <int> <int>    <int>      <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n 1  2013    11     1        5       2359       6     352     345       7 B6     \n 2  2013    11     1       35       2250     105     123    2356      87 B6     \n 3  2013    11     1      455        500      -5     641     651     -10 US     \n 4  2013    11     1      539        545      -6     856     827      29 UA     \n 5  2013    11     1      542        545      -3     831     855     -24 AA     \n 6  2013    11     1      549        600     -11     912     923     -11 UA     \n 7  2013    11     1      550        600     -10     705     659       6 US     \n 8  2013    11     1      554        600      -6     659     701      -2 US     \n 9  2013    11     1      554        600      -6     826     827      -1 DL     \n10  2013    11     1      554        600      -6     749     751      -2 DL     \n# … with 55,393 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n\n### 월별 필터링 - %in% 사용\nflights %>% \n  filter(month %in% c(11,12))\n\n# A tibble: 55,403 × 19\n    year month   day dep_time sched_de…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n   <int> <int> <int>    <int>      <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n 1  2013    11     1        5       2359       6     352     345       7 B6     \n 2  2013    11     1       35       2250     105     123    2356      87 B6     \n 3  2013    11     1      455        500      -5     641     651     -10 US     \n 4  2013    11     1      539        545      -6     856     827      29 UA     \n 5  2013    11     1      542        545      -3     831     855     -24 AA     \n 6  2013    11     1      549        600     -11     912     923     -11 UA     \n 7  2013    11     1      550        600     -10     705     659       6 US     \n 8  2013    11     1      554        600      -6     659     701      -2 US     \n 9  2013    11     1      554        600      -6     826     827      -1 DL     \n10  2013    11     1      554        600      -6     749     751      -2 DL     \n# … with 55,393 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n\n\n\n\n2.2 missing values\n\n결측값 NA\n\n\n### 결측값은 연산 불가능\nNA\n\n[1] NA\n\nNA > 5\n\n[1] NA\n\n10 == NA\n\n[1] NA\n\nNA + 19\n\n[1] NA\n\nNA / 2\n\n[1] NA\n\n### 결측값 개수\nflights$dep_time %>%\n  is.na() %>% \n  sum()\n\n[1] 8255\n\n### 결측값 있는 df 생성\ndf <- data.frame(x = c(1, NA, 3))\ndf\n\n   x\n1  1\n2 NA\n3  3\n\n### 타입 확인\ndf %>% class()\n\n[1] \"data.frame\"\n\n### tibble : tidyverse의 df 클래스\ntibble_df <- tibble(x = c(1, NA, 3))\ntibble_df\n\n# A tibble: 3 × 1\n      x\n  <dbl>\n1     1\n2    NA\n3     3\n\ntibble_df %>% filter(x > 1)\n\n# A tibble: 1 × 1\n      x\n  <dbl>\n1     3\n\ntibble_df %>% filter(is.na(x) | x>1)\n\n# A tibble: 2 × 1\n      x\n  <dbl>\n1    NA\n2     3\n\n\n\n\n2.3 arrange\n\n정렬\n\n\n### 오름차순 정렬\nflights %>% arrange(dep_time)\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_de…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n   <int> <int> <int>    <int>      <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n 1  2013     1    13        1       2249      72     108    2357      71 B6     \n 2  2013     1    31        1       2100     181     124    2225     179 WN     \n 3  2013    11    13        1       2359       2     442     440       2 B6     \n 4  2013    12    16        1       2359       2     447     437      10 B6     \n 5  2013    12    20        1       2359       2     430     440     -10 B6     \n 6  2013    12    26        1       2359       2     437     440      -3 B6     \n 7  2013    12    30        1       2359       2     441     437       4 B6     \n 8  2013     2    11        1       2100     181     111    2225     166 WN     \n 9  2013     2    24        1       2245      76     121    2354      87 B6     \n10  2013     3     8        1       2355       6     431     440      -9 B6     \n# … with 336,766 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n\n### 결측값은 가장 하단 배치\ndf <- tibble(x = c(5,2,NA))\narrange(df, x)\n\n# A tibble: 3 × 1\n      x\n  <dbl>\n1     2\n2     5\n3    NA\n\narrange(df, -x)\n\n# A tibble: 3 × 1\n      x\n  <dbl>\n1     5\n2     2\n3    NA\n\n\n\n\n2.4 select\n\n컬럼 선택\n\n\n### 컬럼 선택 (순서 지정 가능)\nflights %>% select(year, month, day)\n\n# A tibble: 336,776 × 3\n    year month   day\n   <int> <int> <int>\n 1  2013     1     1\n 2  2013     1     1\n 3  2013     1     1\n 4  2013     1     1\n 5  2013     1     1\n 6  2013     1     1\n 7  2013     1     1\n 8  2013     1     1\n 9  2013     1     1\n10  2013     1     1\n# … with 336,766 more rows\n\nflights %>% select(year:day)\n\n# A tibble: 336,776 × 3\n    year month   day\n   <int> <int> <int>\n 1  2013     1     1\n 2  2013     1     1\n 3  2013     1     1\n 4  2013     1     1\n 5  2013     1     1\n 6  2013     1     1\n 7  2013     1     1\n 8  2013     1     1\n 9  2013     1     1\n10  2013     1     1\n# … with 336,766 more rows\n\n### 컬럼 제외\nflights %>% select(-(year:day))\n\n# A tibble: 336,776 × 16\n   dep_t…¹ sched…² dep_d…³ arr_t…⁴ sched…⁵ arr_d…⁶ carrier flight tailnum origin\n     <int>   <int>   <dbl>   <int>   <int>   <dbl> <chr>    <int> <chr>   <chr> \n 1     517     515       2     830     819      11 UA        1545 N14228  EWR   \n 2     533     529       4     850     830      20 UA        1714 N24211  LGA   \n 3     542     540       2     923     850      33 AA        1141 N619AA  JFK   \n 4     544     545      -1    1004    1022     -18 B6         725 N804JB  JFK   \n 5     554     600      -6     812     837     -25 DL         461 N668DN  LGA   \n 6     554     558      -4     740     728      12 UA        1696 N39463  EWR   \n 7     555     600      -5     913     854      19 B6         507 N516JB  EWR   \n 8     557     600      -3     709     723     -14 EV        5708 N829AS  LGA   \n 9     557     600      -3     838     846      -8 B6          79 N593JB  JFK   \n10     558     600      -2     753     745       8 AA         301 N3ALAA  LGA   \n# … with 336,766 more rows, 6 more variables: dest <chr>, air_time <dbl>,\n#   distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>, and abbreviated\n#   variable names ¹​dep_time, ²​sched_dep_time, ³​dep_delay, ⁴​arr_time,\n#   ⁵​sched_arr_time, ⁶​arr_delay\n\n### 시작 문자열 지정\nflights %>% select(starts_with('dep'))\n\n# A tibble: 336,776 × 2\n   dep_time dep_delay\n      <int>     <dbl>\n 1      517         2\n 2      533         4\n 3      542         2\n 4      544        -1\n 5      554        -6\n 6      554        -4\n 7      555        -5\n 8      557        -3\n 9      557        -3\n10      558        -2\n# … with 336,766 more rows\n\n### 끝 문자열 지정\nflights %>% select(ends_with(\"time\"))\n\n# A tibble: 336,776 × 5\n   dep_time sched_dep_time arr_time sched_arr_time air_time\n      <int>          <int>    <int>          <int>    <dbl>\n 1      517            515      830            819      227\n 2      533            529      850            830      227\n 3      542            540      923            850      160\n 4      544            545     1004           1022      183\n 5      554            600      812            837      116\n 6      554            558      740            728      150\n 7      555            600      913            854      158\n 8      557            600      709            723       53\n 9      557            600      838            846      140\n10      558            600      753            745      138\n# … with 336,766 more rows\n\n### 포함 문자열 지정\nflights %>% select(contains(\"time\"))\n\n# A tibble: 336,776 × 6\n   dep_time sched_dep_time arr_time sched_arr_time air_time time_hour          \n      <int>          <int>    <int>          <int>    <dbl> <dttm>             \n 1      517            515      830            819      227 2013-01-01 05:00:00\n 2      533            529      850            830      227 2013-01-01 05:00:00\n 3      542            540      923            850      160 2013-01-01 05:00:00\n 4      544            545     1004           1022      183 2013-01-01 05:00:00\n 5      554            600      812            837      116 2013-01-01 06:00:00\n 6      554            558      740            728      150 2013-01-01 05:00:00\n 7      555            600      913            854      158 2013-01-01 06:00:00\n 8      557            600      709            723       53 2013-01-01 06:00:00\n 9      557            600      838            846      140 2013-01-01 06:00:00\n10      558            600      753            745      138 2013-01-01 06:00:00\n# … with 336,766 more rows\n\n### 컬럼 이름 변경 new = old\nflights %>% rename(tail_num = tailnum)\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_de…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n   <int> <int> <int>    <int>      <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n 1  2013     1     1      517        515       2     830     819      11 UA     \n 2  2013     1     1      533        529       4     850     830      20 UA     \n 3  2013     1     1      542        540       2     923     850      33 AA     \n 4  2013     1     1      544        545      -1    1004    1022     -18 B6     \n 5  2013     1     1      554        600      -6     812     837     -25 DL     \n 6  2013     1     1      554        558      -4     740     728      12 UA     \n 7  2013     1     1      555        600      -5     913     854      19 B6     \n 8  2013     1     1      557        600      -3     709     723     -14 EV     \n 9  2013     1     1      557        600      -3     838     846      -8 B6     \n10  2013     1     1      558        600      -2     753     745       8 AA     \n# … with 336,766 more rows, 9 more variables: flight <int>, tail_num <chr>,\n#   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n\n### 순서 배치 + 나머지\nflights %>% select(time_hour, air_time, everything())\n\n# A tibble: 336,776 × 19\n   time_hour           air_t…¹  year month   day dep_t…² sched…³ dep_d…⁴ arr_t…⁵\n   <dttm>                <dbl> <int> <int> <int>   <int>   <int>   <dbl>   <int>\n 1 2013-01-01 05:00:00     227  2013     1     1     517     515       2     830\n 2 2013-01-01 05:00:00     227  2013     1     1     533     529       4     850\n 3 2013-01-01 05:00:00     160  2013     1     1     542     540       2     923\n 4 2013-01-01 05:00:00     183  2013     1     1     544     545      -1    1004\n 5 2013-01-01 06:00:00     116  2013     1     1     554     600      -6     812\n 6 2013-01-01 05:00:00     150  2013     1     1     554     558      -4     740\n 7 2013-01-01 06:00:00     158  2013     1     1     555     600      -5     913\n 8 2013-01-01 06:00:00      53  2013     1     1     557     600      -3     709\n 9 2013-01-01 06:00:00     140  2013     1     1     557     600      -3     838\n10 2013-01-01 06:00:00     138  2013     1     1     558     600      -2     753\n# … with 336,766 more rows, 10 more variables: sched_arr_time <int>,\n#   arr_delay <dbl>, carrier <chr>, flight <int>, tailnum <chr>, origin <chr>,\n#   dest <chr>, distance <dbl>, hour <dbl>, minute <dbl>, and abbreviated\n#   variable names ¹​air_time, ²​dep_time, ³​sched_dep_time, ⁴​dep_delay, ⁵​arr_time\n\n\n\n\n2.5 mutate\n\n새로운 컬럼 생성\n\n\nflights %>% mutate(\n  gain = dep_delay - arr_delay,\n  hours = air_time / 60,\n  gain_per_hour = gain / hours\n)\n\n# A tibble: 336,776 × 22\n    year month   day dep_time sched_de…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n   <int> <int> <int>    <int>      <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n 1  2013     1     1      517        515       2     830     819      11 UA     \n 2  2013     1     1      533        529       4     850     830      20 UA     \n 3  2013     1     1      542        540       2     923     850      33 AA     \n 4  2013     1     1      544        545      -1    1004    1022     -18 B6     \n 5  2013     1     1      554        600      -6     812     837     -25 DL     \n 6  2013     1     1      554        558      -4     740     728      12 UA     \n 7  2013     1     1      555        600      -5     913     854      19 B6     \n 8  2013     1     1      557        600      -3     709     723     -14 EV     \n 9  2013     1     1      557        600      -3     838     846      -8 B6     \n10  2013     1     1      558        600      -2     753     745       8 AA     \n# … with 336,766 more rows, 12 more variables: flight <int>, tailnum <chr>,\n#   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>, gain <dbl>, hours <dbl>,\n#   gain_per_hour <dbl>, and abbreviated variable names ¹​sched_dep_time,\n#   ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n\n### transmute : mutate와 다르게 반환된 값만 출력 \nflights %>% transmute(\n  gain = dep_delay - arr_delay,\n  hours = air_time / 60,\n  gain_per_hour = gain / hours\n)\n\n# A tibble: 336,776 × 3\n    gain hours gain_per_hour\n   <dbl> <dbl>         <dbl>\n 1    -9 3.78          -2.38\n 2   -16 3.78          -4.23\n 3   -31 2.67         -11.6 \n 4    17 3.05           5.57\n 5    19 1.93           9.83\n 6   -16 2.5           -6.4 \n 7   -24 2.63          -9.11\n 8    11 0.883         12.5 \n 9     5 2.33           2.14\n10   -10 2.3           -4.35\n# … with 336,766 more rows\n\n\n\n\n2.6 유용한 기능\n\n### 몫\n5 %/% 3\n\n[1] 1\n\n### 나머지\n5 %% 3\n\n[1] 2\n\n### 그룹화 + 집계값 요약\nflights %>% \n  group_by(year, month, day) %>% \n  summarise(mean = mean(dep_delay))\n\n`summarise()` has grouped output by 'year', 'month'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 365 × 4\n# Groups:   year, month [12]\n    year month   day  mean\n   <int> <int> <int> <dbl>\n 1  2013     1     1    NA\n 2  2013     1     2    NA\n 3  2013     1     3    NA\n 4  2013     1     4    NA\n 5  2013     1     5    NA\n 6  2013     1     6    NA\n 7  2013     1     7    NA\n 8  2013     1     8    NA\n 9  2013     1     9    NA\n10  2013     1    10    NA\n# … with 355 more rows\n\n### 그룹화 + 집계값 요약 + NA 제거\nflights %>% \n  group_by(year, month, day) %>% \n  summarise(mean = mean(dep_delay, na.rm = T))\n\n`summarise()` has grouped output by 'year', 'month'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 365 × 4\n# Groups:   year, month [12]\n    year month   day  mean\n   <int> <int> <int> <dbl>\n 1  2013     1     1 11.5 \n 2  2013     1     2 13.9 \n 3  2013     1     3 11.0 \n 4  2013     1     4  8.95\n 5  2013     1     5  5.73\n 6  2013     1     6  7.15\n 7  2013     1     7  5.42\n 8  2013     1     8  2.55\n 9  2013     1     9  2.28\n10  2013     1    10  2.84\n# … with 355 more rows\n\n### !is.na : 결측값 아닌 값들만 출력\nflights %>% \n  filter(!is.na(dep_delay), !is.na(arr_delay))\n\n# A tibble: 327,346 × 19\n    year month   day dep_time sched_de…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n   <int> <int> <int>    <int>      <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n 1  2013     1     1      517        515       2     830     819      11 UA     \n 2  2013     1     1      533        529       4     850     830      20 UA     \n 3  2013     1     1      542        540       2     923     850      33 AA     \n 4  2013     1     1      544        545      -1    1004    1022     -18 B6     \n 5  2013     1     1      554        600      -6     812     837     -25 DL     \n 6  2013     1     1      554        558      -4     740     728      12 UA     \n 7  2013     1     1      555        600      -5     913     854      19 B6     \n 8  2013     1     1      557        600      -3     709     723     -14 EV     \n 9  2013     1     1      557        600      -3     838     846      -8 B6     \n10  2013     1     1      558        600      -2     753     745       8 AA     \n# … with 327,336 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay\n\n### 그룹화 해제 후 집계값 요약\ngroup <- flights %>%\n  group_by(year, month, day)\n\ngroup %>% \n  ungroup() %>% \n  summarise(flights = n())\n\n# A tibble: 1 × 1\n  flights\n    <int>\n1  336776\n\n### drop_na\ntibble_df %>% drop_na()\n\n# A tibble: 2 × 1\n      x\n  <dbl>\n1     1\n2     3\n\n### mutate 컬럼 위치 지정\nflights %>%\n  mutate(mean_arr_time = mean(arr_time, na.rm = T), .after = arr_time)\n\n# A tibble: 336,776 × 20\n    year month   day dep_time sched_de…¹ dep_d…² arr_t…³ mean_…⁴ sched…⁵ arr_d…⁶\n   <int> <int> <int>    <int>      <int>   <dbl>   <int>   <dbl>   <int>   <dbl>\n 1  2013     1     1      517        515       2     830   1502.     819      11\n 2  2013     1     1      533        529       4     850   1502.     830      20\n 3  2013     1     1      542        540       2     923   1502.     850      33\n 4  2013     1     1      544        545      -1    1004   1502.    1022     -18\n 5  2013     1     1      554        600      -6     812   1502.     837     -25\n 6  2013     1     1      554        558      -4     740   1502.     728      12\n 7  2013     1     1      555        600      -5     913   1502.     854      19\n 8  2013     1     1      557        600      -3     709   1502.     723     -14\n 9  2013     1     1      557        600      -3     838   1502.     846      -8\n10  2013     1     1      558        600      -2     753   1502.     745       8\n# … with 336,766 more rows, 10 more variables: carrier <chr>, flight <int>,\n#   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>,\n#   hour <dbl>, minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​mean_arr_time, ⁵​sched_arr_time,\n#   ⁶​arr_delay\n\n\n\n\n2.7 rank, slice\n\n행의 범위 지정\n\n\n### 년/월/일자별 top10 'arr_delay' 출력\n\n### rank 사용\nflights %>% \n  group_by(year,month,day) %>% \n  filter(rank(desc(arr_delay)) < 11) %>% \n  select(year,month,day, arr_delay)\n\n# A tibble: 3,691 × 4\n# Groups:   year, month, day [365]\n    year month   day arr_delay\n   <int> <int> <int>     <dbl>\n 1  2013     1     1       851\n 2  2013     1     1       338\n 3  2013     1     1       263\n 4  2013     1     1       166\n 5  2013     1     1       174\n 6  2013     1     1       222\n 7  2013     1     1       250\n 8  2013     1     1       246\n 9  2013     1     1       191\n10  2013     1     1       456\n# … with 3,681 more rows\n\n### slice 사용\nflights %>% \n  group_by(year,month,day) %>% \n  slice_max(arr_delay,n=10) %>% \n  select(year,month,day, arr_delay)\n\n# A tibble: 3,697 × 4\n# Groups:   year, month, day [365]\n    year month   day arr_delay\n   <int> <int> <int>     <dbl>\n 1  2013     1     1       851\n 2  2013     1     1       456\n 3  2013     1     1       338\n 4  2013     1     1       263\n 5  2013     1     1       250\n 6  2013     1     1       246\n 7  2013     1     1       222\n 8  2013     1     1       191\n 9  2013     1     1       174\n10  2013     1     1       166\n# … with 3,687 more rows\n\n### 순위 책정\nflights %>% \n  select(year,month,day,arr_delay) %>% \n  arrange(year,month,day,desc(arr_delay)) %>% \n  group_by(year,month,day) %>% \n  mutate(rank = rank(desc(arr_delay)))\n\n# A tibble: 336,776 × 5\n# Groups:   year, month, day [365]\n    year month   day arr_delay  rank\n   <int> <int> <int>     <dbl> <dbl>\n 1  2013     1     1       851     1\n 2  2013     1     1       456     2\n 3  2013     1     1       338     3\n 4  2013     1     1       263     4\n 5  2013     1     1       250     5\n 6  2013     1     1       246     6\n 7  2013     1     1       222     7\n 8  2013     1     1       191     8\n 9  2013     1     1       174     9\n10  2013     1     1       166    10\n# … with 336,766 more rows"
  },
  {
    "objectID": "posts/Opendata_Analysis Ch2/data_transformation.html#quiz-1",
    "href": "posts/Opendata_Analysis Ch2/data_transformation.html#quiz-1",
    "title": "Opendata_Analysis CH2",
    "section": "3 Quiz 1",
    "text": "3 Quiz 1\n\ndep_time의 결측지를 dep_time의 평균 값으로 교체하시오\n\n\n### NA 제외한 평균값\nmean_dep_time <- mean(flights$dep_time,na.rm = T) %>% as.integer()\nmean_dep_time\n\n[1] 1349\n\n### repalce_na \nflights$dep_time <- flights$dep_time %>% \n  replace_na(mean_dep_time)\n\nflights$dep_time %>% is.na() %>% sum()\n\n[1] 0\n\n### mutate + ifelse\nflights %>% \n  mutate(dep_time = ifelse(is.na(dep_time),\n                           mean_dep_time,dep_time))\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_de…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier\n   <int> <int> <int>    <int>      <int>   <dbl>   <int>   <int>   <dbl> <chr>  \n 1  2013     1     1      517        515       2     830     819      11 UA     \n 2  2013     1     1      533        529       4     850     830      20 UA     \n 3  2013     1     1      542        540       2     923     850      33 AA     \n 4  2013     1     1      544        545      -1    1004    1022     -18 B6     \n 5  2013     1     1      554        600      -6     812     837     -25 DL     \n 6  2013     1     1      554        558      -4     740     728      12 UA     \n 7  2013     1     1      555        600      -5     913     854      19 B6     \n 8  2013     1     1      557        600      -3     709     723     -14 EV     \n 9  2013     1     1      557        600      -3     838     846      -8 B6     \n10  2013     1     1      558        600      -2     753     745       8 AA     \n# … with 336,766 more rows, 9 more variables: flight <int>, tailnum <chr>,\n#   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>, and abbreviated variable names\n#   ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay"
  },
  {
    "objectID": "posts/Opendata_Analysis Ch2/data_transformation.html#quiz-2",
    "href": "posts/Opendata_Analysis Ch2/data_transformation.html#quiz-2",
    "title": "Opendata_Analysis CH2",
    "section": "4 Quiz 2",
    "text": "4 Quiz 2\n\n\n월별 비행기 개수를 구하시오\n\n\ndest(도착공항)별로 비행기가 10000대 이상 착륙한 공항으로 도착한 비행정보만 추출\n\n\n\n### 1\nflights %>%\n  group_by(month) %>% \n  summarise(fly_count = n())\n\n# A tibble: 12 × 2\n   month fly_count\n   <int>     <int>\n 1     1     27004\n 2     2     24951\n 3     3     28834\n 4     4     28330\n 5     5     28796\n 6     6     28243\n 7     7     29425\n 8     8     29327\n 9     9     27574\n10    10     28889\n11    11     27268\n12    12     28135\n\n### 2\nair <- flights %>% \n  group_by(dest) %>%\n  filter(n() >= 10000)\n\nair %>% nrow()\n\n[1] 131440\n\ntable(air$dest)\n\n\n  ATL   BOS   CLT   FLL   LAX   MCO   MIA   ORD   SFO \n17215 15508 14064 12055 16174 14082 11728 17283 13331"
  },
  {
    "objectID": "posts/Opendata_Analysis Ch2/data_transformation.html#quiz-3",
    "href": "posts/Opendata_Analysis Ch2/data_transformation.html#quiz-3",
    "title": "Opendata_Analysis CH2",
    "section": "5 Quiz 3",
    "text": "5 Quiz 3\n다음은 미국 NBA 농구리그의 농구선수별 게임당 경기통계(stats)이다.\n\nPos: 농구에서 선수의 포지션\nAge: 나이\nTm: 팀이름\n3P: 3점슛 성공횟수\n3PA: 3점슛 시도횟수\n3P%: 3점 성공률\nPTS: 평균득점\n\n일 때 다음 물음에 답하시오.\n\nstats <-  read_csv(\"C:/Users/seong taek/Desktop/3-1 Opendata_Analysis/opendata/nba2021_per_game.csv\")\n\nRows: 497 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): Player, Pos, Tm\ndbl (26): Age, G, GS, MP, FG, FGA, FG%, 3P, 3PA, 3P%, 2P, 2PA, 2P%, eFG%, FT...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstats\n\n# A tibble: 497 × 29\n   Player      Pos     Age Tm        G    GS    MP    FG   FGA `FG%`  `3P` `3PA`\n   <chr>       <chr> <dbl> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 Precious A… PF       21 MIA      28     2  14.6   2.6   4.4 0.59    0     0  \n 2 Jaylen Ada… PG       24 MIL       6     0   2.8   0.2   1.3 0.125   0     0.3\n 3 Steven Ada… C        27 NOP      27    27  28.1   3.5   5.8 0.603   0     0  \n 4 Bam Adebayo C        23 MIA      26    26  33.6   7.4  12.9 0.573   0.1   0.2\n 5 LaMarcus A… C        35 SAS      18    18  26.7   5.9  12.5 0.476   1.3   3.7\n 6 Ty-Shon Al… SG       22 PHO       3     0   2.7   0     1   0       0     0.3\n 7 Nickeil Al… SG       22 NOP      23     3  19.2   3.3   8.2 0.41    1     3.8\n 8 Grayson Al… SG       25 MEM      19     8  23.9   3.2   7.4 0.429   2.3   5.3\n 9 Jarrett Al… C        22 TOT      28    10  26.2   4.4   6.8 0.642   0     0.1\n10 Jarrett Al… C        22 BRK      12     5  26.7   3.7   5.4 0.677   0     0  \n# … with 487 more rows, and 17 more variables: `3P%` <dbl>, `2P` <dbl>,\n#   `2PA` <dbl>, `2P%` <dbl>, `eFG%` <dbl>, FT <dbl>, FTA <dbl>, `FT%` <dbl>,\n#   ORB <dbl>, DRB <dbl>, TRB <dbl>, AST <dbl>, STL <dbl>, BLK <dbl>,\n#   TOV <dbl>, PF <dbl>, PTS <dbl>\n\n\n\n5.1 1. 위의 dataframe을 가지고 group 별 통계값을 계산하시오.\n\nNBA 농구팀별(Tm)로 가장 평균득점(PTS)이 높은 사람과 낮은 사람을 추출하는 코드를 작성하시오.\n또한, 팀별로 평균득점 최대값과 최소값의 차이를 gap이라는 컬럼을 새로 만들어 나타내시오.\n\n\nstats %>% \n  group_by(Tm) %>% \n  summarise(max_score = max(PTS),\n         min_score = min(PTS),\n         gap = max_score - min_score,\n         max_player = Player[which.max(PTS)],\n         min_player = Player[which.min(PTS)])\n\n# A tibble: 31 × 6\n   Tm    max_score min_score   gap max_player     min_player       \n   <chr>     <dbl>     <dbl> <dbl> <chr>          <chr>            \n 1 ATL        26.5       1.8  24.7 Trae Young     Bruno Fernando   \n 2 BOS        25.9       2.4  23.5 Jaylen Brown   Tremont Waters   \n 3 BRK        29         0    29   Kevin Durant   Noah Vonleh      \n 4 CHI        28.5       1.5  27   Zach LaVine    Luke Kornet      \n 5 CHO        22.3       1    21.3 Gordon Hayward Vernon Carey Jr. \n 6 CLE        22.8       1.2  21.6 Collin Sexton  Marques Bolden   \n 7 DAL        29.1       1    28.1 Luka Dončić    Tyrell Terry     \n 8 DEN        27.4       0.6  26.8 Nikola Jokić   Vlatko Čančar    \n 9 DET        23.8       0    23.8 Jerami Grant   Deividas Sirvydis\n10 GSW        30         1.5  28.5 Stephen Curry  Nico Mannion     \n# … with 21 more rows\n\n\n\n\n5.2 2. 각 포지션별로(Pos) 평균나이 대비 해당 선수의 나이가 몇배 높거나 낮은지 비율을 계산하시오\n\n예를들어 PG 포지션의 평균나이는 27세이고, Chris Paul의 나이는 35세이므로, 비율은 35/27이 된다.\n모든 선수에 대해 이 비율을 age_ratio_by_position이라는 새로운 컬럼으로 저장하는 코드를 작성하시오.\n단 , 포지션 별로 비율이 가장 높은 한명만 추출\n\n\nstats %>% \n  group_by(Pos) %>% \n  mutate(mean_age = mean(Age),\n         age_ratio_by_position = Age/mean_age) %>% \n  select(Player, Pos, Age, mean_age, age_ratio_by_position) %>% \n  arrange(-age_ratio_by_position) %>% \n  slice_max(age_ratio_by_position, n=1)\n\n# A tibble: 9 × 5\n# Groups:   Pos [9]\n  Player          Pos     Age mean_age age_ratio_by_position\n  <chr>           <chr> <dbl>    <dbl>                 <dbl>\n1 Marc Gasol      C        36     26.1                  1.38\n2 Noah Vonleh     F        25     24                    1.04\n3 Norvel Pelle    F-C      27     27                    1   \n4 Derrick Rose    G        32     27                    1.19\n5 Carmelo Anthony PF       36     25.9                  1.39\n6 LeBron James    PG       36     25.6                  1.41\n7 Andre Iguodala  SF       37     25.4                  1.46\n8 Rodions Kurucs  SF-PF    22     22                    1   \n9 J.J. Redick     SG       36     25.3                  1.43"
  },
  {
    "objectID": "posts/Opendata_Analysis Ch3/import & write.html",
    "href": "posts/Opendata_Analysis Ch3/import & write.html",
    "title": "Opendata_Analysis CH3",
    "section": "",
    "text": "HTML파일로 보기\nImport & Write"
  },
  {
    "objectID": "posts/Opendata_Analysis Ch3/import & write.html#오픈데이터-분석-실습-importwrite-data",
    "href": "posts/Opendata_Analysis Ch3/import & write.html#오픈데이터-분석-실습-importwrite-data",
    "title": "Opendata_Analysis CH3",
    "section": "1 오픈데이터 분석 실습 : Import/Write Data",
    "text": "1 오픈데이터 분석 실습 : Import/Write Data\n\n1.1 패키지 불러오기\n\nread_csv 기능 존재\n\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "posts/Opendata_Analysis Ch3/import & write.html#import-data",
    "href": "posts/Opendata_Analysis Ch3/import & write.html#import-data",
    "title": "Opendata_Analysis CH3",
    "section": "2 Import Data",
    "text": "2 Import Data\n\n2.1 read_csv\n\nread.csv와 다르게 문자열 factor 처리X\n\n\n### 현재 경로 확인\ngetwd()\n\n[1] \"G:/내 드라이브/taek_blog/posts/Opendata_Analysis Ch3\"\n\n### 경로 설정\n#setwd()\n\n### csv파일 불러오기\nheights <- read_csv(\"heights.csv\")\nheights\n\n# A tibble: 1,192 × 6\n    earn height sex       ed   age race    \n   <dbl>  <dbl> <chr>  <dbl> <dbl> <chr>   \n 1 50000   74.4 male      16    45 white   \n 2 60000   65.5 female    16    58 white   \n 3 30000   63.6 female    16    29 white   \n 4 50000   63.1 female    16    91 other   \n 5 51000   63.4 female    17    39 white   \n 6  9000   64.4 female    15    26 white   \n 7 29000   61.7 female    12    49 white   \n 8 32000   72.7 male      17    46 white   \n 9  2000   72.0 male      15    21 hispanic\n10 27000   72.2 male      12    26 white   \n# … with 1,182 more rows\n\n### 절대경로 사용\nheights <- read_csv(\"C:/Users/seong taek/Desktop/3-1 Opendata_Analysis/opendata/heights.csv\")\nheights\n\n# A tibble: 1,192 × 6\n    earn height sex       ed   age race    \n   <dbl>  <dbl> <chr>  <dbl> <dbl> <chr>   \n 1 50000   74.4 male      16    45 white   \n 2 60000   65.5 female    16    58 white   \n 3 30000   63.6 female    16    29 white   \n 4 50000   63.1 female    16    91 other   \n 5 51000   63.4 female    17    39 white   \n 6  9000   64.4 female    15    26 white   \n 7 29000   61.7 female    12    49 white   \n 8 32000   72.7 male      17    46 white   \n 9  2000   72.0 male      15    21 hispanic\n10 27000   72.2 male      12    26 white   \n# … with 1,182 more rows\n\n### tibble 데이터 프레임 생성\nread_csv(\"a,b,c\n         1,2,3\n         4,5,6\")\n\n# A tibble: 2 × 3\n      a     b     c\n  <dbl> <dbl> <dbl>\n1     1     2     3\n2     4     5     6\n\n### 라인 스킵\nread_csv(\"The first line of metadata\n         The second line of metadata\n         x,y,z\n         1,2,3\" , skip=2)\n\n# A tibble: 1 × 3\n      x     y     z\n  <dbl> <dbl> <dbl>\n1     1     2     3\n\n### 주석 스킵\nread_csv(\"#A comment I want to skip\n         x,y,z\n         1,2,3\", comment=\"#\")\n\n# A tibble: 1 × 3\n      x     y     z\n  <dbl> <dbl> <dbl>\n1     1     2     3\n\n### 컬럼 이름 없이 내용만\nread_csv(\"1,2,3\n         4,5,6\", col_names = F)\n\n# A tibble: 2 × 3\n     X1    X2    X3\n  <dbl> <dbl> <dbl>\n1     1     2     3\n2     4     5     6\n\n### '\\n' : 한줄 띄우기\nread_csv(\"1,2,3 \\n 4,5,6,\", col_names = F)\n\n# A tibble: 2 × 3\n     X1    X2    X3\n  <dbl> <dbl> <dbl>\n1     1     2     3\n2     4     5     6\n\n### 컬럼 이름 지정\nread_csv(\"1,2,3 \\n 4,5,6,\", col_names = c(\"A\",\"B\",\"C\"))\n\n# A tibble: 2 × 3\n      A     B     C\n  <dbl> <dbl> <dbl>\n1     1     2     3\n2     4     5     6\n\n### NA값 부여\nread_csv(\"a,b,c \\n 1,2,.\", na=\".\")\n\n# A tibble: 1 × 3\n      a     b c    \n  <dbl> <dbl> <lgl>\n1     1     2 NA   \n\n\n\n\n2.2 Locale 설정/확인\n\nSys.getlocale()\n\n[1] \"LC_COLLATE=Korean_Korea.utf8;LC_CTYPE=ko_KR.UTF-8;LC_MONETARY=Korean_Korea.utf8;LC_NUMERIC=C;LC_TIME=Korean_Korea.utf8\"\n\n### 언어 영어로\n#Sys.setlocale(\"LC_ALL\", \"English\")\n\n### 강제 언어 삭제\n#Sys.setlocale(\"LC_ALL\", \"C\")\n\n\n\n2.3 한글 파일 읽기\n\n### 인코딩 찾기\nguess_encoding(\"exercise.csv\")\n\n# A tibble: 2 × 2\n  encoding   confidence\n  <chr>           <dbl>\n1 EUC-KR           1   \n2 IBM420_ltr       0.25\n\n### 인코딩 입력으로 에러해결\nexercise <- read_csv(\"exercise.csv\", locale = locale(encoding = \"EUC-KR\"))\nexercise\n\n# A tibble: 5 × 2\n  이름  선호도\n  <chr>  <dbl>\n1 하민       5\n2 하준       4\n3 하진       4\n4 태산       3\n5 태민       2\n\n### csv파일을 미리 열어보고 인코딩 변경\nexercise <- read_csv(\"exercise_utf_8.csv\")\nexercise\n\n# A tibble: 5 × 2\n  이름  선호도\n  <chr>  <dbl>\n1 하민       5\n2 하준       4\n3 하진       4\n4 태산       3\n5 태민       2\n\nguess_encoding(\"exercise_utf_8.csv\")\n\n# A tibble: 3 × 2\n  encoding     confidence\n  <chr>             <dbl>\n1 UTF-8              1   \n2 windows-1255       0.38\n3 windows-1255       0.29"
  },
  {
    "objectID": "posts/Opendata_Analysis Ch3/import & write.html#write-data",
    "href": "posts/Opendata_Analysis Ch3/import & write.html#write-data",
    "title": "Opendata_Analysis CH3",
    "section": "3 Write Data",
    "text": "3 Write Data\n\n3.1 파일 저장/삭제\n\nheights <- read_csv(\"heights.csv\")\nheights\n\n# A tibble: 1,192 × 6\n    earn height sex       ed   age race    \n   <dbl>  <dbl> <chr>  <dbl> <dbl> <chr>   \n 1 50000   74.4 male      16    45 white   \n 2 60000   65.5 female    16    58 white   \n 3 30000   63.6 female    16    29 white   \n 4 50000   63.1 female    16    91 other   \n 5 51000   63.4 female    17    39 white   \n 6  9000   64.4 female    15    26 white   \n 7 29000   61.7 female    12    49 white   \n 8 32000   72.7 male      17    46 white   \n 9  2000   72.0 male      15    21 hispanic\n10 27000   72.2 male      12    26 white   \n# … with 1,182 more rows\n\n### 현재 경로에 csv파일 저장\nwrite_csv(heights, \"만들 파일 이름.csv\")\n\n### rds 확장자 \nwrite_rds(heights, \"만들 파일 이름.rds\")\nread_rds(\"만들 파일 이름.rds\")\n\n# A tibble: 1,192 × 6\n    earn height sex       ed   age race    \n   <dbl>  <dbl> <chr>  <dbl> <dbl> <chr>   \n 1 50000   74.4 male      16    45 white   \n 2 60000   65.5 female    16    58 white   \n 3 30000   63.6 female    16    29 white   \n 4 50000   63.1 female    16    91 other   \n 5 51000   63.4 female    17    39 white   \n 6  9000   64.4 female    15    26 white   \n 7 29000   61.7 female    12    49 white   \n 8 32000   72.7 male      17    46 white   \n 9  2000   72.0 male      15    21 hispanic\n10 27000   72.2 male      12    26 white   \n# … with 1,182 more rows\n\n### 파일 삭제\nfile.remove(\"만들 파일 이름.csv\")\n\n[1] TRUE\n\n\n\n\n3.2 feather 패키지\n\n#install.packages(\"feather\")\nlibrary(feather)\n\nwrite_feather(heights, \"heights.feather\")\nread_feather(\"heights.feather\")\n\n# A tibble: 1,192 × 6\n    earn height sex       ed   age race    \n   <dbl>  <dbl> <chr>  <dbl> <dbl> <chr>   \n 1 50000   74.4 male      16    45 white   \n 2 60000   65.5 female    16    58 white   \n 3 30000   63.6 female    16    29 white   \n 4 50000   63.1 female    16    91 other   \n 5 51000   63.4 female    17    39 white   \n 6  9000   64.4 female    15    26 white   \n 7 29000   61.7 female    12    49 white   \n 8 32000   72.7 male      17    46 white   \n 9  2000   72.0 male      15    21 hispanic\n10 27000   72.2 male      12    26 white   \n# … with 1,182 more rows"
  },
  {
    "objectID": "posts/Opendata_Analysis Ch4/data_tyding.html",
    "href": "posts/Opendata_Analysis Ch4/data_tyding.html",
    "title": "Opendata_Analysis CH4",
    "section": "",
    "text": "HTML파일로 보기\nData Tyding"
  },
  {
    "objectID": "posts/Opendata_Analysis Ch4/data_tyding.html#오픈데이터-분석-실습-data-tyding",
    "href": "posts/Opendata_Analysis Ch4/data_tyding.html#오픈데이터-분석-실습-data-tyding",
    "title": "Opendata_Analysis CH4",
    "section": "1 오픈데이터 분석 실습 : Data Tyding",
    "text": "1 오픈데이터 분석 실습 : Data Tyding\n\n1.1 패키지 불러오기\n\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n\n\n1.2 Tidy data\n\n원칙\n\n각 변수는 각각의 열을 가져야한다\n각 변수는 각각의 행을 가져야한다\n각 셀은 하나의 값을 가져야한다\n\n\n\n### 확진자수 데이터셋\ntable1\n\n# A tibble: 6 × 4\n  country      year  cases population\n  <chr>       <dbl>  <dbl>      <dbl>\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n### rate 컬럼 생성\ntable1 %>% \n  mutate(rate = cases/population*10000)\n\n# A tibble: 6 × 5\n  country      year  cases population  rate\n  <chr>       <dbl>  <dbl>      <dbl> <dbl>\n1 Afghanistan  1999    745   19987071 0.373\n2 Afghanistan  2000   2666   20595360 1.29 \n3 Brazil       1999  37737  172006362 2.19 \n4 Brazil       2000  80488  174504898 4.61 \n5 China        1999 212258 1272915272 1.67 \n6 China        2000 213766 1280428583 1.67 \n\n### 연도별 cases(확진자) 수\ntable1 %>% \n  count(year, wt=cases) # wt=weight=가중치\n\n# A tibble: 2 × 2\n   year      n\n  <dbl>  <dbl>\n1  1999 250740\n2  2000 296920\n\n### 국가별 확진자 수 추이\nggplot(table1, aes(x=year, y=cases)) +\n  geom_line(aes(group = country), colour = \"grey50\") + # 국가별 선 구분, 색상 통일\n  geom_point(aes(colour = country, shape = country)) + # 국가별 점 색상/모양 구분\n  scale_x_continuous(breaks = c(1999,2000)) +          # x축 눈금 지정\n  facet_wrap(vars(country), scales = \"free\")           # 국가별 패널 구분, 눈금-독립조정\n\n\n\n### 2000년도 국가별 ratio \ndata<- table1 %>% \n  group_by(country) %>% \n  mutate(ratio = cases/min(cases)) %>%  # ratio = 확진자 수 최소 정규화\n  filter(year==2000)\n\ndata\n\n# A tibble: 3 × 5\n# Groups:   country [3]\n  country      year  cases population ratio\n  <chr>       <dbl>  <dbl>      <dbl> <dbl>\n1 Afghanistan  2000   2666   20595360  3.58\n2 Brazil       2000  80488  174504898  2.13\n3 China        2000 213766 1280428583  1.01\n\n### 국가별 2000년도 확진자 수 최소 정규화\nggplot(data, aes(x = country, y = ratio, fill = country)) + # 색상:국가별\n  geom_bar(stat = \"identity\", width = 0.5) +                # 막대너비 지정\n  labs(title = \"Ratio by Country\",\n       x = \"Country\", y = \"Ratio\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")                           # 범례 미지정\n\n\n\n### 1999 - 2000 증가량 계산\ndata <- table1\n\ndata<- table1 %>% \n  group_by(country) %>%  \n  mutate(increase = cases - cases[year == 1999])  \n\ndata\n\n# A tibble: 6 × 5\n# Groups:   country [3]\n  country      year  cases population increase\n  <chr>       <dbl>  <dbl>      <dbl>    <dbl>\n1 Afghanistan  1999    745   19987071        0\n2 Afghanistan  2000   2666   20595360     1921\n3 Brazil       1999  37737  172006362        0\n4 Brazil       2000  80488  174504898    42751\n5 China        1999 212258 1272915272        0\n6 China        2000 213766 1280428583     1508\n\n### 국가별 확진자 수 증가량 추이\nggplot(data %>% filter(year==2000), \n       aes(x = country, y = increase, fill = country)) + \n  geom_bar(stat = \"identity\", width = 0.5) +\n  labs(title = \"Cases Increase from 1999 to 2000 by Country\",\n       x = \"Country\", y = \"Cases Increase\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/Opendata_Analysis Ch4/data_tyding.html#pivoting",
    "href": "posts/Opendata_Analysis Ch4/data_tyding.html#pivoting",
    "title": "Opendata_Analysis CH4",
    "section": "2 Pivoting",
    "text": "2 Pivoting\n\n2.1 pivot_longer\n\n데이터를 긴 형태에서 넓은 형태로 변환\n여러 컬럼을 하나의 컬럼으로 정리\n주로 정규화 작업을 수행\n\n\n### 국가/연도별 확진자 수\ntable4a\n\n# A tibble: 3 × 3\n  country     `1999` `2000`\n  <chr>        <dbl>  <dbl>\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\n### pivot_longer\ntable4a_pivot_longer <- table4a %>%\n  pivot_longer(\n    cols = c(`1999`, `2000`),        # 변환할 컬럼 지정\n    names_to = \"year\",               # 변환될 컬럼 이름 지정\n    values_to = \"cases\"              # 변환된 값들이 저장될 컬럼 이름 지정\n    ) %>%\n  mutate(year = parse_integer(year)) # 정수형 타입 변환\n\ntable4a_pivot_longer\n\n# A tibble: 6 × 3\n  country      year  cases\n  <chr>       <int>  <dbl>\n1 Afghanistan  1999    745\n2 Afghanistan  2000   2666\n3 Brazil       1999  37737\n4 Brazil       2000  80488\n5 China        1999 212258\n6 China        2000 213766\n\n### ggplot 그래프\ntable4a_pivot_longer %>%\n  ggplot(aes(x = year, y = cases)) +\n  geom_line(aes(group = country), colour = \"grey50\") +\n  geom_point(aes(colour = country, shape = country)) +\n  scale_x_continuous(breaks = c(1999, 2000))\n\n\n\n\n\n\n2.2 pivot_longer + left_join\n\n### 국가/연도별 인구수\ntable4b\n\n# A tibble: 3 × 3\n  country         `1999`     `2000`\n  <chr>            <dbl>      <dbl>\n1 Afghanistan   19987071   20595360\n2 Brazil       172006362  174504898\n3 China       1272915272 1280428583\n\n### pivot_longer\ntable4b_pivot_longer <- table4b %>%\n  pivot_longer(\n    cols = c(`1999`, `2000`),\n    names_to = \"year\",\n    values_to = \"population\"\n  ) %>%\n  mutate(year = parse_integer(year))\n\ntable4b_pivot_longer\n\n# A tibble: 6 × 3\n  country      year population\n  <chr>       <int>      <dbl>\n1 Afghanistan  1999   19987071\n2 Afghanistan  2000   20595360\n3 Brazil       1999  172006362\n4 Brazil       2000  174504898\n5 China        1999 1272915272\n6 China        2000 1280428583\n\n### 확진자 수 + 인구수 left_join\ntable4a_pivot_longer %>% left_join(table4b_pivot_longer, by = c(\"country\", \"year\"))\n\n# A tibble: 6 × 4\n  country      year  cases population\n  <chr>       <int>  <dbl>      <dbl>\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\n\n2.3 pivot_wider\n\n데이터를 넓은 형태에서 긴 형태로 변환\n하나의 컬럼을 여러개 컬럼으로 확장\n주로 비정규화 작업을 수행\n\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   <chr>       <dbl> <chr>           <dbl>\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\n### pivot_wider\ntable2_pivot_wider <- table2 %>%\n  pivot_wider(names_from = type,       # 여러 컬럼으로 확장할 컬럼 지정\n              values_from = count) %>% # 확장할 컬럼에 해당하는 값\n  mutate(rate = cases / population)\n\ntable2_pivot_wider\n\n# A tibble: 6 × 5\n  country      year  cases population      rate\n  <chr>       <dbl>  <dbl>      <dbl>     <dbl>\n1 Afghanistan  1999    745   19987071 0.0000373\n2 Afghanistan  2000   2666   20595360 0.000129 \n3 Brazil       1999  37737  172006362 0.000219 \n4 Brazil       2000  80488  174504898 0.000461 \n5 China        1999 212258 1272915272 0.000167 \n6 China        2000 213766 1280428583 0.000167 \n\n### ggplot 그래프\ntable2_pivot_wider %>% ggplot(aes(x = year, y = rate)) +\n  geom_line(aes(group = country), colour = \"grey50\") +\n  geom_point(aes(colour = country, shape = country)) +\n  scale_x_continuous(breaks = c(1999, 2000))"
  },
  {
    "objectID": "posts/Opendata_Analysis Ch4/data_tyding.html#advanced-pivoting-missing-values",
    "href": "posts/Opendata_Analysis Ch4/data_tyding.html#advanced-pivoting-missing-values",
    "title": "Opendata_Analysis CH4",
    "section": "3 Advanced Pivoting (Missing values)",
    "text": "3 Advanced Pivoting (Missing values)\n\n3.1 NA값이 있을 때의 pivot_longer\n\n#remotes::install_github(\"dcl-docs/dcldata\")\nlibrary(dcldata)\n\nexample_migration\n\n# A tibble: 3 × 6\n  dest     Afghanistan Canada India Japan `South Africa`\n  <chr>    <chr>       <chr>  <chr> <chr> <chr>         \n1 Albania  <NA>        913    <NA>  <NA>  <NA>          \n2 Bulgaria 483         713    281   213   260           \n3 Romania  <NA>        <NA>   102   <NA>  <NA>          \n\n### drop_na로 제거\nexample_migration %>% \n  pivot_longer(cols = !dest, \n               names_to = \"origin\", \n               values_to = \"migrants\") %>% \n  drop_na(migrants)\n\n# A tibble: 7 × 3\n  dest     origin       migrants\n  <chr>    <chr>        <chr>   \n1 Albania  Canada       913     \n2 Bulgaria Afghanistan  483     \n3 Bulgaria Canada       713     \n4 Bulgaria India        281     \n5 Bulgaria Japan        213     \n6 Bulgaria South Africa 260     \n7 Romania  India        102"
  },
  {
    "objectID": "posts/Opendata_Analysis Ch5/data visualize.html",
    "href": "posts/Opendata_Analysis Ch5/data visualize.html",
    "title": "Opendata_Analysis CH5",
    "section": "",
    "text": "HTML파일로 보기\nData Visualization"
  },
  {
    "objectID": "posts/Opendata_Analysis Ch5/data visualize.html#오픈데이터-분석-실습-data-visualization",
    "href": "posts/Opendata_Analysis Ch5/data visualize.html#오픈데이터-분석-실습-data-visualization",
    "title": "Opendata_Analysis CH5",
    "section": "1 오픈데이터 분석 실습 : Data Visualization",
    "text": "1 오픈데이터 분석 실습 : Data Visualization\n\n1.1 패키지 불러오기\n\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n\n\n1.2 데이터 불러오기\n\nmpg (자동차 정보 데이터셋)\n\n\nmpg %>% head()\n\n# A tibble: 6 × 11\n  manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class \n  <chr>        <chr> <dbl> <int> <int> <chr>      <chr> <int> <int> <chr> <chr> \n1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa…\n2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa…\n3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa…\n4 audi         a4      2    2008     4 auto(av)   f        21    30 p     compa…\n5 audi         a4      2.8  1999     6 auto(l5)   f        16    26 p     compa…\n6 audi         a4      2.8  1999     6 manual(m5) f        18    26 p     compa…\n\n### 차원 파악\nmpg %>% dim()\n\n[1] 234  11\n\n\n\n\n1.3 가정\n\n엔진이 큰 차는 엔진이 작은 차보다 연료를 더 많이 사용하는가\n\ndispl : 엔진의 크기\nhwy : 연비\n\n\n\n\n1.4 ggplot 생성\n\ngeom_point : 점 찍기\n\nx축 : 엔진 크기\ny축 : 연비\n\n\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy))\n\n\n\n\n\n### 더 간단한 코드\nggplot(mpg) + \n  geom_point(aes(displ,hwy))"
  },
  {
    "objectID": "posts/Opendata_Analysis Ch5/data visualize.html#심미적-mapping",
    "href": "posts/Opendata_Analysis Ch5/data visualize.html#심미적-mapping",
    "title": "Opendata_Analysis CH5",
    "section": "2 심미적 mapping",
    "text": "2 심미적 mapping\n\n2.1 포인트 색상 지정\n\n### class별 색상 지정\ntable(mpg$class)\n\n\n   2seater    compact    midsize    minivan     pickup subcompact        suv \n         5         47         41         11         33         35         62 \n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, color = class)) # col='blue' (통일된 색상)\n\n\n\n\n\n\n2.2 포인트 사이즈 지정\n\n단점 : 구별 어려움\n\n\n### class 별 크기 지정\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, size = class))\n\nWarning: Using size for a discrete variable is not advised.\n\n\n\n\n\n\n\n2.3 포인트 투명성 제어\n\n단점 : 구별 어려움\n\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, alpha = class))\n\nWarning: Using alpha for a discrete variable is not advised.\n\n\n\n\n\n\n\n2.4 포인트 모양 변경\n\n단점 : 최대 6가지 모양만 보임\n\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, shape = class))\n\nWarning: The shape palette can deal with a maximum of 6 discrete values because\nmore than 6 becomes difficult to discriminate; you have 7. Consider\nspecifying shapes manually if you must have them.\n\n\nWarning: Removed 62 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "posts/Opendata_Analysis Ch5/data visualize.html#면",
    "href": "posts/Opendata_Analysis Ch5/data visualize.html#면",
    "title": "Opendata_Analysis CH5",
    "section": "3 면",
    "text": "3 면\n\n3.1 facet_wrap()\n\n패널plot 생성\n\n\n### class별 2행의 패널\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  facet_wrap(~ class, nrow = 2)\n\n\n\n\n\n\n3.2 facet_grid()\n\n두 개 이상의 변수에 대해 그리드형태의 패널plot 생성\n\n\ntable(mpg$drv)\n\n\n  4   f   r \n103 106  25 \n\ntable(mpg$cyl)\n\n\n 4  5  6  8 \n81  4 79 70 \n\nggplot(mpg) + \n  geom_point(aes(x = displ, y = hwy)) + \n  facet_grid(drv ~ cyl) # facet 하지 않으려면 변수 대신 마침표 사용가능"
  },
  {
    "objectID": "posts/Opendata_Analysis Ch5/data visualize.html#기하학적-객체",
    "href": "posts/Opendata_Analysis Ch5/data visualize.html#기하학적-객체",
    "title": "Opendata_Analysis CH5",
    "section": "4 기하학적 객체",
    "text": "4 기하학적 객체\n\n4.1 geom_smooth\n\n스무스한 선\n\n\n### linetype = drv : drv에 따른 라인 구분\nggplot(mpg) + \n  geom_smooth(aes(x = displ, y = hwy,linetype = drv))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n### 점, 컬러 추가\nggplot(mpg,aes(x=displ,\n               y=hwy,\n               color=drv)) +\n  geom_point() +\n  geom_smooth(aes(linetype=drv))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n### 그룹화만 했을 경우\nggplot(mpg) +\n  geom_smooth(aes(x=displ, y=hwy, group = drv))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n### 선 색상으로 구분 + 범례 미표기\nggplot(mpg) +\n  geom_smooth(aes(x=displ, y=hwy, color = drv),\n              show.legend = F)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n### 선/색상/점 모두 구분\nggplot(mpg, aes(x=displ, y=hwy)) +\n  geom_point(aes(color= class)) +\n  geom_smooth(aes(linetype = class))\n\n\n\n\n\n### 특정 데이터 필터링\n### se=F : 회귀선의 표준오차 미표시\nggplot(mpg, aes(x=displ, y=hwy)) +\n  geom_point(aes(color= class)) +\n  geom_smooth(data = mpg %>% filter(class == 'subcompact'),se=F) \n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'"
  },
  {
    "objectID": "posts/Opendata_Analysis Ch5/data visualize.html#통계적-변환",
    "href": "posts/Opendata_Analysis Ch5/data visualize.html#통계적-변환",
    "title": "Opendata_Analysis CH5",
    "section": "5 통계적 변환",
    "text": "5 통계적 변환\n\n5.1 데이터셋 파악\n\ndiamonds\n\n\ndiamonds\n\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# … with 53,930 more rows\n\n### 특정 컬럼 빈도수\ndiamonds$cut %>% table()\n\n.\n     Fair      Good Very Good   Premium     Ideal \n     1610      4906     12082     13791     21551 \n\n\n\n\n5.2 데이터 시각화\n\n### 특정 컬럼 빈도수 시각화\nggplot(diamonds) +\n  geom_bar(aes(x=cut)) # stat_count와 동일\n\n\n\n\n\n\n5.3 세부 조정\n\nposition\n\n\n### 막대그래프 색상 조정 color= cut\nggplot(diamonds) +\n  geom_bar(aes(x=cut, color= cut))\n\n\n\n\n\n### 막대그래프 색상 조정 fill= cut\nggplot(diamonds) +\n  geom_bar(aes(x=cut, fill= cut))\n\n\n\n\n\n### 막대그래프 색상 조정 fill= clarity\n### x축에 대해서 색상 조정 가능\nggplot(diamonds) +\n  geom_bar(aes(x=cut, fill= clarity))\n\n\n\n\n\n### 투명도 + identity: 막대 안겹치게 포지셔닝\nggplot(diamonds, aes(x=cut, fill=clarity)) +\n  geom_bar(alpha=1/5, position = \"identity\")\n\n\n\n\n\n### 채움색상 NA(투명) \nggplot(diamonds, aes(x=cut, color=clarity)) +\n  geom_bar(fill=NA, position = \"identity\")\n\n\n\n\n\n### fill : 전체높이에 대한 비율 표현 → 상대적 비교 가능\nggplot(diamonds) +\n  geom_bar(aes(x=cut, fill=clarity), position = \"fill\")\n\n\n\n\n\n### dodge : 비교선상의 막대 나란히 배열\nggplot(diamonds) +\n  geom_bar(aes(x=cut, fill=clarity), position = \"dodge\")\n\n\n\n\n\n\n5.4 Quiz 1\n다이아몬드 cut이 Ideal인 다이아몬드 중에서 Clarity가 IF인 비율은 얼마인가?\n\n### 그래프\nggplot(data = diamonds %>% filter(cut == 'Ideal')) +\n  geom_bar(aes(x=cut, fill=clarity), position = \"dodge\")\n\n\n\n### 다이아몬드 cut이 Ideal인 다이아몬드 중애서 Clarity가 IF인 비율\n(diamonds %>% filter(cut == 'Ideal', clarity==\"IF\") %>% nrow)/(diamonds %>% filter(cut==\"Ideal\") %>% \n                                                                nrow)\n\n[1] 0.05623869\n\n\n\n\n5.5 Quiz 2\n다이아몬드 cut 등급별로, clarity의 비율이 나오도록 데이터프레임을 가공해봅시다\n\ndiamonds %>%\n  group_by(cut, clarity) %>%\n  summarize(n=n()) %>%\n  mutate(total = sum(n), prop = n/total) %>%\n  select(-n) %>%\n  pivot_wider(names_from = clarity, values_from = prop)\n\n`summarise()` has grouped output by 'cut'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 5 × 10\n# Groups:   cut [5]\n  cut       total      I1   SI2   SI1   VS2   VS1   VVS2   VVS1      IF\n  <ord>     <int>   <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl>  <dbl>   <dbl>\n1 Fair       1610 0.130   0.289 0.253 0.162 0.106 0.0429 0.0106 0.00559\n2 Good       4906 0.0196  0.220 0.318 0.199 0.132 0.0583 0.0379 0.0145 \n3 Very Good 12082 0.00695 0.174 0.268 0.214 0.147 0.102  0.0653 0.0222 \n4 Premium   13791 0.0149  0.214 0.259 0.243 0.144 0.0631 0.0447 0.0167 \n5 Ideal     21551 0.00677 0.121 0.199 0.235 0.167 0.121  0.0950 0.0562 \n\n\n\n\n5.6 jitter\n\n### jitter : 점 뿌리기\nggplot(mpg) +\n  geom_point(aes(x=displ, y=hwy), position = 'jitter')\n\n\n\nggplot(mpg) +\n  geom_jitter(aes(x=displ, y= hwy))\n\n\n\n\n\n\n5.7 세부 조정2\n\ncoord\n\n\n### boxplot\nggplot(mpg) +\n  geom_boxplot(aes(x=class, y=hwy))\n\n\n\n### 색상 지정 가능\nggplot(mpg) +\n  geom_boxplot(aes(x=class,y=hwy, fill = class))\n\n\n\n\n\n### coord_flip : x축, y축 변경\nggplot(mpg) +\n  geom_boxplot(aes(x=class, y=hwy)) +\n  coord_flip()\n\n\n\n\n\n#install.packages(\"maps\")\nlibrary(maps)\n\n\nAttaching package: 'maps'\n\n\nThe following object is masked from 'package:purrr':\n\n    map\n\n### maps 패키지의 데이터셋 저장/생성\nnz <- map_data(\"nz\")\nnz %>% head()\n\n      long       lat group order        region subregion\n1 172.7433 -34.44215     1     1 North.Island       <NA>\n2 172.7983 -34.45562     1     2 North.Island       <NA>\n3 172.8528 -34.44846     1     3 North.Island       <NA>\n4 172.8986 -34.41786     1     4 North.Island       <NA>\n5 172.9593 -34.42503     1     5 North.Island       <NA>\n6 173.0184 -34.39895     1     6 North.Island       <NA>\n\n### geom_polygon : 지도 형식 시각화\nggplot(nz, aes(x=long, y=lat, group=group)) +\n  geom_polygon(fill=\"white\", color=\"black\")\n\n\n\n\n\n### coord_quickmap : 가로 세율 비율 보존 (정확한 지도)\nggplot(nz, aes(x=long, y=lat, group=group)) +\n  geom_polygon(fill=\"white\", color=\"black\") +\n  coord_quickmap()\n\n\n\n\n\nbar <- ggplot(diamonds) +\n  geom_bar(aes(x=cut, fill=cut),\n           show.legend = F,\n           width = 1) +\n  theme(aspect.ratio = 1) + # 그래프 가로:세로 비율\n  labs(x=NULL, y=NULL)\n\nbar\n\n\n\n\n\n### coord_polar : 원형 막대 그래프\nbar + coord_polar()"
  },
  {
    "objectID": "posts/Opendata_Analysis Ch5/data visualize.html#quiz",
    "href": "posts/Opendata_Analysis Ch5/data visualize.html#quiz",
    "title": "Opendata_Analysis CH5",
    "section": "6 Quiz",
    "text": "6 Quiz\n\ngeoom_errorbar\n\n\n### 차종에 따라서 고속도로에서 연비차이가 날까?\n\n### errorbar로 연비 범위(평균-표준오차, 평균+표준오차) 구하기\nmpg %>% \n  group_by(class) %>% \n  summarise(mean_hwy = mean(hwy), sd_hwy = sd(hwy)) %>% \n  ggplot(aes(x=class, y=mean_hwy, fill=class)) +\n  geom_bar(stat = \"identity\") +\n  geom_errorbar(aes(ymin = mean_hwy - sd_hwy, ymax = mean_hwy + sd_hwy),\n                width = 0.5,\n                position = position_dodge(width = 0.9),\n                color = \"black\")"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html",
    "title": "Data_Mining CH1",
    "section": "",
    "text": "Numpy Basic"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#numpy-기본",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#numpy-기본",
    "title": "Data_Mining CH1",
    "section": "1 Numpy 기본",
    "text": "1 Numpy 기본\n도구 - 넘파이(NumPy)\n\n넘파이(NumPy)는 파이썬의 과학 컴퓨팅을 위한 기본 라이브러리입니다.\n넘파이의 핵심은 강력한 N-차원 배열 객체입니다.\n선형 대수, 푸리에(Fourier) 변환, 유사 난수 생성과 같은 유용한 함수들도 제공합니다.\n\n\n\n\nJupyter에서 실행하기\n\n\n\n\n\n구글 코랩에서 실행하기"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#배열-생성",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#배열-생성",
    "title": "Data_Mining CH1",
    "section": "2 배열 생성",
    "text": "2 배열 생성\nnumpy를 임포트해 보죠. 대부분의 사람들이 np로 알리아싱하여 임포트합니다:\nimport numpy as np\n\n2.1 np.zeros\nzeros 함수는 0으로 채워진 배열을 만듭니다:\nnp.zeros(5)\narray([0., 0., 0., 0., 0.])\n2D 배열(즉, 행렬)을 만들려면 원하는 행과 열의 크기를 튜플로 전달합니다. 예를 들어 다음은 \\(3 \\times 4\\) 크기의 행렬입니다:\nnp.zeros((3,4))\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#용어",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#용어",
    "title": "Data_Mining CH1",
    "section": "3 용어",
    "text": "3 용어\n\n넘파이에서 각 차원을 축(axis) 이라고 합니다\n축의 개수를 랭크(rank) 라고 합니다.\n\n예를 들어, 위의 \\(3 \\times 4\\) 행렬은 랭크 2인 배열입니다(즉 2차원입니다).\n첫 번째 축의 길이는 3이고 두 번째 축의 길이는 4입니다.\n\n배열의 축 길이를 배열의 크기(shape)라고 합니다.\n\n예를 들어, 위 행렬의 크기는 (3, 4)입니다.\n랭크는 크기의 길이와 같습니다.\n\n배열의 사이즈(size)는 전체 원소의 개수입니다. 축의 길이를 모두 곱해서 구할 수 있습니다(가령, \\(3 \\times 4=12\\)).\n\na = np.zeros((3,4))\na\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])\na.shape\n(3, 4)\na.ndim  # len(a.shape)와 같습니다\n2\na.size\n12"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#n-차원-배열",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#n-차원-배열",
    "title": "Data_Mining CH1",
    "section": "4 N-차원 배열",
    "text": "4 N-차원 배열\n임의의 랭크 수를 가진 N-차원 배열을 만들 수 있습니다. 예를 들어, 다음은 크기가 (2,3,4)인 3D 배열(랭크=3)입니다:\nnp.zeros((2,3,4))\narray([[[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]],\n\n       [[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#배열-타입",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#배열-타입",
    "title": "Data_Mining CH1",
    "section": "5 배열 타입",
    "text": "5 배열 타입\n넘파이 배열의 타입은 ndarray입니다:\ntype(np.zeros((3,4)))\nnumpy.ndarray\n\n5.1 np.ones\nndarray를 만들 수 있는 넘파이 함수가 많습니다.\n다음은 1로 채워진 \\(3 \\times 4\\) 크기의 행렬입니다:\nnp.ones((3,4))\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.]])\n\n\n5.2 np.full\n주어진 값으로 지정된 크기의 배열을 초기화합니다. 다음은 π로 채워진 \\(3 \\times 4\\) 크기의 행렬입니다.\nnp.full((3,4), np.pi)\narray([[3.14159265, 3.14159265, 3.14159265, 3.14159265],\n       [3.14159265, 3.14159265, 3.14159265, 3.14159265],\n       [3.14159265, 3.14159265, 3.14159265, 3.14159265]])\n\n\n5.3 np.empty\n초기화되지 않은 \\(2 \\times 3\\) 크기의 배열을 만듭니다(배열의 내용은 예측이 불가능하며 메모리 상황에 따라 달라집니다):\nnp.empty((2,3))\narray([[0., 0., 0.],\n       [0., 0., 0.]])\n\n\n5.4 np.array\narray 함수는 파이썬 리스트를 사용하여 ndarray를 초기화합니다:\nnp.array([[1,2,3,4], [10, 20, 30, 40],[3,4,5,6]])\narray([[ 1,  2,  3,  4],\n       [10, 20, 30, 40],\n       [ 3,  4,  5,  6]])\n\n\n5.5 np.arange\n파이썬의 기본 range 함수와 비슷한 넘파이 arange 함수를 사용하여 ndarray를 만들 수 있습니다:\nnp.arange(1, 5)\narray([1, 2, 3, 4])\n부동 소수도 가능합니다:\nnp.arange(1.0, 5.0)\narray([1., 2., 3., 4.])\n파이썬의 기본 range 함수처럼 건너 뛰는 정도를 지정할 수 있습니다:\nnp.arange(1, 5, 0.5)\narray([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5])\n부동 소수를 사용하면 원소의 개수가 일정하지 않을 수 있습니다. 예를 들면 다음과 같습니다:\nprint(np.arange(0, 5/3, 1/3)) # 부동 소수 오차 때문에, 최댓값은 4/3 또는 5/3이 됩니다.\nprint(np.arange(0, 5/3, 0.333333333))\nprint(np.arange(0, 5/3, 0.333333334))\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]\n[0.         0.33333333 0.66666667 1.         1.33333334]\n\n\n5.6 np.linspace\n이런 이유로 부동 소수를 사용할 땐 arange 대신에 linspace 함수를 사용하는 것이 좋습니다. linspace 함수는 지정된 개수만큼 두 값 사이를 나눈 배열을 반환합니다(arange와는 다르게 최댓값이 포함됩니다):\nprint(np.linspace(0, 5/3, 6)) #n=6\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]\n\n\n5.7 np.rand & np.randn\n넘파이의 random 모듈에는 ndarray를 랜덤한 값으로 초기화할 수 있는 함수들이 많이 있습니다. 예를 들어, 다음은 (균등 분포인) 0과 1사이의 랜덤한 부동 소수로 \\(3 \\times 4\\) 행렬을 초기화합니다:\nnp.random.rand(3,4)\narray([[0.10446653, 0.08608099, 0.39113434, 0.51368828],\n       [0.5422316 , 0.60730771, 0.14296635, 0.65012684],\n       [0.32191595, 0.67644108, 0.97250068, 0.02085189]])\n다음은 평균이 0이고 분산이 1인 일변량 정규 분포(가우시안 분포)에서 샘플링한 랜덤한 부동 소수를 담은 \\(3 \\times 4\\) 행렬입니다:\nnp.random.randn(3,4)\narray([[-8.45928911e-01, -2.54018182e-04,  1.58171732e-01,\n        -1.56553088e-01],\n       [ 1.26984307e+00,  1.06076160e-01,  1.79499825e+00,\n        -1.34570830e-01],\n       [ 1.47976512e+00,  3.92371306e-01,  1.82902549e+00,\n         1.14404415e+00]])\n이 분포의 모양을 알려면 맷플롯립을 사용해 그려보는 것이 좋습니다(더 자세한 것은 맷플롯립 튜토리얼을 참고하세요):\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.hist(np.random.rand(100000), density=True, bins=100, histtype=\"step\", color=\"blue\", label=\"rand\")\nplt.hist(np.random.randn(100000), density=True, bins=100, histtype=\"step\", color=\"red\", label=\"randn\")\nplt.axis([-2.5, 2.5, 0, 1.1])\nplt.legend(loc = \"upper left\")\nplt.title(\"Random distributions\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Density\")\nplt.show()\n\n\n\npng\n\n\n\n\n5.8 np.fromfunction\n함수를 사용하여 ndarray를 초기화할 수도 있습니다:\ndef my_function(z, y, x):\n    return x + 10 * y + 100 * z\n\nnp.fromfunction(my_function, (3, 2, 10))\narray([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.],\n        [ 10.,  11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.]],\n\n       [[100., 101., 102., 103., 104., 105., 106., 107., 108., 109.],\n        [110., 111., 112., 113., 114., 115., 116., 117., 118., 119.]],\n\n       [[200., 201., 202., 203., 204., 205., 206., 207., 208., 209.],\n        [210., 211., 212., 213., 214., 215., 216., 217., 218., 219.]]])\n넘파이는 먼저 크기가 (3, 2, 10)인 세 개의 ndarray(차원마다 하나씩)를 만듭니다. 각 배열은 축을 따라 좌표 값과 같은 값을 가집니다. 예를 들어, z 축에 있는 배열의 모든 원소는 z-축의 값과 같습니다:\n[[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n\n [[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]]\n\n [[ 2.  2.  2.  2.  2.  2.  2.  2.  2.  2.]\n  [ 2.  2.  2.  2.  2.  2.  2.  2.  2.  2.]]]\n위의 식 x + 10 * y + 100 * z에서 x, y, z는 사실 ndarray입니다(배열의 산술 연산에 대해서는 아래에서 설명합니다). 중요한 점은 함수 my_function이 원소마다 호출되는 것이 아니고 딱 한 번 호출된다는 점입니다. 그래서 매우 효율적으로 초기화할 수 있습니다."
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#배열-데이터",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#배열-데이터",
    "title": "Data_Mining CH1",
    "section": "6 배열 데이터",
    "text": "6 배열 데이터\n\n6.1 dtype\n넘파이의 ndarray는 모든 원소가 동일한 타입(보통 숫자)을 가지기 때문에 효율적입니다. dtype 속성으로 쉽게 데이터 타입을 확인할 수 있습니다:\nc = np.arange(1, 5)\nprint(c.dtype, c)\nint32 [1 2 3 4]\nc = np.arange(1.0, 5.0)\nprint(c.dtype, c)\nfloat64 [1. 2. 3. 4.]\n넘파이가 데이터 타입을 결정하도록 내버려 두는 대신 dtype 매개변수를 사용해서 배열을 만들 때 명시적으로 지정할 수 있습니다:\nd = np.arange(1, 5, dtype=np.complex64)\nprint(d.dtype, d)\ncomplex64 [1.+0.j 2.+0.j 3.+0.j 4.+0.j]\n가능한 데이터 타입은 int8, int16, int32, int64, uint8|16|32|64, float16|32|64, complex64|128가 있습니다. 전체 리스트는 온라인 문서를 참고하세요.\n\n\n6.2 itemsize\nitemsize 속성은 각 아이템의 크기(바이트)를 반환합니다:\ne = np.arange(1, 5, dtype=np.complex64)\nprint(e)\nprint(e.itemsize)\n[1.+0.j 2.+0.j 3.+0.j 4.+0.j]\n8\n\n\n6.3 data 버퍼\n배열의 데이터는 1차원 바이트 버퍼로 메모리에 저장됩니다. data 속성을 사용해 참조할 수 있습니다(사용할 일은 거의 없겠지만요).\nf = np.array([[1,2],[1000, 2000]], dtype=np.int32)\nprint(f)\nprint(f.data)\n[[   1    2]\n [1000 2000]]\n<memory at 0x000002664FD6BEE0>\n파이썬 2에서는 f.data가 버퍼이고 파이썬 3에서는 memoryview입니다.\nif (hasattr(f.data, \"tobytes\")):\n    data_bytes = f.data.tobytes() # python 3\nelse:\n    data_bytes = memoryview(f.data).tobytes() # python 2\n\ndata_bytes\nb'\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\xe8\\x03\\x00\\x00\\xd0\\x07\\x00\\x00'\n여러 개의 ndarray가 데이터 버퍼를 공유할 수 있습니다. 하나를 수정하면 다른 것도 바뀝니다. 잠시 후에 예를 살펴 보겠습니다."
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#배열-크기-변경",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#배열-크기-변경",
    "title": "Data_Mining CH1",
    "section": "7 배열 크기 변경",
    "text": "7 배열 크기 변경\n\n7.1 자신을 변경\nndarray의 shape 속성을 지정하면 간단히 크기를 바꿀 수 있습니다. 배열의 원소 개수는 동일하게 유지됩니다.\ng = np.arange(24)\nprint(g)\nprint(\"랭크:\", g.ndim)\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\n랭크: 1\ng.shape = (6, 4)\nprint(g)\nprint(\"랭크:\", g.ndim)\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]\n [12 13 14 15]\n [16 17 18 19]\n [20 21 22 23]]\n랭크: 2\ng.shape = (2, 3, 4)\nprint(g)\nprint(\"랭크:\", g.ndim)\n[[[ 0  1  2  3]\n  [ 4  5  6  7]\n  [ 8  9 10 11]]\n\n [[12 13 14 15]\n  [16 17 18 19]\n  [20 21 22 23]]]\n랭크: 3\ng[1,1,1] # 3차원 인덱싱 0,1 ~ 순\n17\n\n\n7.2 reshape\nreshape 함수는 동일한 데이터를 가리키는 새로운 ndarray 객체를 반환합니다. 한 배열을 수정하면 다른 것도 함께 바뀝니다.\ng2 = g.reshape(4,6)\nprint(g2)\nprint(\"랭크:\", g2.ndim)\n[[ 0  1  2  3  4  5]\n [ 6  7  8  9 10 11]\n [12 13 14 15 16 17]\n [18 19 20 21 22 23]]\n랭크: 2\n행 1, 열 2의 원소를 999로 설정합니다(인덱싱 방식은 아래를 참고하세요).\ng2[1, 2] = 999\ng2\narray([[  0,   1,   2,   3,   4,   5],\n       [  6,   7, 999,   9,  10,  11],\n       [ 12,  13,  14,  15,  16,  17],\n       [ 18,  19,  20,  21,  22,  23]])\n이에 상응하는 g의 원소도 수정됩니다.\ng\narray([[[  0,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [999,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]]])\n\n\n7.3 ravel\n마지막으로 ravel 함수는 동일한 데이터를 가리키는 새로운 1차원 ndarray를 반환합니다:\ng.ravel()\narray([  0,   1,   2,   3,   4,   5,   6,   7, 999,   9,  10,  11,  12,\n        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#산술-연산",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#산술-연산",
    "title": "Data_Mining CH1",
    "section": "8 산술 연산",
    "text": "8 산술 연산\n일반적인 산술 연산자(+, -, *, /, //, ** 등)는 모두 ndarray와 사용할 수 있습니다. 이 연산자는 원소별로 적용됩니다:\na = np.array([14, 23, 32, 41])\nb = np.array([5,  4,  3,  2])\nprint(\"a + b  =\", a + b)\nprint(\"a - b  =\", a - b)\nprint(\"a * b  =\", a * b)\nprint(\"a / b  =\", a / b)\nprint(\"a // b  =\", a // b) # 몫\nprint(\"a % b  =\", a % b)   # 나머지\nprint(\"a ** b =\", a ** b)\na + b  = [19 27 35 43]\na - b  = [ 9 19 29 39]\na * b  = [70 92 96 82]\na / b  = [ 2.8         5.75       10.66666667 20.5       ]\na // b  = [ 2  5 10 20]\na % b  = [4 3 2 1]\na ** b = [537824 279841  32768   1681]\n여기 곱셈은 행렬 곱셈이 아닙니다. 행렬 연산은 아래에서 설명합니다.\n배열의 크기는 같아야 합니다. 그렇지 않으면 넘파이가 브로드캐스팅 규칙을 적용합니다."
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#브로드캐스팅",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#브로드캐스팅",
    "title": "Data_Mining CH1",
    "section": "9 브로드캐스팅",
    "text": "9 브로드캐스팅\n일반적으로 넘파이는 동일한 크기의 배열을 기대합니다. 그렇지 않은 상황에는 브로드캐시틍 규칙을 적용합니다:\n\n9.1 규칙 1\n배열의 랭크가 동일하지 않으면 랭크가 맞을 때까지 랭크가 작은 배열 앞에 1을 추가합니다.\nh = np.arange(5).reshape(1,1, 5)\nh\narray([[[0, 1, 2, 3, 4]]])\n여기에 (1,1,5) 크기의 3D 배열에 (5,) 크기의 1D 배열을 더해 보죠. 브로드캐스팅의 규칙 1이 적용됩니다!\nh + [10, 20, 30, 40, 50]  # 다음과 동일합니다: h + [[[10, 20, 30, 40, 50]]]\narray([[[10, 21, 32, 43, 54]]])\n\n\n9.2 규칙 2\n특정 차원이 1인 배열은 그 차원에서 크기가 가장 큰 배열의 크기에 맞춰 동작합니다. 배열의 원소가 차원을 따라 반복됩니다.\nk = np.arange(6).reshape(2, 3)\nk\narray([[0, 1, 2],\n       [3, 4, 5]])\n(2,3) 크기의 2D ndarray에 (2,1) 크기의 2D 배열을 더해 보죠. 넘파이는 브로드캐스팅 규칙 2를 적용합니다:\nk + [[100], [200]]  # 다음과 같습니다: k + [[100, 100, 100], [200, 200, 200]]\narray([[100, 101, 102],\n       [203, 204, 205]])\n규칙 1과 2를 합치면 다음과 같이 동작합니다:\nk + [100, 200, 300]  # 규칙 1 적용: [[100, 200, 300]], 규칙 2 적용: [[100, 200, 300], [100, 200, 300]]\narray([[100, 201, 302],\n       [103, 204, 305]])\n또 매우 간단히 다음 처럼 해도 됩니다:\nk + 1000  # 다음과 같습니다: k + [[1000, 1000, 1000], [1000, 1000, 1000]]\narray([[1000, 1001, 1002],\n       [1003, 1004, 1005]])\n\n\n9.3 규칙 3\n규칙 1 & 2을 적용했을 때 모든 배열의 크기가 맞아야 합니다.\n### 차원(배열 크기)가 달라서 생기는 문제\ntry:\n    k + [33, 44]\nexcept ValueError as e:\n    print(e)\noperands could not be broadcast together with shapes (2,3) (2,) \n브로드캐스팅 규칙은 산술 연산 뿐만 아니라 넘파이 연산에서 많이 사용됩니다. 아래에서 더 보도록 하죠. 브로드캐스팅에 관한 더 자세한 정보는 온라인 문서를 참고하세요."
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#업캐스팅",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#업캐스팅",
    "title": "Data_Mining CH1",
    "section": "10 업캐스팅",
    "text": "10 업캐스팅\n\n10.1 스킵\ndtype이 다른 배열을 합칠 때 넘파이는 (실제 값에 상관없이) 모든 값을 다룰 수 있는 타입으로 업캐스팅합니다.\nk1 = np.arange(5, dtype=np.uint8)\nprint(k1.dtype, k1)\nuint8 [0 1 2 3 4]\nk2 = k1 + np.array([5, 6, 7, 8, 9], dtype=np.int8)\nprint(k2.dtype, k2)\nint16 [ 5  7  9 11 13]\n모든 int8과 uint8 값(-128에서 255까지)을 표현하기 위해 int16이 필요합니다. 이 코드에서는 uint8이면 충분하지만 업캐스팅되었습니다.\nk3 = k1 + 1.5\nprint(k3.dtype, k3)\nfloat64 [1.5 2.5 3.5 4.5 5.5]"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#조건-연산자",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#조건-연산자",
    "title": "Data_Mining CH1",
    "section": "11 조건 연산자",
    "text": "11 조건 연산자\n조건 연산자도 원소별로 적용됩니다:\nm = np.array([20, -5, 30, 40])\nm < [15, 16, 35, 36]\narray([False,  True,  True, False])\n브로드캐스팅을 사용합니다:\nm < 25  # m < [25, 25, 25, 25] 와 동일\narray([ True,  True, False, False])\n불리언 인덱싱과 함께 사용하면 아주 유용합니다(아래에서 설명하겠습니다).\nm[m < 25]\narray([20, -5])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#수학-함수와-통계-함수",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#수학-함수와-통계-함수",
    "title": "Data_Mining CH1",
    "section": "12 수학 함수와 통계 함수",
    "text": "12 수학 함수와 통계 함수\nndarray에서 사용할 수 있는 수학 함수와 통계 함수가 많습니다.\n\n12.1 ndarray 메서드\n일부 함수는 ndarray 메서드로 제공됩니다. 예를 들면:\na = np.array([[-2.5, 3.1, 7], [10, 11, 12]])\nprint(a)\nprint(\"평균 =\", a.mean())\n# 축에 따라서 다르게 : axis= n\n[[-2.5  3.1  7. ]\n [10.  11.  12. ]]\n평균 = 6.766666666666667\n이 명령은 크기에 상관없이 ndarray에 있는 모든 원소의 평균을 계산합니다.\n다음은 유용한 ndarray 메서드입니다:\nfor func in (a.min, a.max, a.sum, a.prod, a.std, a.var):\n    print(func.__name__, \"=\", func())\nmin = -2.5\nmax = 12.0\nsum = 40.6\nprod = -71610.0\nstd = 5.084835843520964\nvar = 25.855555555555554\n이 함수들은 선택적으로 매개변수 axis를 사용합니다. 지정된 축을 따라 원소에 연산을 적용하는데 사용합니다. 예를 들면:\nc=np.arange(24).reshape(2,3,4)\nc\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]])\nc.sum(axis=0)  # 첫 번째 축을 따라 더함, 결과는 3x4 배열\narray([[12, 14, 16, 18],\n       [20, 22, 24, 26],\n       [28, 30, 32, 34]])\nc.sum(axis=1)  # 두 번째 축을 따라 더함, 결과는 2x4 배열\narray([[12, 15, 18, 21],\n       [48, 51, 54, 57]])\nc.sum(axis=2)\narray([[ 6, 22, 38],\n       [54, 70, 86]])\n여러 축에 대해서 더할 수도 있습니다:\nc.sum(axis=(0,2))  # 첫 번째 축과 세 번째 축을 따라 더함, 결과는 (3,) 배열\narray([ 60,  92, 124])\n0+1+2+3 + 12+13+14+15, 4+5+6+7 + 16+17+18+19, 8+9+10+11 + 20+21+22+23\n(60, 92, 124)"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#일반-함수",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#일반-함수",
    "title": "Data_Mining CH1",
    "section": "13 일반 함수",
    "text": "13 일반 함수\n넘파이는 일반 함수(universal function) 또는 ufunc라고 부르는 원소별 함수를 제공합니다. 예를 들면 square 함수는 원본 ndarray를 복사하여 각 원소를 제곱한 새로운 ndarray 객체를 반환합니다:\na = np.array([[-2.5, 3.1, 7], [10, 11, 12]])\nnp.square(a)\narray([[  6.25,   9.61,  49.  ],\n       [100.  , 121.  , 144.  ]])\n다음은 유용한 단항 일반 함수들입니다:\nprint(\"원본 ndarray\")\nprint(a)\nfor func in (np.abs, np.sqrt, np.exp, np.log, np.sign, np.ceil, np.modf, np.isnan, np.cos):\n    print(\"\\n\", func.__name__)\n    print(func(a))\n원본 ndarray\n[[-2.5  3.1  7. ]\n [10.  11.  12. ]]\n\n absolute\n[[ 2.5  3.1  7. ]\n [10.  11.  12. ]]\n\n sqrt\n[[       nan 1.76068169 2.64575131]\n [3.16227766 3.31662479 3.46410162]]\n\n exp\n[[8.20849986e-02 2.21979513e+01 1.09663316e+03]\n [2.20264658e+04 5.98741417e+04 1.62754791e+05]]\n\n log\n[[       nan 1.13140211 1.94591015]\n [2.30258509 2.39789527 2.48490665]]\n\n sign\n[[-1.  1.  1.]\n [ 1.  1.  1.]]\n\n ceil\n[[-2.  4.  7.]\n [10. 11. 12.]]\n\n modf\n(array([[-0.5,  0.1,  0. ],\n       [ 0. ,  0. ,  0. ]]), array([[-2.,  3.,  7.],\n       [10., 11., 12.]]))\n\n isnan\n[[False False False]\n [False False False]]\n\n cos\n[[-0.80114362 -0.99913515  0.75390225]\n [-0.83907153  0.0044257   0.84385396]]\n\n\nC:\\Users\\seong taek\\AppData\\Local\\Temp\\ipykernel_8044\\4103705789.py:5: RuntimeWarning: invalid value encountered in sqrt\n  print(func(a))\nC:\\Users\\seong taek\\AppData\\Local\\Temp\\ipykernel_8044\\4103705789.py:5: RuntimeWarning: invalid value encountered in log\n  print(func(a))"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#이항-일반-함수",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#이항-일반-함수",
    "title": "Data_Mining CH1",
    "section": "14 이항 일반 함수",
    "text": "14 이항 일반 함수\n두 개의 ndarray에 원소별로 적용되는 이항 함수도 많습니다. 두 배열이 동일한 크기가 아니면 브로드캐스팅 규칙이 적용됩니다:\na = np.array([1, -2, 3, 4])\nb = np.array([2, 8, -1, 7])\nnp.add(a, b)  # a + b 와 동일\narray([ 3,  6,  2, 11])\nnp.greater(a, b)  # a > b 와 동일\narray([False, False,  True, False])\nnp.maximum(a, b)\narray([2, 8, 3, 7])\nnp.copysign(a, b)\narray([ 1.,  2., -3.,  4.])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#배열-인덱싱",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#배열-인덱싱",
    "title": "Data_Mining CH1",
    "section": "15 배열 인덱싱",
    "text": "15 배열 인덱싱\n\n15.1 1차원 배열\n1차원 넘파이 배열은 보통의 파이썬 배열과 비슷하게 사용할 수 있습니다:\na = np.array([1, 5, 3, 19, 13, 7, 3])\na[3]\n19\na[2:5]\narray([ 3, 19, 13])\na[2:-1]\narray([ 3, 19, 13,  7])\na[:2]\narray([1, 5])\na[2::2]\narray([ 3, 13,  3])\na[::-1]\narray([ 3,  7, 13, 19,  3,  5,  1])\n물론 원소를 수정할 수 있죠:\na[3]=999\na\narray([  1,   5,   3, 999,  13,   7,   3])\n슬라이싱을 사용해 ndarray를 수정할 수 있습니다:\na[2:5] = [997, 998, 999]\na\narray([  1,   5, 997, 998, 999,   7,   3])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#보통의-파이썬-배열과-차이점",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#보통의-파이썬-배열과-차이점",
    "title": "Data_Mining CH1",
    "section": "16 보통의 파이썬 배열과 차이점",
    "text": "16 보통의 파이썬 배열과 차이점\n보통의 파이썬 배열과 대조적으로 ndarray 슬라이싱에 하나의 값을 할당하면 슬라이싱 전체에 복사됩니다. 위에서 언급한 브로드캐스팅 덕택입니다.\na[2:5] = -1\na\narray([ 1,  5, -1, -1, -1,  7,  3])\n또한 이런 식으로 ndarray 크기를 늘리거나 줄일 수 없습니다:\ntry:\n    a[2:5] = [1,2,3,4,5,6]  # 너무 길어요\nexcept ValueError as e:\n    print(e)\ncould not broadcast input array from shape (6,) into shape (3,)\n원소를 삭제할 수도 없습니다:\ntry:\n    del a[2:5]\nexcept ValueError as e:\n    print(e)\ncannot delete array elements\n중요한 점은 ndarray의 슬라이싱은 같은 데이터 버퍼를 바라보는 뷰(view)입니다. 슬라이싱된 객체를 수정하면 실제 원본 ndarray가 수정됩니다!\na_slice = a[2:6]\nprint(a_slice)\n\na_slice[1] = 1000\nprint(a_slice)\n\nprint(a)  # 원본 배열이 수정됩니다!\n[-1 -1 -1  7]\n[  -1 1000   -1    7]\n[   1    5   -1 1000   -1    7    3]\na[3] = 2000\na_slice  # 비슷하게 원본 배열을 수정하면 슬라이싱 객체에도 반영됩니다!\narray([  -1, 2000,   -1,    7])\n데이터를 복사하려면 copy 메서드를 사용해야 합니다:\nprint(a)\n\nanother_slice = a[2:6].copy()\nanother_slice[1] = 3000\n\nprint(a)  # 원본 배열이 수정되지 않습니다\n[   1    5   -1 2000   -1    7    3]\n[   1    5   -1 2000   -1    7    3]\na[3] = 4000\nanother_slice  # 마찬가지로 원본 배열을 수정해도 복사된 배열은 바뀌지 않습니다\narray([  -1, 3000,   -1,    7])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#다차원-배열",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#다차원-배열",
    "title": "Data_Mining CH1",
    "section": "17 다차원 배열",
    "text": "17 다차원 배열\n다차원 배열은 비슷한 방식으로 각 축을 따라 인덱싱 또는 슬라이싱해서 사용합니다. 콤마로 구분합니다:\nb = np.arange(48).reshape(4, 12)\nb\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n       [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])\nb[1, 2]  # 행 1, 열 2\n14\nb[1, :]  # 행 1, 모든 열\narray([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\nb[1, :].shape\n(12,)\nb[:, 1]  # 모든 행, 열 1\narray([ 1, 13, 25, 37])\n주의: 다음 두 표현에는 미묘한 차이가 있습니다:\nb[1, :]\narray([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\nb[1:2, :]\narray([[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]])\nb[1, :].shape\n(12,)\nb[1:2, :].shape\n(1, 12)\n첫 번째 표현식은 (12,) 크기인 1D 배열로 행이 하나입니다. 두 번째는 (1, 12) 크기인 2D 배열로 같은 행을 반환합니다."
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#팬시-인덱싱fancy-indexing",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#팬시-인덱싱fancy-indexing",
    "title": "Data_Mining CH1",
    "section": "18 팬시 인덱싱(Fancy indexing)",
    "text": "18 팬시 인덱싱(Fancy indexing)\n관심 대상의 인덱스 리스트를 지정할 수도 있습니다. 이를 팬시 인덱싱이라고 부릅니다.\nb\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n       [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])\nb[(0,2), 2:5]  # 행 0과 2, 열 2에서 4(5-1)까지\narray([[ 2,  3,  4],\n       [26, 27, 28]])\nb[:, (-1, 2, -1)]  # 모든 행, 열 -1 (마지막), 2와 -1 (다시 반대 방향으로)\narray([[11,  2, 11],\n       [23, 14, 23],\n       [35, 26, 35],\n       [47, 38, 47]])\n여러 개의 인덱스 리스트를 지정하면 인덱스에 맞는 값이 포함된 1D ndarray를 반환됩니다.\nb[(-1, 2, -1, 2), (5, 9, 1, 9)]  # returns a 1D array with b[-1, 5], b[2, 9], b[-1, 1] and b[2, 9] (again)\narray([41, 33, 37, 33])\n\n18.1 Quiz\narray([[24, 25], [36, 37]])\n출력해보기\nb[(2,3),0:2], b[2:4,0:2]\n(array([[24, 25],\n        [36, 37]]),\n array([[24, 25],\n        [36, 37]]))\n\n\n18.2 Quiz\n\n2차원 배열 ’array_2d’에서 첫번째 행의 모든 요소 선택\n2차원 배열 ’array_2d’에서 두번째 열의 모든 요소 선택\n2차원 배열 ’array_2d’에서 25,30,40,45 선택\n\narray_2d = np.array([[5, 10, 15],\n                    [20, 25, 30],\n                    [35, 40, 45]])\narray_2d\narray([[ 5, 10, 15],\n       [20, 25, 30],\n       [35, 40, 45]])\n# 1\nprint(array_2d[0,:])\n# 2\nprint(array_2d[:,1])\n# 3\nprint(array_2d[1:3,1:3])\n[ 5 10 15]\n[10 25 40]\n[[25 30]\n [40 45]]\n# 또는\narray_2d[0,:],array_2d[:,1], array_2d[(1,2),1:3]\n(array([ 5, 10, 15]),\n array([10, 25, 40]),\n array([[25, 30],\n        [40, 45]]))"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#고차원",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#고차원",
    "title": "Data_Mining CH1",
    "section": "19 고차원",
    "text": "19 고차원\n고차원에서도 동일한 방식이 적용됩니다. 몇 가지 예를 살펴 보겠습니다:\nc = b.reshape(4,2,6)\nc\narray([[[ 0,  1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10, 11]],\n\n       [[12, 13, 14, 15, 16, 17],\n        [18, 19, 20, 21, 22, 23]],\n\n       [[24, 25, 26, 27, 28, 29],\n        [30, 31, 32, 33, 34, 35]],\n\n       [[36, 37, 38, 39, 40, 41],\n        [42, 43, 44, 45, 46, 47]]])\nc[2, 1, 4]  # 행렬 2, 행 1, 열 4\n34\nc[2, :, 3]  # 행렬 2, 모든 행, 열 3\narray([27, 33])\n어떤 축에 대한 인덱스를 지정하지 않으면 이 축의 모든 원소가 반환됩니다:\nc[2, 1]  # 행렬 2, 행 1, 모든 열이 반환됩니다. c[2, 1, :]와 동일합니다.\narray([30, 31, 32, 33, 34, 35])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#생략-부호-...",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#생략-부호-...",
    "title": "Data_Mining CH1",
    "section": "20 생략 부호 (...)",
    "text": "20 생략 부호 (...)\n생략 부호(...)를 쓰면 모든 지정하지 않은 축의 원소를 포함합니다.\nc[2, ...]  #  행렬 2, 모든 행, 모든 열. c[2, :, :]와 동일\narray([[24, 25, 26, 27, 28, 29],\n       [30, 31, 32, 33, 34, 35]])\nc[2, 1, ...]  # 행렬 2, 행 1, 모든 열. c[2, 1, :]와 동일\narray([30, 31, 32, 33, 34, 35])\nc[2, ..., 3]  # 행렬 2, 모든 행, 열 3. c[2, :, 3]와 동일\narray([27, 33])\nc[..., 3]  # 모든 행렬, 모든 행, 열 3. c[:, :, 3]와 동일\narray([[ 3,  9],\n       [15, 21],\n       [27, 33],\n       [39, 45]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#불리언-인덱싱",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#불리언-인덱싱",
    "title": "Data_Mining CH1",
    "section": "21 불리언 인덱싱",
    "text": "21 불리언 인덱싱\n불리언 값을 가진 ndarray를 사용해 축의 인덱스를 지정할 수 있습니다.\nb = np.arange(48).reshape(4, 12)\nb\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n       [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])\nrows_on = np.array([True, False, True, False])\nb[rows_on, :]  # True에 해당하는 값 가져오기, 행 0과 2, 모든 열. b[(0, 2), :]와 동일\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]])\ncols_on = np.array([False, True, False] * 4)\nb[:, cols_on]  # 모든 행, 열 1, 4, 7, 10\narray([[ 1,  4,  7, 10],\n       [13, 16, 19, 22],\n       [25, 28, 31, 34],\n       [37, 40, 43, 46]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#np.ix_",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#np.ix_",
    "title": "Data_Mining CH1",
    "section": "22 np.ix_",
    "text": "22 np.ix_\n여러 축에 걸쳐서는 불리언 인덱싱을 사용할 수 없고 ix_ 함수를 사용합니다:\nb[np.ix_(rows_on, cols_on)]\narray([[ 1,  4,  7, 10],\n       [25, 28, 31, 34]])\nnp.ix_(rows_on, cols_on) # 추출 위치 출력\n(array([[0],\n        [2]], dtype=int64),\n array([[ 1,  4,  7, 10]], dtype=int64))\nndarray와 같은 크기의 불리언 배열을 사용하면 해당 위치가 True인 모든 원소를 담은 1D 배열이 반환됩니다. 일반적으로 조건 연산자와 함께 사용합니다:\nb[b % 3 == 1]\narray([ 1,  4,  7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#배열-쌓기",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#배열-쌓기",
    "title": "Data_Mining CH1",
    "section": "24 배열 쌓기",
    "text": "24 배열 쌓기\n종종 다른 배열을 쌓아야 할 때가 있습니다. 넘파이는 이를 위해 몇 개의 함수를 제공합니다. 먼저 배열 몇 개를 만들어 보죠.\nq1 = np.full((3,4), 1.0)\nq1\n# axis\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.]])\nq2 = np.full((4,4), 2.0)\nq2\narray([[2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.]])\nq3 = np.full((3,4), 3.0)\nq3\narray([[3., 3., 3., 3.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.]])\n\n24.1 vstack\nvstack 함수를 사용하여 수직으로 쌓아보죠:\nq4 = np.vstack((q1, q2, q3))\nq4\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.]])\nq4.shape\n(10, 4)\nq1, q2, q3가 모두 같은 크기이므로 가능합니다(수직으로 쌓기 때문에 수직 축은 크기가 달라도 됩니다).\n\n\n24.2 hstack\nhstack을 사용해 수평으로도 쌓을 수 있습니다:\nq5 = np.hstack((q1, q3))\nq5\narray([[1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.]])\n# concatenate도 가능\nnp.concatenate((q1,q3),axis=1)\narray([[1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.]])\nq5.shape\n(3, 8)\nq1과 q3가 모두 3개의 행을 가지고 있기 때문에 가능합니다. q2는 4개의 행을 가지고 있기 때문에 q1, q3와 수평으로 쌓을 수 없습니다:\ntry:\n    q5 = np.hstack((q1, q2, q3))\nexcept ValueError as e:\n    print(e)\nall the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 3 and the array at index 1 has size 4\n\n\n24.3 concatenate\nconcatenate 함수는 지정한 축으로도 배열을 쌓습니다.\nq7 = np.concatenate((q1, q2, q3), axis=0)  # vstack과 동일\nq7\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.]])\nq7.shape\n(10, 4)\n예상했겠지만 hstack은 axis=1으로 concatenate를 호출하는 것과 같습니다.\n\n\n24.4 stack\nstack 함수는 새로운 축을 따라 배열을 쌓습니다. 모든 배열은 같은 크기를 가져야 합니다.\nq8 = np.stack((q1, q3))\nq8\narray([[[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]],\n\n       [[3., 3., 3., 3.],\n        [3., 3., 3., 3.],\n        [3., 3., 3., 3.]]])\nq8.shape\n(2, 3, 4)"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#배열-분할",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#배열-분할",
    "title": "Data_Mining CH1",
    "section": "25 배열 분할",
    "text": "25 배열 분할\n분할은 쌓기의 반대입니다. 예를 들어 vsplit 함수는 행렬을 수직으로 분할합니다.\n먼저 6x4 행렬을 만들어 보죠:\nr = np.arange(24).reshape(6,4)\nr\narray([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11],\n       [12, 13, 14, 15],\n       [16, 17, 18, 19],\n       [20, 21, 22, 23]])\n수직으로 동일한 크기로 나누어 보겠습니다:\n### 3개의 변수로 분할 - 수직\nr1, r2, r3 = np.vsplit(r, 3)\nr1\narray([[0, 1, 2, 3],\n       [4, 5, 6, 7]])\nr2\narray([[ 8,  9, 10, 11],\n       [12, 13, 14, 15]])\nr3\narray([[16, 17, 18, 19],\n       [20, 21, 22, 23]])\nsplit 함수는 주어진 축을 따라 배열을 분할합니다. vsplit는 axis=0으로 split를 호출하는 것과 같습니다. hsplit 함수는 axis=1로 split를 호출하는 것과 같습니다:\n### 2개의 변수로 분할 - 수평\nr4, r5 = np.hsplit(r, 2)\nr4\narray([[ 0,  1],\n       [ 4,  5],\n       [ 8,  9],\n       [12, 13],\n       [16, 17],\n       [20, 21]])\nr5\narray([[ 2,  3],\n       [ 6,  7],\n       [10, 11],\n       [14, 15],\n       [18, 19],\n       [22, 23]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#배열-전치",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#배열-전치",
    "title": "Data_Mining CH1",
    "section": "26 배열 전치",
    "text": "26 배열 전치\n읽어만 보기\ntranspose 메서드는 주어진 순서대로 축을 뒤바꾸어 ndarray 데이터에 대한 새로운 뷰를 만듭니다.\n예를 위해 3D 배열을 만들어 보죠:\nt = np.arange(24).reshape(4,2,3)\nt\narray([[[ 0,  1,  2],\n        [ 3,  4,  5]],\n\n       [[ 6,  7,  8],\n        [ 9, 10, 11]],\n\n       [[12, 13, 14],\n        [15, 16, 17]],\n\n       [[18, 19, 20],\n        [21, 22, 23]]])\n0, 1, 2(깊이, 높이, 너비) 축을 1, 2, 0 (깊이→너비, 높이→깊이, 너비→높이) 순서로 바꾼 ndarray를 만들어 보겠습니다:\nt1 = t.transpose((1,2,0))\nt1\narray([[[ 0,  6, 12, 18],\n        [ 1,  7, 13, 19],\n        [ 2,  8, 14, 20]],\n\n       [[ 3,  9, 15, 21],\n        [ 4, 10, 16, 22],\n        [ 5, 11, 17, 23]]])\nt1.shape\n(2, 3, 4)\ntranspose 기본값은 차원의 순서를 역전시킵니다:\nt2 = t.transpose()  # t.transpose((2, 1, 0))와 동일\nt2\narray([[[ 0,  6, 12, 18],\n        [ 3,  9, 15, 21]],\n\n       [[ 1,  7, 13, 19],\n        [ 4, 10, 16, 22]],\n\n       [[ 2,  8, 14, 20],\n        [ 5, 11, 17, 23]]])\nt2.shape\n(3, 2, 4)\n넘파이는 두 축을 바꾸는 swapaxes 함수를 제공합니다. 예를 들어 깊이와 높이를 뒤바꾸어 t의 새로운 뷰를 만들어 보죠:\nt3 = t.swapaxes(0,1)  # t.transpose((1, 0, 2))와 동일\nt3\narray([[[ 0,  1,  2],\n        [ 6,  7,  8],\n        [12, 13, 14],\n        [18, 19, 20]],\n\n       [[ 3,  4,  5],\n        [ 9, 10, 11],\n        [15, 16, 17],\n        [21, 22, 23]]])\nt3.shape\n(2, 4, 3)"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#선형-대수학",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#선형-대수학",
    "title": "Data_Mining CH1",
    "section": "27 선형 대수학",
    "text": "27 선형 대수학\n넘파이 2D 배열을 사용하면 파이썬에서 행렬을 효율적으로 표현할 수 있습니다. 주요 행렬 연산을 간단히 둘러 보겠습니다. 선형 대수학, 벡터와 행렬에 관한 자세한 내용은 Linear Algebra tutorial를 참고하세요."
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#행렬-전치",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#행렬-전치",
    "title": "Data_Mining CH1",
    "section": "28 행렬 전치",
    "text": "28 행렬 전치\nT 속성은 랭크가 2보다 크거나 같을 때 transpose()를 호출하는 것과 같습니다:\nm1 = np.arange(10).reshape(2,5)\nm1\narray([[0, 1, 2, 3, 4],\n       [5, 6, 7, 8, 9]])\nm1.T\narray([[0, 5],\n       [1, 6],\n       [2, 7],\n       [3, 8],\n       [4, 9]])\nT 속성은 랭크가 0이거나 1인 배열에는 아무런 영향을 미치지 않습니다:\nm2 = np.arange(5)\nm2\narray([0, 1, 2, 3, 4])\nm2.T\narray([0, 1, 2, 3, 4])\n먼저 1D 배열을 하나의 행이 있는 행렬(2D)로 바꾼다음 전치를 수행할 수 있습니다:\nm2r = m2.reshape(1,5)\nm2r\narray([[0, 1, 2, 3, 4]])\nm2.shape, m2r.shape\n((5,), (1, 5))\nm2r.T\narray([[0],\n       [1],\n       [2],\n       [3],\n       [4]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#행렬-곱셈",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#행렬-곱셈",
    "title": "Data_Mining CH1",
    "section": "29 행렬 곱셈",
    "text": "29 행렬 곱셈\n두 개의 행렬을 만들어 dot 메서드로 행렬 곱셈을 실행해 보죠.\nn1 = np.arange(10).reshape(2, 5)\nn1\narray([[0, 1, 2, 3, 4],\n       [5, 6, 7, 8, 9]])\nn2 = np.arange(15).reshape(5,3)\nn2\narray([[ 0,  1,  2],\n       [ 3,  4,  5],\n       [ 6,  7,  8],\n       [ 9, 10, 11],\n       [12, 13, 14]])\nn1.dot(n2)\narray([[ 90, 100, 110],\n       [240, 275, 310]])\n주의: 앞서 언급한 것처럼 n1*n2는 행렬 곱셈이 아니라 원소별 곱셈(또는 아다마르 곱이라 부릅니다)입니다."
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#역행렬과-유사-역행렬",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#역행렬과-유사-역행렬",
    "title": "Data_Mining CH1",
    "section": "30 역행렬과 유사 역행렬",
    "text": "30 역행렬과 유사 역행렬\nnumpy.linalg 모듈 안에 많은 선형 대수 함수들이 있습니다. 특히 inv 함수는 정방 행렬의 역행렬을 계산합니다:\nimport numpy.linalg as linalg\n\nm3 = np.array([[1,2,3],[5,7,11],[21,29,31]])\nm3\narray([[ 1,  2,  3],\n       [ 5,  7, 11],\n       [21, 29, 31]])\nlinalg.inv(m3)\narray([[-2.31818182,  0.56818182,  0.02272727],\n       [ 1.72727273, -0.72727273,  0.09090909],\n       [-0.04545455,  0.29545455, -0.06818182]])\npinv 함수를 사용하여 유사 역행렬을 계산할 수도 있습니다:\nlinalg.pinv(m3)\narray([[-2.31818182,  0.56818182,  0.02272727],\n       [ 1.72727273, -0.72727273,  0.09090909],\n       [-0.04545455,  0.29545455, -0.06818182]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#단위-행렬",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#단위-행렬",
    "title": "Data_Mining CH1",
    "section": "31 단위 행렬",
    "text": "31 단위 행렬\n행렬과 그 행렬의 역행렬을 곱하면 단위 행렬이 됩니다(작은 소숫점 오차가 있습니다):\nm3.dot(linalg.inv(m3))\narray([[ 1.00000000e+00, -5.55111512e-17,  0.00000000e+00],\n       [-2.98372438e-16,  1.00000000e+00, -5.55111512e-17],\n       [ 5.78009862e-15,  1.27675648e-15,  1.00000000e+00]])\neye 함수는 NxN 크기의 단위 행렬을 만듭니다:\nnp.eye(3)\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#qr-분해",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#qr-분해",
    "title": "Data_Mining CH1",
    "section": "32 QR 분해",
    "text": "32 QR 분해\nqr 함수는 행렬을 QR 분해합니다:\nq, r = linalg.qr(m3)\nq\narray([[-0.04627448,  0.98786672,  0.14824986],\n       [-0.23137241,  0.13377362, -0.96362411],\n       [-0.97176411, -0.07889213,  0.22237479]])\nr\narray([[-21.61018278, -29.89331494, -32.80860727],\n       [  0.        ,   0.62427688,   1.9894538 ],\n       [  0.        ,   0.        ,  -3.26149699]])\nq.dot(r)  # q.r는 m3와 같습니다\narray([[ 1.,  2.,  3.],\n       [ 5.,  7., 11.],\n       [21., 29., 31.]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#행렬식",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#행렬식",
    "title": "Data_Mining CH1",
    "section": "33 행렬식",
    "text": "33 행렬식\ndet 함수는 행렬식을 계산합니다:\nlinalg.det(m3)  # 행렬식 계산\n43.99999999999999"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#고윳값과-고유벡터",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#고윳값과-고유벡터",
    "title": "Data_Mining CH1",
    "section": "34 고윳값과 고유벡터",
    "text": "34 고윳값과 고유벡터\neig 함수는 정방 행렬의 고윳값과 고유벡터를 계산합니다:\neigenvalues, eigenvectors = linalg.eig(m3)\neigenvalues # λ\narray([42.26600592, -0.35798416, -2.90802176])\neigenvectors # v\narray([[-0.08381182, -0.76283526, -0.18913107],\n       [-0.3075286 ,  0.64133975, -0.6853186 ],\n       [-0.94784057, -0.08225377,  0.70325518]])\nm3.dot(eigenvectors) - eigenvalues * eigenvectors  # m3.v - λ*v = 0\narray([[6.66133815e-15, 1.11022302e-16, 4.44089210e-16],\n       [7.10542736e-15, 2.88657986e-15, 3.10862447e-15],\n       [3.55271368e-14, 6.81746326e-15, 4.88498131e-15]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#특잇값-분해",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#특잇값-분해",
    "title": "Data_Mining CH1",
    "section": "35 특잇값 분해",
    "text": "35 특잇값 분해\nsvd 함수는 행렬을 입력으로 받아 그 행렬의 특잇값 분해를 반환합니다:\nm4 = np.array([[1,0,0,0,2], [0,0,3,0,0], [0,0,0,0,0], [0,2,0,0,0]])\nm4\narray([[1, 0, 0, 0, 2],\n       [0, 0, 3, 0, 0],\n       [0, 0, 0, 0, 0],\n       [0, 2, 0, 0, 0]])\nU, S_diag, V = linalg.svd(m4)\nU\narray([[ 0.,  1.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 0.,  0.,  0., -1.],\n       [ 0.,  0.,  1.,  0.]])\nS_diag\narray([3.        , 2.23606798, 2.        , 0.        ])\nsvd 함수는 Σ의 대각 원소 값만 반환합니다. 전체 Σ 행렬은 다음과 같이 만듭니다:\nS = np.zeros((4, 5))\nS[np.diag_indices(4)] = S_diag\nS  # Σ\narray([[3.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 2.23606798, 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 2.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ]])\nV\narray([[-0.        ,  0.        ,  1.        ,  0.        ,  0.        ],\n       [ 0.4472136 ,  0.        ,  0.        ,  0.        ,  0.89442719],\n       [-0.        ,  1.        ,  0.        ,  0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ],\n       [-0.89442719,  0.        ,  0.        ,  0.        ,  0.4472136 ]])\nU.dot(S).dot(V) # U.Σ.V == m4\narray([[1., 0., 0., 0., 2.],\n       [0., 0., 3., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 2., 0., 0., 0.]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#대각원소와-대각합",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#대각원소와-대각합",
    "title": "Data_Mining CH1",
    "section": "36 대각원소와 대각합",
    "text": "36 대각원소와 대각합\nnp.diag(m3)  # m3의 대각 원소입니다(왼쪽 위에서 오른쪽 아래)\narray([ 1,  7, 31])\nnp.trace(m3)  # np.diag(m3).sum()와 같습니다\n39"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#선형-방정식-풀기",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#선형-방정식-풀기",
    "title": "Data_Mining CH1",
    "section": "37 선형 방정식 풀기",
    "text": "37 선형 방정식 풀기\nsolve 함수는 다음과 같은 선형 방정식을 풉니다:\n\n\\(2x + 6y = 6\\)\n\\(5x + 3y = -9\\)\n\ncoeffs  = np.array([[2, 6], [5, 3]])\ndepvars = np.array([6, -9])\nsolution = linalg.solve(coeffs, depvars)\nsolution\narray([-3.,  2.])\nsolution을 확인해 보죠:\ncoeffs.dot(solution), depvars  # 네 같네요\n(array([ 6., -9.]), array([ 6, -9]))\n좋습니다! 다른 방식으로도 solution을 확인해 보죠:\nnp.allclose(coeffs.dot(solution), depvars)\nTrue"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#벡터화",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#벡터화",
    "title": "Data_Mining CH1",
    "section": "38 벡터화",
    "text": "38 벡터화\n한 번에 하나씩 개별 배열 원소에 대해 연산을 실행하는 대신 배열 연산을 사용하면 훨씬 효율적인 코드를 만들 수 있습니다. 이를 벡터화라고 합니다. 이를 사용하여 넘파이의 최적화된 성능을 활용할 수 있습니다.\n예를 들어, \\(sin(xy/40.5)\\) 식을 기반으로 768x1024 크기 배열을 생성하려고 합니다. 중첩 반복문 안에 파이썬의 math 함수를 사용하는 것은 나쁜 방법입니다:\nimport math\ndata = np.empty((768, 1024))\nfor y in range(768):\n    for x in range(1024):\n        data[y, x] = math.sin(x*y/40.5)  # 매우 비효율적입니다!\n작동은 하지만 순수한 파이썬 코드로 반복문이 진행되기 때문에 아주 비효율적입니다. 이 알고리즘을 벡터화해 보죠. 먼저 넘파이 meshgrid 함수로 좌표 벡터를 사용해 행렬을 만듭니다.\nx_coords = np.arange(0, 1024)  # [0, 1, 2, ..., 1023]\ny_coords = np.arange(0, 768)   # [0, 1, 2, ..., 767]\nX, Y = np.meshgrid(x_coords, y_coords)\nX\narray([[   0,    1,    2, ..., 1021, 1022, 1023],\n       [   0,    1,    2, ..., 1021, 1022, 1023],\n       [   0,    1,    2, ..., 1021, 1022, 1023],\n       ...,\n       [   0,    1,    2, ..., 1021, 1022, 1023],\n       [   0,    1,    2, ..., 1021, 1022, 1023],\n       [   0,    1,    2, ..., 1021, 1022, 1023]])\nY\narray([[  0,   0,   0, ...,   0,   0,   0],\n       [  1,   1,   1, ...,   1,   1,   1],\n       [  2,   2,   2, ...,   2,   2,   2],\n       ...,\n       [765, 765, 765, ..., 765, 765, 765],\n       [766, 766, 766, ..., 766, 766, 766],\n       [767, 767, 767, ..., 767, 767, 767]])\n여기서 볼 수 있듯이 X와 Y 모두 768x1024 배열입니다. X에 있는 모든 값은 수평 좌표에 해당합니다. Y에 있는 모든 값은 수직 좌표에 해당합니다.\n이제 간단히 배열 연산을 사용해 계산할 수 있습니다:\ndata = np.sin(X*Y/40.5)\n맷플롯립의 imshow 함수를 사용해 이 데이터를 그려보죠(matplotlib tutorial을 참조하세요).\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfig = plt.figure(1, figsize=(7, 6))\nplt.imshow(data, cmap=cm.hot)\nplt.show()\n\n\n\npng"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#저장과-로딩",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#저장과-로딩",
    "title": "Data_Mining CH1",
    "section": "39 저장과 로딩",
    "text": "39 저장과 로딩\n넘파이는 ndarray를 바이너리 또는 텍스트 포맷으로 손쉽게 저장하고 로드할 수 있습니다."
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#바이너리-.npy-포맷",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#바이너리-.npy-포맷",
    "title": "Data_Mining CH1",
    "section": "40 바이너리 .npy 포맷",
    "text": "40 바이너리 .npy 포맷\n랜덤 배열을 만들고 저장해 보죠.\na = np.random.rand(2,3)\na\narray([[0.64010765, 0.96817264, 0.58635902],\n       [0.84834041, 0.25887928, 0.52484385]])\nnp.save(\"my_array\", a)\n끝입니다! 파일 이름의 확장자를 지정하지 않았기 때문에 넘파이는 자동으로 .npy를 붙입니다. 파일 내용을 확인해 보겠습니다:\nwith open(\"my_array.npy\", \"rb\") as f:\n    content = f.read()\n\ncontent\nb\"\\x93NUMPY\\x01\\x00v\\x00{'descr': '<f8', 'fortran_order': False, 'shape': (2, 3), }                                                          \\n%u\\x9e\\x0b\\xc3{\\xe4?\\n\\\\\\x9e2E\\xfb\\xee?`,\\xcc\\xffs\\xc3\\xe2?\\x90\\x10F\\xc8\\x9a%\\xeb?V\\x0e-gz\\x91\\xd0?\\x19y\\xa1V\\x85\\xcb\\xe0?\"\n이 파일을 넘파이 배열로 로드하려면 load 함수를 사용합니다:\na_loaded = np.load(\"my_array.npy\")\na_loaded\narray([[0.64010765, 0.96817264, 0.58635902],\n       [0.84834041, 0.25887928, 0.52484385]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#텍스트-포맷",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#텍스트-포맷",
    "title": "Data_Mining CH1",
    "section": "41 텍스트 포맷",
    "text": "41 텍스트 포맷\n배열을 텍스트 포맷으로 저장해 보죠:\nnp.savetxt(\"my_array.csv\", a)\n파일 내용을 확인해 보겠습니다:\nwith open(\"my_array.csv\", \"rt\") as f:\n    print(f.read())\n6.401076533253770018e-01 9.681726444858906877e-01 5.863590236707629799e-01\n8.483404075236808950e-01 2.588792808130383483e-01 5.248438541418948278e-01\n이 파일은 탭으로 구분된 CSV 파일입니다. 다른 구분자를 지정할 수도 있습니다:\nnp.savetxt(\"my_array.csv\", a, delimiter=\",\")\n이 파일을 로드하려면 loadtxt 함수를 사용합니다:\na_loaded = np.loadtxt(\"my_array.csv\", delimiter=\",\")\na_loaded\narray([[0.64010765, 0.96817264, 0.58635902],\n       [0.84834041, 0.25887928, 0.52484385]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#압축된-.npz-포맷",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#압축된-.npz-포맷",
    "title": "Data_Mining CH1",
    "section": "42 압축된 .npz 포맷",
    "text": "42 압축된 .npz 포맷\n여러 개의 배열을 압축된 한 파일로 저장하는 것도 가능합니다:\nb = np.arange(24, dtype=np.uint8).reshape(2, 3, 4)\nb\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]], dtype=uint8)\nnp.savez(\"my_arrays\", my_a=a, my_b=b)\n파일 내용을 확인해 보죠. .npz 파일 확장자가 자동으로 추가되었습니다.\nwith open(\"my_arrays.npz\", \"rb\") as f:\n    content = f.read()\n\nrepr(content)[:180] + \"[...]\"\n'b\"PK\\\\x03\\\\x04\\\\x14\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00!\\\\x00D\\\\x08%/\\\\xb0\\\\x00\\\\x00\\\\x00\\\\xb0\\\\x00\\\\x00\\\\x00\\\\x08\\\\x00\\\\x14\\\\x00my_a.npy\\\\x01\\\\x00\\\\x10\\\\x00\\\\xb0\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\xb0\\\\x00\\\\x00\\\\x00\\\\x00[...]'\n다음과 같이 이 파일을 로드할 수 있습니다:\nmy_arrays = np.load(\"my_arrays.npz\")\nmy_arrays\n<numpy.lib.npyio.NpzFile at 0x2665066fd00>\n게으른 로딩을 수행하는 딕셔너리와 유사한 객체입니다:\nmy_arrays.keys()\nKeysView(<numpy.lib.npyio.NpzFile object at 0x000002665066FD00>)\nmy_arrays[\"my_a\"]\narray([[0.64010765, 0.96817264, 0.58635902],\n       [0.84834041, 0.25887928, 0.52484385]])"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#반복",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#반복",
    "title": "Data_Mining CH1",
    "section": "23 반복",
    "text": "23 반복\nndarray를 반복하는 것은 일반적인 파이썬 배열을 반복한는 것과 매우 유사합니다. 다차원 배열을 반복하면 첫 번째 축에 대해서 수행됩니다.\nc = np.arange(24).reshape(2, 3, 4)  # 3D 배열 (두 개의 3x4 행렬로 구성됨)\nc\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]])\nfor m in c:\n    print(\"아이템:\")\n    print(m)\n아이템:\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]]\n아이템:\n[[12 13 14 15]\n [16 17 18 19]\n [20 21 22 23]]\nfor i in range(len(c)):  # len(c) == c.shape[0]\n    print(\"아이템:\")\n    print(c[i])\n아이템:\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]]\n아이템:\n[[12 13 14 15]\n [16 17 18 19]\n [20 21 22 23]]\nndarray에 있는 모든 원소를 반복하려면 flat 속성을 사용합니다:\nfor i in c.flat:\n    print(\"아이템:\", i)\n아이템: 0\n아이템: 1\n아이템: 2\n아이템: 3\n아이템: 4\n아이템: 5\n아이템: 6\n아이템: 7\n아이템: 8\n아이템: 9\n아이템: 10\n아이템: 11\n아이템: 12\n아이템: 13\n아이템: 14\n아이템: 15\n아이템: 16\n아이템: 17\n아이템: 18\n아이템: 19\n아이템: 20\n아이템: 21\n아이템: 22\n아이템: 23"
  },
  {
    "objectID": "posts/Data_Mining_Numpy/numpy 공부2.html#그-다음은",
    "href": "posts/Data_Mining_Numpy/numpy 공부2.html#그-다음은",
    "title": "Data_Mining CH1",
    "section": "43 그 다음은?",
    "text": "43 그 다음은?\n넘파이 기본 요소를 모두 배웠지만 훨씬 더 많은 기능이 있습니다. 이를 배우는 가장 좋은 방법은 넘파이를 직접 실습해 보고 훌륭한 넘파이 문서에서 필요한 함수와 기능을 찾아 보세요."
  },
  {
    "objectID": "posts/Spatial_Info_Analysis Ch1/공간정보분석 CH2.html",
    "href": "posts/Spatial_Info_Analysis Ch1/공간정보분석 CH2.html",
    "title": "Spatial_Info_Analysis CH1",
    "section": "",
    "text": "HTML파일로 보기\n지리 공간 데이터 - 벡터 & 래스터"
  },
  {
    "objectID": "posts/Spatial_Info_Analysis Ch1/공간정보분석 CH2.html#공간-정보-분석-ch2-geographic-data-in-r",
    "href": "posts/Spatial_Info_Analysis Ch1/공간정보분석 CH2.html#공간-정보-분석-ch2-geographic-data-in-r",
    "title": "Spatial_Info_Analysis CH1",
    "section": "1 공간 정보 분석 CH2 : Geographic data in R",
    "text": "1 공간 정보 분석 CH2 : Geographic data in R"
  },
  {
    "objectID": "posts/Spatial_Info_Analysis Ch1/공간정보분석 CH2.html#패키지-불러오기",
    "href": "posts/Spatial_Info_Analysis Ch1/공간정보분석 CH2.html#패키지-불러오기",
    "title": "Spatial_Info_Analysis CH1",
    "section": "2 패키지 불러오기",
    "text": "2 패키지 불러오기\n\nsf : 지리 공간 벡터 데이터(vector data) 분석을 위한 패키지\nraster : 지리 공간 레스터 데이터(raster data)를 처리 및 분석하는데 사용\nspData : 37개의 지리공간 데이터셋이 내장\nspDataLarge : 지리 공간 데이터 샘플을 내장\n\n\n#install.packages(\"sf\")     \n#install.packages(\"raster\") \n#install.packages(\"spData\") \n#install.packages(\"spDataLarge\", repos = \"https://nowosad.github.io/drat/\",type = \"source\")\n\nlibrary(sf)\nlibrary(raster)\nlibrary(spData)\nlibrary(spDataLarge)"
  },
  {
    "objectID": "posts/Spatial_Info_Analysis Ch1/공간정보분석 CH2.html#벡터vector-데이터",
    "href": "posts/Spatial_Info_Analysis Ch1/공간정보분석 CH2.html#벡터vector-데이터",
    "title": "Spatial_Info_Analysis CH1",
    "section": "3 벡터(Vector) 데이터",
    "text": "3 벡터(Vector) 데이터\n\n3.1 sf 패키지\n\nEdzer Pebesma, Roger Bivand 등이 2016년 10월에 최초로 오픈소스로 공개하였으며, R로 단순 지리특성 기하 (Simple Feature Geometry) 형태로 지리 벡터 데이터를 인코딩하는 표준화된 방법을 지원\nsf 패키지는 sp 패키지의 기능을 승계하였으며, 이에 더해 지리공간 데이터를 읽고 쓰는 ‘GDAL’, 지리적 연산을 할 때 사용하는 ‘GEOS’, 지도의 투영 변환(projection conversions)과 데이터 변환(datum transformations)을 위한 ‘PROJ’ 와 R과의 인터페이스를 제공\n선택적으로 지리적 좌표에 대한 구면 기하 연산 (spherical geometry operations) 을 위해 ‘s2’ 패키지를 사용\nsf 는 모든 벡터 유형(점,선,면, 다각형)을 지원함(raster는 지원하지 않음)\nsf 패키지의 장점\n\n지리공간 벡터 데이터를 빠르게 읽고 쓸 수 있음\n지리공간 벡터 데이터 시각화 성능의 고도화(tmap, leaflet, mapview 지리공간 데이터 시각화 패키지가 sf 클래스 지원)\n대부분의 연산에서 sf 객체는 DataFrame 처럼 처리가 가능함\nsf 함수들은 ‘%>%’ 연산자와 함께 사용할 수 있고, R의 tidyverse 패키지들과도 잘 작동함(sp 패키지도 spdplyr 패키지를 설치하면 dplyr의 %>% 체인 연산자와 기능을 사용할 수 있음)\nsf 함수이름은 ‘st_’ 로 시작하여 상대적으로 일관성이 있고 직관적임\n\n\n\n\n3.2 sf 패키지 확인\n\nvignetee(package=””) : 비니에트 함수는 설치된 모든 패키지에 대한 이용가능한 모든 목록을 출력\n\n\n# vignette(package = \"sf\") # 이용가능 목록 출력\n# vignette(\"sf1\")          # help창 (소개)\n\n\nworld 데이터셋은 spData에 의해 제공됨\n\n\nworld %>% head()\n\nSimple feature collection with 6 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -18.28799 xmax: 180 ymax: 83.23324\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 11\n  iso_a2 name_long conti…¹ regio…² subre…³ type  area_…⁴     pop lifeExp gdpPe…⁵\n  <chr>  <chr>     <chr>   <chr>   <chr>   <chr>   <dbl>   <dbl>   <dbl>   <dbl>\n1 FJ     Fiji      Oceania Oceania Melane… Sove…  1.93e4  8.86e5    70.0   8222.\n2 TZ     Tanzania  Africa  Africa  Easter… Sove…  9.33e5  5.22e7    64.2   2402.\n3 EH     Western … Africa  Africa  Northe… Inde…  9.63e4 NA         NA       NA \n4 CA     Canada    North … Americ… Northe… Sove…  1.00e7  3.55e7    82.0  43079.\n5 US     United S… North … Americ… Northe… Coun…  9.51e6  3.19e8    78.8  51922.\n6 KZ     Kazakhst… Asia    Asia    Centra… Sove…  2.73e6  1.73e7    71.6  23587.\n# … with 1 more variable: geom <MULTIPOLYGON [°]>, and abbreviated variable\n#   names ¹​continent, ²​region_un, ³​subregion, ⁴​area_km2, ⁵​gdpPercap\n\nnames(world)\n\n [1] \"iso_a2\"    \"name_long\" \"continent\" \"region_un\" \"subregion\" \"type\"     \n [7] \"area_km2\"  \"pop\"       \"lifeExp\"   \"gdpPercap\" \"geom\"     \n\nplot(world)\n\nWarning: plotting the first 9 out of 10 attributes; use max.plot = 10 to plot\nall\n\n\n\n\nworld_mini <- world[1:2, 1:3]\nworld_mini\n\nSimple feature collection with 2 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -18.28799 xmax: 180 ymax: -0.95\nGeodetic CRS:  WGS 84\n# A tibble: 2 × 4\n  iso_a2 name_long continent                                                geom\n  <chr>  <chr>     <chr>                                      <MULTIPOLYGON [°]>\n1 FJ     Fiji      Oceania   (((-180 -16.55522, -179.9174 -16.50178, -179.7933 …\n2 TZ     Tanzania  Africa    (((33.90371 -0.95, 31.86617 -1.02736, 30.76986 -1.…\n\n\n\n기존 sp에 사용되는 공간 데이터는 sf로 변환을 통해 사용 가능\n\nst_as_sf() : sf로 변환 하는 함수\n\n\n\nlibrary(sp)\n\nworld_sp <- as(world, Class = \"Spatial\")\nworld_sp %>% head()\n\n  iso_a2      name_long     continent region_un        subregion\n1     FJ           Fiji       Oceania   Oceania        Melanesia\n2     TZ       Tanzania        Africa    Africa   Eastern Africa\n3     EH Western Sahara        Africa    Africa  Northern Africa\n4     CA         Canada North America  Americas Northern America\n5     US  United States North America  Americas Northern America\n6     KZ     Kazakhstan          Asia      Asia     Central Asia\n               type    area_km2       pop  lifeExp gdpPercap\n1 Sovereign country    19289.97    885806 69.96000  8222.254\n2 Sovereign country   932745.79  52234869 64.16300  2402.099\n3     Indeterminate    96270.60        NA       NA        NA\n4 Sovereign country 10036042.98  35535348 81.95305 43079.143\n5           Country  9510743.74 318622525 78.84146 51921.985\n6 Sovereign country  2729810.51  17288285 71.62000 23587.338\n\n### sp → sf 변환 : geometry 정보, 컬럼 추가\nworld_sf <- st_as_sf(world_sp)\nworld_sf %>% head()\n\nSimple feature collection with 6 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -18.28799 xmax: 180 ymax: 83.23324\nGeodetic CRS:  WGS 84\n  iso_a2      name_long     continent region_un        subregion\n1     FJ           Fiji       Oceania   Oceania        Melanesia\n2     TZ       Tanzania        Africa    Africa   Eastern Africa\n3     EH Western Sahara        Africa    Africa  Northern Africa\n4     CA         Canada North America  Americas Northern America\n5     US  United States North America  Americas Northern America\n6     KZ     Kazakhstan          Asia      Asia     Central Asia\n               type    area_km2       pop  lifeExp gdpPercap\n1 Sovereign country    19289.97    885806 69.96000  8222.254\n2 Sovereign country   932745.79  52234869 64.16300  2402.099\n3     Indeterminate    96270.60        NA       NA        NA\n4 Sovereign country 10036042.98  35535348 81.95305 43079.143\n5           Country  9510743.74 318622525 78.84146 51921.985\n6 Sovereign country  2729810.51  17288285 71.62000 23587.338\n                        geometry\n1 MULTIPOLYGON (((-180 -16.55...\n2 MULTIPOLYGON (((33.90371 -0...\n3 MULTIPOLYGON (((-8.66559 27...\n4 MULTIPOLYGON (((-132.71 54....\n5 MULTIPOLYGON (((-171.7317 6...\n6 MULTIPOLYGON (((87.35997 49...\n\n\n\nplot 함수를 이용해서 기본 지도 만들기\n\n\nplot(world[3:6])\n\n\n\nplot(world[\"pop\"])\n\n\n\n\n\n다른 지도층을 추가하기\n\nplot() 함수 내에 add = TRUE 매개변수를 사용하면 나중에 그 위에 다른 지도를 겹쳐서, 즉 층을 추가하여 지도를 덮어쓰기로 그릴 수 있음\n단, 첫번째 지도 그래프에 키(key)가 있을 경우에는 reset = FALSE 매개변수를 꼭 설정해준 다음에, 이후에 다음번 plot(add = TRUE)를 사용\n\n\n\nworld_asia = world[world$continent == \"Asia\", ]\nasia = st_union(world_asia) #아시아 국가 합치기\n\n### 아시아만 빨간색으로 표시\nplot(world[\"pop\"], reset = FALSE) #reset = FLASE이면 지도 요소를 더 추가할 수 있는 모드로 플롯을 유지\nplot(asia, add = TRUE, col = \"red\")\n\n\n\n\n\n\n3.3 Base Plot arguments\n\n대륙별 중심점에 원을 덮어 씌우기\n\nst_centroid() : 폴리곤의 중심점을 계산하는 함수\nof_largest = TRUE : if TRUE, return centroid of the largest (sub)polygon of a MULTIPOLYGON rather than of the wholeMULTIPOLYGON\n\n\n\nplot(world[\"continent\"], reset = FALSE)\ncex = sqrt(world$pop) / 10000                         # pop변수에 제곱근을 취하고 1000으로 나누어서 지도 시각화를 위해 크기를 맞춤\nworld_cents = st_centroid(world, of_largest = TRUE)   # 다각형(국가별) 중앙점 계산\n\nWarning: st_centroid assumes attributes are constant over geometries\n\nplot(st_geometry(world_cents), add = TRUE, cex = cex) # 인구크기에 따라 대륙별 중앙점에 원그려넣기\n\n\n\n\n\n특정 나라를 중심으로 확장하여 주변 나라 표시하기\n\nlwd : 선굵기\nworld_asia[0] : 아시아에 대한 geometry column\nexpandBB : 각 방향으로 경계 상자를 확장(아래, 왼쪽, 위, 오른쪽)\n\n\n\nindia = world[world$name_long == \"India\", ]\nplot(st_geometry(india), expandBB = c(0, 0.2, 0.1, 1), col = \"gray\", lwd = 3)\nplot(world_asia[0], add = TRUE)"
  },
  {
    "objectID": "posts/Spatial_Info_Analysis Ch1/공간정보분석 CH2.html#geometry-types",
    "href": "posts/Spatial_Info_Analysis Ch1/공간정보분석 CH2.html#geometry-types",
    "title": "Spatial_Info_Analysis CH1",
    "section": "4 Geometry types",
    "text": "4 Geometry types\n\nsf 패키지에서 지원하는 17개의 geometry types이 있음"
  },
  {
    "objectID": "posts/Spatial_Info_Analysis Ch1/공간정보분석 CH2.html#simple-feature-geometriessfg",
    "href": "posts/Spatial_Info_Analysis Ch1/공간정보분석 CH2.html#simple-feature-geometriessfg",
    "title": "Spatial_Info_Analysis CH1",
    "section": "5 Simple feature geometries(sfg)",
    "text": "5 Simple feature geometries(sfg)\n\nsfg는 “Simple Feature Geometries”의 약어로, 공간 데이터의 기하학적 특성을 표현하는 방법 중 하나\nsfg는 지오메트리 객체를 나타내며, 일반적으로 점(Point), 선(Line), 면(Polygon) 등과 같은 기본 기하학적 요소를 포함\n\n예를 들어, 도로 네트워크를 나타내는 경우, 각 도로는 점과 선으로 구성된 지오메트리 객체로 표현됨\n\nR에서 simple feature geometry types\n\nA point:st_point()\nA linestring:st_linestring()\nA polygon:st_polygon()\nA multipoint:st_multipoint()\nA multilinestring:st_multilinestring()\nA multipolygon:st_multipolygon()\nA geometry collection:st_geometrycollection()\n\nsfg objects can be created from 3 base R data types:\n\n\nA numeric vector : a single point\nA matrix : a set of points, where each row represents a point, a multipoint or linestring\nA list : a collection of objects such as matrices, multilinestrings or geometry collections\n\n\n5.1 st_point()\n\nst_point(c(5, 2))                 # XY point\n\nPOINT (5 2)\n\nst_point(c(5, 2, 3))              # XYZ point\n\nPOINT Z (5 2 3)\n\nst_point(c(5, 2, 1), dim = \"XYM\") # XYM point\n\nPOINT M (5 2 1)\n\nst_point(c(5, 2, 3, 1))           # XYZM point\n\nPOINT ZM (5 2 3 1)\n\n\n\n\n5.2 multipoint (st_multipoint()) and linestring (st_linestring())\n\n### MULTIPOINT\nmultipoint_matrix <- rbind(c(5, 2), c(1, 3), c(3, 4), c(3, 2))\nst_multipoint(multipoint_matrix)\n\nMULTIPOINT ((5 2), (1 3), (3 4), (3 2))\n\n### LINESTRING\nlinestring_matrix <- rbind(c(1, 5), c(4, 4), c(4, 1), c(2, 2), c(3, 2))\nst_linestring(linestring_matrix)\n\nLINESTRING (1 5, 4 4, 4 1, 2 2, 3 2)\n\n\n\n\n5.3 list를 사용 : multilinestrings, (multi-)polygons and geometry collections\n\n## POLYGON\npolygon_list = list(rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5)))\nst_polygon(polygon_list)\n\nPOLYGON ((1 5, 2 2, 4 1, 4 4, 1 5))\n\n## POLYGON with a hole\npolygon_border = rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5))\npolygon_hole = rbind(c(2, 4), c(3, 4), c(3, 3), c(2, 3), c(2, 4))\npolygon_with_hole_list = list(polygon_border, polygon_hole)\nst_polygon(polygon_with_hole_list)\n\nPOLYGON ((1 5, 2 2, 4 1, 4 4, 1 5), (2 4, 3 4, 3 3, 2 3, 2 4))\n\n## MULTILINESTRING\nmultilinestring_list = list(rbind(c(1, 5), c(4, 4), c(4, 1), c(2, 2), c(3, 2)), \n                            rbind(c(1, 2), c(2, 4)))\nst_multilinestring((multilinestring_list))\n\nMULTILINESTRING ((1 5, 4 4, 4 1, 2 2, 3 2), (1 2, 2 4))\n\n## MULTIPOLYGON\nmultipolygon_list = list(list(rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5))),\n                         list(rbind(c(0, 2), c(1, 2), c(1, 3), c(0, 3), c(0, 2))))\nst_multipolygon(multipolygon_list)\n\nMULTIPOLYGON (((1 5, 2 2, 4 1, 4 4, 1 5)), ((0 2, 1 2, 1 3, 0 3, 0 2)))\n\n## GEOMETRYCOLLECTION\ngemetrycollection_list = list(st_multipoint(multipoint_matrix),\n                              st_linestring(linestring_matrix))\nst_geometrycollection(gemetrycollection_list)\n\nGEOMETRYCOLLECTION (MULTIPOINT ((5 2), (1 3), (3 4), (3 2)), LINESTRING (1 5, 4 4, 4 1, 2 2, 3 2))"
  },
  {
    "objectID": "posts/Spatial_Info_Analysis Ch1/공간정보분석 CH2.html#simple-feature-columnssfc",
    "href": "posts/Spatial_Info_Analysis Ch1/공간정보분석 CH2.html#simple-feature-columnssfc",
    "title": "Spatial_Info_Analysis CH1",
    "section": "6 Simple feature columns(sfc)",
    "text": "6 Simple feature columns(sfc)\n\n두 개의 지리특성(features)를 하나의 칼럼 객체로 합침\nSFC는 “Simple Feature Columns”의 약어로, 공간 데이터를 표현하는 방법 중 하나\nSFC는 일반적으로 지리 정보 시스템(GIS)에서 사용되며, 지도 및 공간 데이터를 저장하고 분석하는 데 사용됨\nSFC는 일반적으로 공간 데이터를 테이블 형태로 나타내며, 각 행은 하나의 공간 객체를 나타냄\n\n예를 들어, 도시의 경계를 포함하는 행정 구역 데이터를 저장할 때, 각 행은 구역의 이름, 인구, 경계 등을 포함하는 속성 데이터와 함께 구역의 경계를 나타내는 지오메트리 데이터를 포함\nsfc와 sfg는 공간 데이터를 다루는 데 사용되는 서로 다른 개념. sfc는 공간 데이터를 저장하고 관리하는 방법을 나타내며, sfg는 공간 데이터의 기하학적 특성을 표현하는 방법을 나타냄\nsfc 공간 데이터를 sfg공간 데이터로 변경할 때 st_sfc() 함수를 사용\n\n\n두개의 단순 지리특성 기하 점(2 sfg points)를 st_sfc() 함수로 한개의 단순 지리특성 칼럼(1 sfc)객체로 합치기\n\n\n\n### sfc POINT\npoint1 <- st_point(c(5, 2))\npoint1\n\nPOINT (5 2)\n\npoint2 <- st_point(c(1, 3))\npoint2\n\nPOINT (1 3)\n\npoints_sfc <- st_sfc(point1, point2)\npoints_sfc\n\nGeometry set for 2 features \nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1 ymin: 2 xmax: 5 ymax: 3\nCRS:           NA\n\n\nPOINT (5 2)\n\n\nPOINT (1 3)\n\n\n\n\n두개의 단순 지리특성 기하 면(2 sfg polygons)를 st_sfc() 함수로 한개의 단순 지리특성 칼럼(1 sfc) 객체로 합치기\n\n\nst_geometry_type() : 기하유형을 확인\n\n\n\n### sfc POLYGON\npolygon_list1 <- list(rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5)))\npolygon_list1\n\n[[1]]\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    2\n[3,]    4    1\n[4,]    4    4\n[5,]    1    5\n\npolygon1 <- st_polygon(polygon_list1)\npolygon1\n\nPOLYGON ((1 5, 2 2, 4 1, 4 4, 1 5))\n\npolygon_list2 <- list(rbind(c(0, 2), c(1, 2), c(1, 3), c(0, 3), c(0, 2)))\npolygon_list2\n\n[[1]]\n     [,1] [,2]\n[1,]    0    2\n[2,]    1    2\n[3,]    1    3\n[4,]    0    3\n[5,]    0    2\n\npolygon2 <- st_polygon(polygon_list2)\npolygon2\n\nPOLYGON ((0 2, 1 2, 1 3, 0 3, 0 2))\n\npolygon_sfc <- st_sfc(polygon1, polygon2)\npolygon_sfc\n\nGeometry set for 2 features \nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 0 ymin: 1 xmax: 4 ymax: 5\nCRS:           NA\n\n\nPOLYGON ((1 5, 2 2, 4 1, 4 4, 1 5))\n\n\nPOLYGON ((0 2, 1 2, 1 3, 0 3, 0 2))\n\nst_geometry_type(polygon_sfc)\n\n[1] POLYGON POLYGON\n18 Levels: GEOMETRY POINT LINESTRING POLYGON MULTIPOINT ... TRIANGLE\n\n### sfc MULTILINESTRING\nmultilinestring_list1 <- list(rbind(c(1, 5), c(4, 4), c(4, 1), c(2, 2), c(3, 2)), \n                             rbind(c(1, 2), c(2, 4)))\nmultilinestring_list1\n\n[[1]]\n     [,1] [,2]\n[1,]    1    5\n[2,]    4    4\n[3,]    4    1\n[4,]    2    2\n[5,]    3    2\n\n[[2]]\n     [,1] [,2]\n[1,]    1    2\n[2,]    2    4\n\nmultilinestring1 <- st_multilinestring((multilinestring_list1))\nmultilinestring1\n\nMULTILINESTRING ((1 5, 4 4, 4 1, 2 2, 3 2), (1 2, 2 4))\n\nmultilinestring_list2 <- list(rbind(c(2, 9), c(7, 9), c(5, 6), c(4, 7), c(2, 7)), \n                             rbind(c(1, 7), c(3, 8)))\nmultilinestring_list2\n\n[[1]]\n     [,1] [,2]\n[1,]    2    9\n[2,]    7    9\n[3,]    5    6\n[4,]    4    7\n[5,]    2    7\n\n[[2]]\n     [,1] [,2]\n[1,]    1    7\n[2,]    3    8\n\nmultilinestring2 <- st_multilinestring((multilinestring_list2))\nmultilinestring2\n\nMULTILINESTRING ((2 9, 7 9, 5 6, 4 7, 2 7), (1 7, 3 8))\n\nmultilinestring_sfc <- st_sfc(multilinestring1, multilinestring2)\nmultilinestring_sfc\n\nGeometry set for 2 features \nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 1 ymin: 1 xmax: 7 ymax: 9\nCRS:           NA\n\n\nMULTILINESTRING ((1 5, 4 4, 4 1, 2 2, 3 2), (1 ...\n\n\nMULTILINESTRING ((2 9, 7 9, 5 6, 4 7, 2 7), (1 ...\n\nst_geometry_type(multilinestring_sfc)\n\n[1] MULTILINESTRING MULTILINESTRING\n18 Levels: GEOMETRY POINT LINESTRING POLYGON MULTIPOINT ... TRIANGLE\n\n\n\n\n단순 지리특성 기하 점과 면을 st_sfc() 함수로 합쳐서 한개의 단순 지리특성 칼럼(1 sfc) 객체로 만들기\n\n\n\n### sfc GEOMETRY\npoint_multilinestring_sfc <- st_sfc(point1, multilinestring1)\npoint_multilinestring_sfc\n\nGeometry set for 2 features \nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 1 ymin: 1 xmax: 5 ymax: 5\nCRS:           NA\n\n\nPOINT (5 2)\n\n\nMULTILINESTRING ((1 5, 4 4, 4 1, 2 2, 3 2), (1 ...\n\nst_geometry_type(point_multilinestring_sfc)\n\n[1] POINT           MULTILINESTRING\n18 Levels: GEOMETRY POINT LINESTRING POLYGON MULTIPOINT ... TRIANGLE\n\n\n\nsfc 객체는 CRS(coordinate reference systems, 좌표계시스템) 에 대한 정보를 추가로 저장할 수 있음\n\n특정 CRS를 지정하기 위해 (a)epsg (SRID)또는(b)proj4string속성을 사용할수 있음\n\n\n\nst_crs(points_sfc)\n\nCoordinate Reference System: NA\n\n\n\n6.1 - 좌표계 정보를 추가하는 방법\n\n\nepsg 코드를 입력\n\nepsg 코드 장점\n\n짧아서 기억하기 쉬움\nEPSG : European Petroleum Survey Group, 지도 투영과 datums 에 대한 좌표계 정보 데이터베이스를 제공\n\nsfc 객체 내에 모든 geometries는 동일한 CRS를 가져야함\nepsg code 를 4326 로 설정\nEPSG:4326 → WGS84 경위도: GPS가 사용하는 좌표계\n\n서비스: 구글 지구(Google Earth)\n단위: 소수점 (decimal degrees)\n+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\n\n\n\n### EPSG definition\npoints_sfc_wgs <- st_sfc(point1, point2, crs = 4326)\nst_crs(points_sfc_wgs)\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\n\n\nPROJ.4 문자열을 직접 입력\n\nproj4string 장단점\n\n투사 유형이나 datum, 타원체 등의 다른 모수들을 구체화할 수 있는 유연성이 있음\n사용자가 구체화를 해야 하므로 길고 복잡하며 기억하기 어려움\nproj4string은 문자열 형식으로 저장되며, 일반적으로 PROJ.4 라이브러리에서 사용하는 형식과 호환됨\n이 문자열에는 좌표계의 이름, 중앙 메리디언, 기준 위도 및 경도, 원점 위도 및 경도, 스케일링 요소 등의 정보가 포함\n\n예를 들어, WGS 84 좌표계의 proj4string은 다음과 같이 표시됨\n\nproj4string을 변경하면 공간 데이터를 다른 좌표계로 변환 가능\n\nst_transform() 함수를 사용하여 다른 좌표계로 변환가능\n\n\n### PROJ4STRING definition\nst_sfc(point1, point2, crs = \"+proj=longlat +datum=WGS84 +no_defs\")\n\nGeometry set for 2 features \nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1 ymin: 2 xmax: 5 ymax: 3\nGeodetic CRS:  +proj=longlat +datum=WGS84 +no_defs\n\n\nPOINT (5 2)\n\n\nPOINT (1 3)"
  },
  {
    "objectID": "posts/Spatial_Info_Analysis Ch1/공간정보분석 CH2.html#sf-class",
    "href": "posts/Spatial_Info_Analysis Ch1/공간정보분석 CH2.html#sf-class",
    "title": "Spatial_Info_Analysis CH1",
    "section": "7 sf class",
    "text": "7 sf class\n\n위의 위치데이터에 속성데이터(이름, 특정 값, 그룹 등)를 추가\n아래 예시는 2017년 6월 21일 런던의 25°C 온도를 나타냄\na geometry (the coordinates), and three attributes with three different classes (place name, temperature and date)\nsimple feature geometry column (sfc )에 속성(data.frame)을 나타내는 sf(simple features)의 calss를 합침\nst_sf() 를 이용하여 sfc와 class sf의 객체들을 하나로 통합할 수 있음\n\n\nlnd_point <- st_point(c(0.1, 51.5)) \nlnd_point                                           # sfg object\n\nPOINT (0.1 51.5)\n\nlnd_geom <- st_sfc(lnd_point, crs = 4326)           # sfc object\nlnd_geom\n\nGeometry set for 1 feature \nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 0.1 ymin: 51.5 xmax: 0.1 ymax: 51.5\nGeodetic CRS:  WGS 84\n\n\nPOINT (0.1 51.5)\n\nlnd_attrib <- data.frame(                           # data.frame object\n  name = \"London\",\n  temperature = 25,\n  date = as.Date(\"2017-06-21\"))\nlnd_attrib\n\n    name temperature       date\n1 London          25 2017-06-21\n\nlnd_sf <- st_sf(lnd_attrib, geometry = lnd_geom)    # sf object\nlnd_sf\n\nSimple feature collection with 1 feature and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 0.1 ymin: 51.5 xmax: 0.1 ymax: 51.5\nGeodetic CRS:  WGS 84\n    name temperature       date         geometry\n1 London          25 2017-06-21 POINT (0.1 51.5)\n\nlnd_sf %>% class()\n\n[1] \"sf\"         \"data.frame\"\n\n\n\nsfg (simple feature geometry) 를 만듬\nCRS(좌표계시스템)를 가지는 sfc (simple feature geometry column)으로 전환\nst_sf() 를 이용하여data.frame 에 저장된 속성 정보와 sfc 를 통합"
  },
  {
    "objectID": "posts/Spatial_Info_Analysis Ch1/공간정보분석 CH2.html#래스터-raster-data",
    "href": "posts/Spatial_Info_Analysis Ch1/공간정보분석 CH2.html#래스터-raster-data",
    "title": "Spatial_Info_Analysis CH1",
    "section": "8 래스터 (Raster) data",
    "text": "8 래스터 (Raster) data\n\n지리적 레스터 데이터 모델은 래스터 헤더와 일반적으로 동일한 간격의 셀(픽셀)을 나타내는 matrix로 구성됨\n\nRaster header : 좌표 참조 시스템(CRS, Coordinate Reference System), 시작점(the origin)과 범위 (the extent)를 정의함\n\n헤더는 열 수, 행 수 및 셀 크기 해상도를 통해 범위를 정의\n셀의 ID를 사용하여 각 단일 셀에 쉽게 접근하고 수정\n\n행렬 (matrix) : 동일한 크기의 픽셀 또는 셀(pixel, or cell)을 표현. 픽셀 ID(pixel IDs)와 픽셀 값(pixel values)\n\n원점(또는 시작점)은 종종 행렬의 왼쪽 아래 모서리 좌표(R의 래스터 패키지는 기본적으로 왼쪽 위 모서리를 사용)\n래스터 레이어의 셀에는 단일 값(숫자 또는 범주)만 포함"
  },
  {
    "objectID": "posts/Spatial_Info_Analysis Ch1/공간정보분석 CH2.html#an-introduction-to-raster",
    "href": "posts/Spatial_Info_Analysis Ch1/공간정보분석 CH2.html#an-introduction-to-raster",
    "title": "Spatial_Info_Analysis CH1",
    "section": "9 An introduction to raster",
    "text": "9 An introduction to raster\n\nraster package는 R에서 raster objects을 만들고, 읽고, 내보내고, 조작 및 처리하기 위한 광범위한 기능을 제공\n래스터 개념을 설명하기 위해 spDataLarge의 데이터 세트를 사용\nZion National Park(미국 유타) 지역을 덮는 몇 개의 래스터 개체와 하나의 벡터 개체로 구성\nsrtm.tif은 이 지역의 디지털 표고 모델\n\n\n#install.packages(\"rgdal\") \nlibrary(rgdal)\n\n#install.packages(\"spDataLarge\",repos = \"https://nowosad.github.io/drat/\",type = \"source\") #지리공간 데이터 샘플을 내장\n#install.packages(\"raster\")\n\nlibrary(spDataLarge)\nlibrary(raster)\n\nraster_filepath <- system.file(\"raster/srtm.tif\", package = \"spDataLarge\") \nraster_filepath\n\n[1] \"C:/Users/seong taek/AppData/Local/R/win-library/4.2/spDataLarge/raster/srtm.tif\"\n\nnew_raster <- raster(raster_filepath)\nnew_raster\n\nclass      : RasterLayer \ndimensions : 457, 465, 212505  (nrow, ncol, ncell)\nresolution : 0.0008333333, 0.0008333333  (x, y)\nextent     : -113.2396, -112.8521, 37.13208, 37.51292  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : srtm.tif \nnames      : srtm \nvalues     : 1024, 2892  (min, max)\n\n# 클래스(class)\n# 차원(dimentions)\n# 해상도(resolution)\n# 범위(extent)\n# 좌표 참조 시스템 (Coordinates Reference System)\n# 출처(Source)\n# 이름(names)\n# 최소/최대 값(min, max values) 속성 정보\n\n### rgdal 설치 error발생시 rgdal 설치및로드 필요\n\n\n9.1 Functions\n\ndim(): returns the number of rows, columns and layers\nncell(): function the number of cells (pixels)\nres(): the raster’s spatial resolution\nextent(): its spatial extent\ncrs(): coordinate reference system\ninMemory(): reports whether the raster data is stored in memory (the default) or on disk\n\n\ndim(new_raster)\n\n[1] 457 465   1\n\nncell(new_raster)\n\n[1] 212505\n\nextent(new_raster)\n\nclass      : Extent \nxmin       : -113.2396 \nxmax       : -112.8521 \nymin       : 37.13208 \nymax       : 37.51292 \n\ncrs(new_raster)\n\nCoordinate Reference System:\nDeprecated Proj.4 representation: +proj=longlat +datum=WGS84 +no_defs \nWKT2 2019 representation:\nGEOGCRS[\"unknown\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ID[\"EPSG\",6326]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433],\n        ID[\"EPSG\",8901]],\n    CS[ellipsoidal,2],\n        AXIS[\"longitude\",east,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433,\n                ID[\"EPSG\",9122]]],\n        AXIS[\"latitude\",north,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433,\n                ID[\"EPSG\",9122]]]] \n\ninMemory(new_raster)\n\n[1] FALSE\n\n# help(\"raster-package\")"
  },
  {
    "objectID": "posts/Spatial_Info_Analysis Ch1/공간정보분석 CH2.html#basic-map-making",
    "href": "posts/Spatial_Info_Analysis Ch1/공간정보분석 CH2.html#basic-map-making",
    "title": "Spatial_Info_Analysis CH1",
    "section": "10 Basic map making",
    "text": "10 Basic map making\n\nsf package와 같이 raster 역시plot() 함수 사용 가능\n\n\nplot(new_raster)"
  },
  {
    "objectID": "posts/Spatial_Info_Analysis Ch1/공간정보분석 CH2.html#raster-classes",
    "href": "posts/Spatial_Info_Analysis Ch1/공간정보분석 CH2.html#raster-classes",
    "title": "Spatial_Info_Analysis CH1",
    "section": "11 Raster classes",
    "text": "11 Raster classes\n3가지의레스터 클래스(Raster Classes)의 특장점\n(1) RasterLayer class\n(2) RasterBrick class\n(3) RasterStack class\n\n11.1 1. RasterLayer class\n\nRasterLayer class는 래스터 객체 중에서 가장 간단한 형태의 클래스이며, 한개의 층으로 구성되어 있음\nRasterLayer Class 객체를 만드는 가장 쉬운 방법은 기존의 RasterLayer Class 객체 파일을 읽어오는 것\n\n아래 예에서는 raster 패키지의 raster() 함수를 사용해서 spDataLarge 패키지에 내장되어 있는 srtm.tif 레스터 층 클래스 객체를 읽어와서 raster_layer 라는 이름의 단 한개의 층만을 가진 RasterLayer Class 객체를 만듬\nnlayers() 함수로 층의 개수를 살펴보면 ’1’개 인 것을 확인할 수 있습니다.\n\n\n\nraster_filepath <- system.file(\"raster/srtm.tif\", package = \"spDataLarge\")\nraster_filepath\n\n[1] \"C:/Users/seong taek/AppData/Local/R/win-library/4.2/spDataLarge/raster/srtm.tif\"\n\nnew_raster <- raster(raster_filepath)\nnew_raster\n\nclass      : RasterLayer \ndimensions : 457, 465, 212505  (nrow, ncol, ncell)\nresolution : 0.0008333333, 0.0008333333  (x, y)\nextent     : -113.2396, -112.8521, 37.13208, 37.51292  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : srtm.tif \nnames      : srtm \nvalues     : 1024, 2892  (min, max)\n\n### number of layers \nnlayers(new_raster)\n\n[1] 1\n\n\n\nRasterLayer 클래스 객체를 raster() 함수를 사용해서 처음부터 직접 만들 수도 있음\n\n8개의 행과 8개의 열, 총 64개의 셀(픽셀)을 가진 RasterLayer 클래스를 직접 만들기\n레스터 객체의 좌표 참조 시스템(CRS, Coordinates Reference System)은 WGS84 가 기본 설정값(해상도(resolution)의 단위가 도 (in degrees))\nres = 0.5 로서 해상도를 0.5도로 설정\n각 셀의 값은 왼쪽 상단부터 시작하여, 행 방향(row-wise)으로 왼쪽에서 오른쪽으로 채워짐\n\n\n\nmy_raster <- raster(nrows = 8, ncols = 8, res = 0.5, \n                   xmn = -2.0, xmx = 2.0, ymn = -2.0, ymx = 2.0, vals = 1:64)\nmy_raster\n\nclass      : RasterLayer \ndimensions : 8, 8, 64  (nrow, ncol, ncell)\nresolution : 0.5, 0.5  (x, y)\nextent     : -2, 2, -2, 2  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : memory\nnames      : layer \nvalues     : 1, 64  (min, max)\n\n## plotting \nplot(my_raster, main = \"my raster (64 cells = 8 rows * 8 cols)\")           \n\n\n\n\n\n\n11.2 2. RasterBrick class\n\nRasterBrickandRasterStack 클래스는 여러 개의 층(multiple layers)을 가질 수 있음\n특히, RasterBrick 클래스는 단일 다중 스펙트럼 위성 파일 (a single multispectral satellite file) 이나 또는 메모리의 단일 다층 객체 (a single multilayer object in memory)의 형태로 다층의 레스터 객체를 구성\n아래의 예는 raster 패키지의 brick()함수를 사용해서 spDataLarge 패키지에 들어있는 landsat.tif 의 다층 레스터 파일을 RasterBrick클래스 객체로 불러온 것\n\nnlayers() : the number of layers stored in a Raster*  object\n\n\n\nmulti_raster_file <- system.file(\"raster/landsat.tif\", package = \"spDataLarge\")\nmulti_raster_file\n\n[1] \"C:/Users/seong taek/AppData/Local/R/win-library/4.2/spDataLarge/raster/landsat.tif\"\n\nr_brick <- brick(multi_raster_file)\nr_brick\n\nclass      : RasterBrick \ndimensions : 1428, 1128, 1610784, 4  (nrow, ncol, ncell, nlayers)\nresolution : 30, 30  (x, y)\nextent     : 301905, 335745, 4111245, 4154085  (xmin, xmax, ymin, ymax)\ncrs        : +proj=utm +zone=12 +datum=WGS84 +units=m +no_defs \nsource     : landsat.tif \nnames      : landsat_1, landsat_2, landsat_3, landsat_4 \nmin values :      7550,      6404,      5678,      5252 \nmax values :     19071,     22051,     25780,     31961 \n\nnlayers(r_brick)\n\n[1] 4\n\nplot(r_brick) #plotting RasterBrick object with 4 layers\n\n\n\n\n\n\n11.3 3. RasterStack class\n\n다 층 (multi-layers) 레스터 객체로 구성\n같은 범위와 해상도를 가진 여러개의 RasterLayer 클래스 객체들을 리스트로 묶어서 RasterStack 클래스 객체를 만듬\nRasterBrick클래스가 동일한 복수개의 RasterLayer층으로 구성되는 반면에, RasterStack클래스는 여러개의 RasterLayer와RasterBrick 클래스 객체가 혼합되어서 구성할 수 있음\n연산 속도면에서 보면 일반적으로 RasterBrick 클래스가 RasterStack 클래스보다 빠름\n아래 예시\n\n\nraster(raster_brick, layer = 1) 함수를 사용해서 위에서 불러왔던 RasterBrick 클래스 객체의 1번째 층만 가져다가 raster_on_disk 라는 이름으로 레스터 객체를 하나 만듬\n\n\nraster() 함수로 동일한 범위와 해상도, 좌표 참조 시스템(CRS)를 가지고 난수로 셀의 값을 채운 raster_in_memory 라는 이름의 메모리에 있는 RasterLayer 클래스 객체를 만듬\n\n\nseq_len(n) : 1부터 n까지 입력(1씩 커짐)\n\n다음에 stac() 함수로 raster_stack = stack(raster_in_memory, raster_on_disk) 처럼 (a) + (b) 하여 쌓아서 raster_stack 라는 이름의 RasterStack 클래스 객체를 만듬\n마지막으로 plot() 함수로 RasterStack 클래스 객체에 쌓여 있는 2개의 객체를 시각화 (raster_in_memory 는 난수를 발생시켜 셀 값을 채웠기 때문에 시각화했을 때 아무런 패턴이 없음)\n\n\n\nraster_on_disk <- raster(r_brick, layer = 1)\nraster_on_disk\n\nclass      : RasterLayer \nband       : 1  (of  4  bands)\ndimensions : 1428, 1128, 1610784  (nrow, ncol, ncell)\nresolution : 30, 30  (x, y)\nextent     : 301905, 335745, 4111245, 4154085  (xmin, xmax, ymin, ymax)\ncrs        : +proj=utm +zone=12 +datum=WGS84 +units=m +no_defs \nsource     : landsat.tif \nnames      : landsat_1 \nvalues     : 7550, 19071  (min, max)\n\nraster_in_memory = raster(xmn = 301905, xmx = 335745,\n                          ymn = 4111245, ymx = 4154085, \n                          res = 30)\nraster_in_memory\n\nclass      : RasterLayer \ndimensions : 1428, 1128, 1610784  (nrow, ncol, ncell)\nresolution : 30, 30  (x, y)\nextent     : 301905, 335745, 4111245, 4154085  (xmin, xmax, ymin, ymax)\ncrs        : NA \n\nvalues(raster_in_memory) <- sample(seq_len(ncell(raster_in_memory)))\n\ncrs(raster_in_memory) = crs(raster_on_disk) #같은 좌표 입력\n\nr_stack <- stack(raster_in_memory, raster_on_disk)\nr_stack\n\nclass      : RasterStack \ndimensions : 1428, 1128, 1610784, 2  (nrow, ncol, ncell, nlayers)\nresolution : 30, 30  (x, y)\nextent     : 301905, 335745, 4111245, 4154085  (xmin, xmax, ymin, ymax)\ncrs        : +proj=utm +zone=12 +datum=WGS84 +units=m +no_defs \nnames      :   layer, landsat_1 \nmin values :       1,      7550 \nmax values : 1610784,     19071 \n\nplot(r_stack)\n\n\n\n\n\n\n11.4 언제 어떤 래스터 클래스를 사용하는 것이 좋은가?\n\n하나의 다층 레스터 파일이나 객체(a single multilayer file or object)를 처리하는 것이라면 RasterBrick 이 적합\n반면에, 여러개의 래스터 파일들(many files)이나 여러 종류의 레스터 클래스를 한꺼번에 연결해서 연산하고 처리해야 하는 경우라면 RasterStack Class 가 적합"
  },
  {
    "objectID": "posts/Spatial_Info_Analysis Ch1/공간정보분석 CH2.html#coordinate-reference-systems",
    "href": "posts/Spatial_Info_Analysis Ch1/공간정보분석 CH2.html#coordinate-reference-systems",
    "title": "Spatial_Info_Analysis CH1",
    "section": "12 Coordinate Reference Systems",
    "text": "12 Coordinate Reference Systems"
  },
  {
    "objectID": "posts/Spatial_Info_Analysis Ch1/공간정보분석 CH2.html#crscoordinate-reference-systems",
    "href": "posts/Spatial_Info_Analysis Ch1/공간정보분석 CH2.html#crscoordinate-reference-systems",
    "title": "Spatial_Info_Analysis CH1",
    "section": "13 CRS(Coordinate Reference Systems)",
    "text": "13 CRS(Coordinate Reference Systems)\n\n지리 공간 데이터 분석에서 가장 기본이 되고 또 처음에 확인을 해보아야 하는 좌표계, 좌표 참조 시스템(CRS, Coordinate Reference Systems)에 대한 소개\n\n지리 좌표계 (Geographic Coordinate Reference Systems)\n투영(투사) 좌표계 (Projected Coordinate Reference Systems)"
  },
  {
    "objectID": "posts/Spatial_Info_Analysis Ch1/공간정보분석 CH2.html#crs-in-r",
    "href": "posts/Spatial_Info_Analysis Ch1/공간정보분석 CH2.html#crs-in-r",
    "title": "Spatial_Info_Analysis CH1",
    "section": "14 CRS in R",
    "text": "14 CRS in R\n\nR에서 좌표계를 표현할 때는 (a) epsg 코드 (epsg code)나 또는 (b)proj4string 정의 (proj4string definition)를 사용\nR에서 CRS를 설명하는 두 가지 주요 방법은(a) epsg코드 또는(b)proj4string정의\nepsg코드\n\n일반적으로 더 짧으므로 기억하기 쉬움\n또한 이 코드는 잘 정의된 좌표 참조 시스템을 하나만 참조\n\nproj4string정의\n\n투영 유형, 데이텀 및 타원체와 같은 다양한 매개변수를 지정할 때 더 많은 유연성을 얻을 수 있음\n다양한 투영을 지정하고 기존 투영을 수정할 수 있음 (이것은 또한 proj4string접근 방식을 더 복잡하게 만듬)\n\n벡터 데이터의 좌표계\n\n벡터 지리 데이터에 대해서는 sf 패키지의 ****st_crs()함수를 사용해서 좌표계를 확인\nspDataLarge 패키지에 들어있는 Zion 국립 공원의 경계를 다각형면(Polygon)으로 나타내는 zion.gpkg 벡터 데이터를 st_read() 함수로 불러와서, st_crs()함수로 좌표계를 조회\n\n\n\nlibrary(sf)\n\nvector_filepath <- system.file(\"vector/zion.gpkg\", package = \"spDataLarge\")\nvector_filepath\n\n[1] \"C:/Users/seong taek/AppData/Local/R/win-library/4.2/spDataLarge/vector/zion.gpkg\"\n\nnew_vector <- st_read(vector_filepath)\n\nReading layer `zion' from data source \n  `C:\\Users\\seong taek\\AppData\\Local\\R\\win-library\\4.2\\spDataLarge\\vector\\zion.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1 feature and 11 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 302903.1 ymin: 4112244 xmax: 334735.5 ymax: 4153087\nProjected CRS: UTM Zone 12, Northern Hemisphere\n\nnew_vector\n\nSimple feature collection with 1 feature and 11 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 302903.1 ymin: 4112244 xmax: 334735.5 ymax: 4153087\nProjected CRS: UTM Zone 12, Northern Hemisphere\n  UNIT_CODE\n1      ZION\n                                                                            GIS_Notes\n1 Lands - http://landsnet.nps.gov/tractsnet/documents/ZION/Metadata/zion_metadata.xml\n           UNIT_NAME  DATE_EDIT STATE REGION GNIS_ID     UNIT_TYPE CREATED_BY\n1 Zion National Park 2017-06-22    UT     IM 1455157 National Park      Lands\n                                                                   METADATA\n1 https://irma.nps.gov/App/Reference/Profile/2181118#Zion National Monument\n  PARKNAME                           geom\n1     Zion POLYGON ((314945.2 4115910,...\n\n## st_read() : read vector dataset in R sf package\n\nst_crs(new_vector) # get CRS\n\nCoordinate Reference System:\n  User input: UTM Zone 12, Northern Hemisphere \n  wkt:\nBOUNDCRS[\n    SOURCECRS[\n        PROJCRS[\"UTM Zone 12, Northern Hemisphere\",\n            BASEGEOGCRS[\"GRS 1980(IUGG, 1980)\",\n                DATUM[\"unknown\",\n                    ELLIPSOID[\"GRS80\",6378137,298.257222101,\n                        LENGTHUNIT[\"metre\",1,\n                            ID[\"EPSG\",9001]]]],\n                PRIMEM[\"Greenwich\",0,\n                    ANGLEUNIT[\"degree\",0.0174532925199433]]],\n            CONVERSION[\"UTM zone 12N\",\n                METHOD[\"Transverse Mercator\",\n                    ID[\"EPSG\",9807]],\n                PARAMETER[\"Latitude of natural origin\",0,\n                    ANGLEUNIT[\"degree\",0.0174532925199433],\n                    ID[\"EPSG\",8801]],\n                PARAMETER[\"Longitude of natural origin\",-111,\n                    ANGLEUNIT[\"degree\",0.0174532925199433],\n                    ID[\"EPSG\",8802]],\n                PARAMETER[\"Scale factor at natural origin\",0.9996,\n                    SCALEUNIT[\"unity\",1],\n                    ID[\"EPSG\",8805]],\n                PARAMETER[\"False easting\",500000,\n                    LENGTHUNIT[\"Meter\",1],\n                    ID[\"EPSG\",8806]],\n                PARAMETER[\"False northing\",0,\n                    LENGTHUNIT[\"Meter\",1],\n                    ID[\"EPSG\",8807]],\n                ID[\"EPSG\",16012]],\n            CS[Cartesian,2],\n                AXIS[\"(E)\",east,\n                    ORDER[1],\n                    LENGTHUNIT[\"Meter\",1]],\n                AXIS[\"(N)\",north,\n                    ORDER[2],\n                    LENGTHUNIT[\"Meter\",1]]]],\n    TARGETCRS[\n        GEOGCRS[\"WGS 84\",\n            DATUM[\"World Geodetic System 1984\",\n                ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                    LENGTHUNIT[\"metre\",1]]],\n            PRIMEM[\"Greenwich\",0,\n                ANGLEUNIT[\"degree\",0.0174532925199433]],\n            CS[ellipsoidal,2],\n                AXIS[\"latitude\",north,\n                    ORDER[1],\n                    ANGLEUNIT[\"degree\",0.0174532925199433]],\n                AXIS[\"longitude\",east,\n                    ORDER[2],\n                    ANGLEUNIT[\"degree\",0.0174532925199433]],\n            ID[\"EPSG\",4326]]],\n    ABRIDGEDTRANSFORMATION[\"Transformation from GRS 1980(IUGG, 1980) to WGS84\",\n        METHOD[\"Position Vector transformation (geog2D domain)\",\n            ID[\"EPSG\",9606]],\n        PARAMETER[\"X-axis translation\",0,\n            ID[\"EPSG\",8605]],\n        PARAMETER[\"Y-axis translation\",0,\n            ID[\"EPSG\",8606]],\n        PARAMETER[\"Z-axis translation\",0,\n            ID[\"EPSG\",8607]],\n        PARAMETER[\"X-axis rotation\",0,\n            ID[\"EPSG\",8608]],\n        PARAMETER[\"Y-axis rotation\",0,\n            ID[\"EPSG\",8609]],\n        PARAMETER[\"Z-axis rotation\",0,\n            ID[\"EPSG\",8610]],\n        PARAMETER[\"Scale difference\",1,\n            ID[\"EPSG\",8611]]]]\n\n\n\n좌표계가 비어있거나 잘못 입력되어 있는 경우 **st_set_crs(vector_object,EPSG code)** 구문으로 좌표계를 설정할수 있음\n**st_set_crs()** 함수는 좌표계를 변경하는 것이 투영 데이터를 변환하는 것은 아니며, 투영 데이터 변환을 하려면 st_transform() 함수를 이용\n\n\n## -- st_set_crs() : setting a CRS (coordinate reference system) \nnew_vector_2 <- st_set_crs(new_vector, 4326) # set CRS with EPSG 4326 code\n\nWarning: st_crs<- : replacing crs does not reproject data; use st_transform for\nthat\n\nnew_vector_2\n\nSimple feature collection with 1 feature and 11 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 302903.1 ymin: 4112244 xmax: 334735.5 ymax: 4153087\nGeodetic CRS:  WGS 84\n  UNIT_CODE\n1      ZION\n                                                                            GIS_Notes\n1 Lands - http://landsnet.nps.gov/tractsnet/documents/ZION/Metadata/zion_metadata.xml\n           UNIT_NAME  DATE_EDIT STATE REGION GNIS_ID     UNIT_TYPE CREATED_BY\n1 Zion National Park 2017-06-22    UT     IM 1455157 National Park      Lands\n                                                                   METADATA\n1 https://irma.nps.gov/App/Reference/Profile/2181118#Zion National Monument\n  PARKNAME                           geom\n1     Zion POLYGON ((314945.2 4115910,...\n\n# 경고 메세지 확인\n\n\n래스터 데이터에서 좌표계\n\n레스터 모델의 객체에 대해서는 raster 패키지의 projection()  함수를 사용해서 좌표계를 확인하거나 설정\n\n\n\n## -- raster::projection() : get or set CRS in raster* objects \nlibrary(raster) \n\nraster_filepath = system.file(\"raster/srtm.tif\", package = \"spDataLarge\") \nraster_filepath\n\n[1] \"C:/Users/seong taek/AppData/Local/R/win-library/4.2/spDataLarge/raster/srtm.tif\"\n\nnew_raster = raster(raster_filepath)\nnew_raster\n\nclass      : RasterLayer \ndimensions : 457, 465, 212505  (nrow, ncol, ncell)\nresolution : 0.0008333333, 0.0008333333  (x, y)\nextent     : -113.2396, -112.8521, 37.13208, 37.51292  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : srtm.tif \nnames      : srtm \nvalues     : 1024, 2892  (min, max)\n\nprojection(new_raster) # get CRS in raster objects # [1] \"+proj=longlat +datum=WGS84 +no_defs\"\n\n[1] \"+proj=longlat +datum=WGS84 +no_defs\"\n\n\n\n레스터 데이터에 대해서 좌표계를 새로 설정할 때도 역시 projection()함수를 사용\n\n\nnew_raster3  <-  new_raster\nnew_raster3\n\nclass      : RasterLayer \ndimensions : 457, 465, 212505  (nrow, ncol, ncell)\nresolution : 0.0008333333, 0.0008333333  (x, y)\nextent     : -113.2396, -112.8521, 37.13208, 37.51292  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : srtm.tif \nnames      : srtm \nvalues     : 1024, 2892  (min, max)\n\nprojection(new_raster3) <-  \"+proj=utm +zone=12 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 \n                            +units=m +no_defs\" # set CRS\nnew_raster3\n\nclass      : RasterLayer \ndimensions : 457, 465, 212505  (nrow, ncol, ncell)\nresolution : 0.0008333333, 0.0008333333  (x, y)\nextent     : -113.2396, -112.8521, 37.13208, 37.51292  (xmin, xmax, ymin, ymax)\ncrs        : +proj=utm +zone=12 +ellps=GRS80 +units=m +no_defs \nsource     : srtm.tif \nnames      : srtm \nvalues     : 1024, 2892  (min, max)\n\n\n\n벡터 데이터의 경우 좌표계를 설정할 때 ‘EPSG 코드’나 ’Proj4string 정의’ 모두 사용 가능한 반면에, 레스터 데이터는 ’Proj4string 정의’만 사용\n중요한 것은 st_crs()및 projection()함수는 좌표의 값이나 지오메트리를 변경하지 않음"
  },
  {
    "objectID": "posts/Spatial_Info_Analysis Ch1/공간정보분석 CH2.html#unit단위",
    "href": "posts/Spatial_Info_Analysis Ch1/공간정보분석 CH2.html#unit단위",
    "title": "Spatial_Info_Analysis CH1",
    "section": "15 Unit(단위)",
    "text": "15 Unit(단위)\n\n좌표계 (CRS) 정보 안에 들어있는 공간의 단위 (Spatial Units)\n지도를 제작하거나 볼 때 측정 단위 (measurement units)가 미터(meters) 인지 혹은 피트(feets) 인지 명시적으로 표현하고 정확하게 확인할 필요\n벡터의 지리적 데이터나 레스터의 픽셀에서 측정되는 단위라는 맥락(context)를 알 수 있고, 실제 지표면과 지도 표현 간의 관계, 거리를 알 수 있고, 또 거리나 면적 등을 계산할 수 있음\n\n\n지리공간 벡터 데이터의 측정 단위(Units in Vector data)\n\n\nsf 객체의 지리공간 벡터 데이터는 단위에 대해서 native support 이여서, 다른 외부 모듈이나 확장 프로그램을 설치하지 않아도 sf 객체 내에 단위가 들어가 있음\n그래서 sf 객체 벡터 데이터에 대해서 연산을 하게 되면 units 패키지에 의해 정의된 “단위 속성”도 같이 반환해주어서 단위로 인한 혼란을 미연에 방지할 수 있음(대부분의 좌표계는 미터(meters)를 사용하지만, 일부는 피트(feets)를 사용하기 때문에 단위가 혼란스러울 수 있음. raster 패키지는 단위가 native support가 아님.)\n\nR의 spData 패키지에 들어있는 “world” 데이터셋을 활용하여 Luxembourgd와 대한민국의 벡터 데이터를 가져와서, st_area()함수로 면적을 계산\nsf 패키지의 st_area() 함수로 벡터 데이터의 면적으로 계산 하면, 결과값의 뒤에 [m^2] 이라고 해서 2차원 공간 상의 “제곱미터” 단위가 같이 반환\n\n\n\nlibrary(spData)\n\nnames(world)\n\n [1] \"iso_a2\"    \"name_long\" \"continent\" \"region_un\" \"subregion\" \"type\"     \n [7] \"area_km2\"  \"pop\"       \"lifeExp\"   \"gdpPercap\" \"geom\"     \n\nluxembourg = world[world$name_long == \"Luxembourg\", ]\nluxembourg\n\nSimple feature collection with 1 feature and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 5.674052 ymin: 49.44267 xmax: 6.242751 ymax: 50.12805\nGeodetic CRS:  WGS 84\n# A tibble: 1 × 11\n  iso_a2 name_long  conti…¹ regio…² subre…³ type  area_…⁴    pop lifeExp gdpPe…⁵\n  <chr>  <chr>      <chr>   <chr>   <chr>   <chr>   <dbl>  <dbl>   <dbl>   <dbl>\n1 LU     Luxembourg Europe  Europe  Wester… Sove…   2417. 556319    82.2  93655.\n# … with 1 more variable: geom <MULTIPOLYGON [°]>, and abbreviated variable\n#   names ¹​continent, ²​region_un, ³​subregion, ⁴​area_km2, ⁵​gdpPercap\n\nsouth_korea = world[world$name_long == \"Republic of Korea\", ] \nsouth_korea\n\nSimple feature collection with 1 feature and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 126.1174 ymin: 34.39005 xmax: 129.4683 ymax: 38.61224\nGeodetic CRS:  WGS 84\n# A tibble: 1 × 11\n  iso_a2 name_long  conti…¹ regio…² subre…³ type  area_…⁴    pop lifeExp gdpPe…⁵\n  <chr>  <chr>      <chr>   <chr>   <chr>   <chr>   <dbl>  <dbl>   <dbl>   <dbl>\n1 KR     Republic … Asia    Asia    Easter… Sove…  99044. 5.07e7    81.7  33426.\n# … with 1 more variable: geom <MULTIPOLYGON [°]>, and abbreviated variable\n#   names ¹​continent, ²​region_un, ³​subregion, ⁴​area_km2, ⁵​gdpPercap\n\nplot(south_korea)\n\n\n\nplot(south_korea[1])\n\n\n\nst_area(luxembourg) \n\n2408817306 [m^2]\n\nst_area(south_korea)\n\n99020196082 [m^2]\n\n\n\n면적 단위가 [m^2] 이다보니 결과값의 자리수가 너무 길게 표현됨\n계산의 단위를 “제곱킬로미터[km^2]로 변경하려면 units 패키지의 set_units(st_object, units) 함수로 단위를 설정할 수 있음\n기존의 면적 단위인 ‘제곱미터(m^2)’ 로 계산된 결과값을 1,000,000 으로 나누게 되면 결과값은 맞더라도 단위가 ‘제곱미터(m^2)’ 로 그대로여서, 우리가 원하던 단위인 ‘제곱킬로미터(km^2)’ 가 아니게 되므로 주의가 필요\n\n\n지리공간 래스터 데이터의 측정 단위(Units in Raster data)\n\n\n벡터 데이터를 다루는 sf 패키지는 단위가 native support 여서 조회나 계산 결과를 반환할 때 단위(units)를 속성으로 반환\n하지만 레스터 데이터를 다루는 raster 패키지는 단위에 대해서 native support 가 아니므로, 단위에 대해서 혼란스러울 수 있으므로 조심해야 함\n\nspDataLarge 패키지에 들어있는 strm.tif 파일을 raster() 함수로 읽어옴\n이 데이터는 st_crs() 함수로 좌표계를 확인해보면 “WGS84 투영”을 사용하므로, 십진수 각도 (decimal degrees as units) 를 단위로 사용\nres() 함수로 해상도를 확인해보면, 단지 숫자형 벡터 (numeric vector) 만 반환할 뿐, 단위에 대한 속성 정보는 없음 (no units attributes)\n\n\n\n## -- units in raster data library(raster) \nlibrary(spDataLarge) \nraster_filepath = system.file(\"raster/srtm.tif\", package = \"spDataLarge\") \nnew_raster = raster(raster_filepath)\n\n## -- getting CRS \nst_crs(new_raster) \n\nCoordinate Reference System:\n  User input: +proj=longlat +datum=WGS84 +no_defs \n  wkt:\nGEOGCRS[\"unknown\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ID[\"EPSG\",6326]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433],\n        ID[\"EPSG\",8901]],\n    CS[ellipsoidal,2],\n        AXIS[\"longitude\",east,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433,\n                ID[\"EPSG\",9122]]],\n        AXIS[\"latitude\",north,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433,\n                ID[\"EPSG\",9122]]]]\n\nplot(new_raster)\n\n\n\nres(new_raster)\n\n[1] 0.0008333333 0.0008333333\n\n\n\nUTM 투영을 사용한다면, 이에 따라서 단위가 바뀌지만, res()로 해상도를 살펴보면 역시 단지 숫자형 벡터만 반환할 뿐, 단위 속성 정보는 없음\n\n\n## -- if we used the UTM projection, the units would change. \nrepr <- projectRaster(new_raster, crs = \"+init=epsg:26912\") \n\n## -- no units attributes, just only returns numeric vector \nres(repr)\n\n[1] 73.8 92.5"
  },
  {
    "objectID": "posts/Spatial_Info_Analysis Ch2/공간정보분석 CH3.html",
    "href": "posts/Spatial_Info_Analysis Ch2/공간정보분석 CH3.html",
    "title": "Spatial_Info_Analysis CH2",
    "section": "",
    "text": "HTML파일로 보기\n속성 데이터 - 벡터 & 래스터"
  },
  {
    "objectID": "posts/Spatial_Info_Analysis Ch2/공간정보분석 CH3.html#공간-정보-분석-ch3-attribute-data-operations",
    "href": "posts/Spatial_Info_Analysis Ch2/공간정보분석 CH3.html#공간-정보-분석-ch3-attribute-data-operations",
    "title": "Spatial_Info_Analysis CH2",
    "section": "1 공간 정보 분석 CH3 : Attribute data operations",
    "text": "1 공간 정보 분석 CH3 : Attribute data operations"
  },
  {
    "objectID": "posts/Spatial_Info_Analysis Ch2/공간정보분석 CH3.html#패키지-불러오기",
    "href": "posts/Spatial_Info_Analysis Ch2/공간정보분석 CH3.html#패키지-불러오기",
    "title": "Spatial_Info_Analysis CH2",
    "section": "2 패키지 불러오기",
    "text": "2 패키지 불러오기\n\nlibrary(sf)\nlibrary(raster)\nlibrary(dplyr)\nlibrary(stringr) # for working with strings (pattern matching)\nlibrary(tidyr)   # for unite() and separate()\nlibrary(spData)  # 데이터 세트를 로드"
  },
  {
    "objectID": "posts/Spatial_Info_Analysis Ch2/공간정보분석 CH3.html#속성데이터-벡터-vs-래스터",
    "href": "posts/Spatial_Info_Analysis Ch2/공간정보분석 CH3.html#속성데이터-벡터-vs-래스터",
    "title": "Spatial_Info_Analysis CH2",
    "section": "3 3.1 속성데이터 (벡터 vs 래스터)",
    "text": "3 3.1 속성데이터 (벡터 vs 래스터)\n\n지리공간 벡터 데이터\n\n점, 선, 면 등의 리스트로 이뤄진 지리공간 데이터\n지리공간 데이터 제외한 속성데이터\n\nsf 객체: 벡터 데이터에서 속성 정보 가져옴, 클래스 지원\n\n속성 정보만 가져오기 : st_drop_geometry()\nbase R 구문, dplyr : 벡더 데이터 속성 정보의 dim\n\n\n\n### sf 객체 methods\nmethods(class = 'sf') %>% head()\n\n[1] \"$<-.sf\"       \"[.sf\"         \"[[<-.sf\"      \"aggregate.sf\" \"anti_join.sf\"\n[6] \"arrange.sf\"  \n\nmethods(class = 'sf') %>% tail()\n\n[1] \"summarise.sf\" \"transform.sf\" \"transmute.sf\" \"ungroup.sf\"   \"unite.sf\"    \n[6] \"unnest.sf\"   \n\nlength(methods(class = 'sf'))\n\n[1] 123"
  },
  {
    "objectID": "posts/Spatial_Info_Analysis Ch2/공간정보분석 CH3.html#spdata-세계-나라-데이터world",
    "href": "posts/Spatial_Info_Analysis Ch2/공간정보분석 CH3.html#spdata-세계-나라-데이터world",
    "title": "Spatial_Info_Analysis CH2",
    "section": "4 spData : 세계 나라 데이터(world)",
    "text": "4 spData : 세계 나라 데이터(world)\n\n### 10개의 속성 데이터, 1개의 지리기하 열\n\ndim(world)\n\n[1] 177  11\n\nworld\n\nSimple feature collection with 177 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -89.9 xmax: 180 ymax: 83.64513\nGeodetic CRS:  WGS 84\n# A tibble: 177 × 11\n   iso_a2 name_l…¹ conti…² regio…³ subre…⁴ type  area_…⁵     pop lifeExp gdpPe…⁶\n * <chr>  <chr>    <chr>   <chr>   <chr>   <chr>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 FJ     Fiji     Oceania Oceania Melane… Sove…  1.93e4  8.86e5    70.0   8222.\n 2 TZ     Tanzania Africa  Africa  Easter… Sove…  9.33e5  5.22e7    64.2   2402.\n 3 EH     Western… Africa  Africa  Northe… Inde…  9.63e4 NA         NA       NA \n 4 CA     Canada   North … Americ… Northe… Sove…  1.00e7  3.55e7    82.0  43079.\n 5 US     United … North … Americ… Northe… Coun…  9.51e6  3.19e8    78.8  51922.\n 6 KZ     Kazakhs… Asia    Asia    Centra… Sove…  2.73e6  1.73e7    71.6  23587.\n 7 UZ     Uzbekis… Asia    Asia    Centra… Sove…  4.61e5  3.08e7    71.0   5371.\n 8 PG     Papua N… Oceania Oceania Melane… Sove…  4.65e5  7.76e6    65.2   3709.\n 9 ID     Indones… Asia    Asia    South-… Sove…  1.82e6  2.55e8    68.9  10003.\n10 AR     Argenti… South … Americ… South … Sove…  2.78e6  4.30e7    76.3  18798.\n# … with 167 more rows, 1 more variable: geom <MULTIPOLYGON [°]>, and\n#   abbreviated variable names ¹​name_long, ²​continent, ³​region_un, ⁴​subregion,\n#   ⁵​area_km2, ⁶​gdpPercap\n\nnames(world)\n\n [1] \"iso_a2\"    \"name_long\" \"continent\" \"region_un\" \"subregion\" \"type\"     \n [7] \"area_km2\"  \"pop\"       \"lifeExp\"   \"gdpPercap\" \"geom\""
  },
  {
    "objectID": "posts/Spatial_Info_Analysis Ch2/공간정보분석 CH3.html#sf-객체에서-속성-정보만-가져오기-st_drop_geometry",
    "href": "posts/Spatial_Info_Analysis Ch2/공간정보분석 CH3.html#sf-객체에서-속성-정보만-가져오기-st_drop_geometry",
    "title": "Spatial_Info_Analysis CH2",
    "section": "5 sf 객체에서 속성 정보만 가져오기 : st_drop_geometry()",
    "text": "5 sf 객체에서 속성 정보만 가져오기 : st_drop_geometry()\n\n점, 선, 면 등의 지리기하 데이터를 리스트로 갖는 geom 컬럼 항상 존재\ngeom 컬럼을 제거하고 나머지 속성 정보만으로 d.f 만들 때 사용\ngeom 컬럼 : 지리기하 리스트 정보를 가지기때문에 메모리 점유 큼\n사용할 필요 없으면 웬만하면 제거하기\n\n\n### geom 컬럼 제거\n\nworld_df = st_drop_geometry(world)\n\nworld_df\n\n# A tibble: 177 × 10\n   iso_a2 name_l…¹ conti…² regio…³ subre…⁴ type  area_…⁵     pop lifeExp gdpPe…⁶\n * <chr>  <chr>    <chr>   <chr>   <chr>   <chr>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 FJ     Fiji     Oceania Oceania Melane… Sove…  1.93e4  8.86e5    70.0   8222.\n 2 TZ     Tanzania Africa  Africa  Easter… Sove…  9.33e5  5.22e7    64.2   2402.\n 3 EH     Western… Africa  Africa  Northe… Inde…  9.63e4 NA         NA       NA \n 4 CA     Canada   North … Americ… Northe… Sove…  1.00e7  3.55e7    82.0  43079.\n 5 US     United … North … Americ… Northe… Coun…  9.51e6  3.19e8    78.8  51922.\n 6 KZ     Kazakhs… Asia    Asia    Centra… Sove…  2.73e6  1.73e7    71.6  23587.\n 7 UZ     Uzbekis… Asia    Asia    Centra… Sove…  4.61e5  3.08e7    71.0   5371.\n 8 PG     Papua N… Oceania Oceania Melane… Sove…  4.65e5  7.76e6    65.2   3709.\n 9 ID     Indones… Asia    Asia    South-… Sove…  1.82e6  2.55e8    68.9  10003.\n10 AR     Argenti… South … Americ… South … Sove…  2.78e6  4.30e7    76.3  18798.\n# … with 167 more rows, and abbreviated variable names ¹​name_long, ²​continent,\n#   ³​region_un, ⁴​subregion, ⁵​area_km2, ⁶​gdpPercap\n\nclass(world_df)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nnames(world_df)\n\n [1] \"iso_a2\"    \"name_long\" \"continent\" \"region_un\" \"subregion\" \"type\"     \n [7] \"area_km2\"  \"pop\"       \"lifeExp\"   \"gdpPercap\""
  },
  {
    "objectID": "posts/Spatial_Info_Analysis Ch2/공간정보분석 CH3.html#base-r-구문으로-벡터-데이터-속성정보의-차원-가져오기",
    "href": "posts/Spatial_Info_Analysis Ch2/공간정보분석 CH3.html#base-r-구문으로-벡터-데이터-속성정보의-차원-가져오기",
    "title": "Spatial_Info_Analysis CH2",
    "section": "6 Base R 구문으로 벡터 데이터 속성정보의 차원 가져오기",
    "text": "6 Base R 구문으로 벡터 데이터 속성정보의 차원 가져오기\n\n행, 열 가져올때 df[i,j], subset() 등 활용\n\ni행, j열에는 정수로 위치 파악\nj열의 이름 사용\n논리 벡터 사용가능 subset 등\n\ndplyr 패키지 : select(), filter(), pull() 등 _ 특정 행, 열의 부분집합 가져오면 끝에 geom 칼럼 따라옴\n\n\n6.0.1 1. 위치 지정해서 지리공간 벡터 데이터 가져오기\n\n### 1행 ~ 6행 정보\nworld[1:6,]\n\nSimple feature collection with 6 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -18.28799 xmax: 180 ymax: 83.23324\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 11\n  iso_a2 name_long conti…¹ regio…² subre…³ type  area_…⁴     pop lifeExp gdpPe…⁵\n  <chr>  <chr>     <chr>   <chr>   <chr>   <chr>   <dbl>   <dbl>   <dbl>   <dbl>\n1 FJ     Fiji      Oceania Oceania Melane… Sove…  1.93e4  8.86e5    70.0   8222.\n2 TZ     Tanzania  Africa  Africa  Easter… Sove…  9.33e5  5.22e7    64.2   2402.\n3 EH     Western … Africa  Africa  Northe… Inde…  9.63e4 NA         NA       NA \n4 CA     Canada    North … Americ… Northe… Sove…  1.00e7  3.55e7    82.0  43079.\n5 US     United S… North … Americ… Northe… Coun…  9.51e6  3.19e8    78.8  51922.\n6 KZ     Kazakhst… Asia    Asia    Centra… Sove…  2.73e6  1.73e7    71.6  23587.\n# … with 1 more variable: geom <MULTIPOLYGON [°]>, and abbreviated variable\n#   names ¹​continent, ²​region_un, ³​subregion, ⁴​area_km2, ⁵​gdpPercap\n\n### 1열 ~ 3열 정보\nworld[,1:3]\n\nSimple feature collection with 177 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -89.9 xmax: 180 ymax: 83.64513\nGeodetic CRS:  WGS 84\n# A tibble: 177 × 4\n   iso_a2 name_long        continent                                        geom\n   <chr>  <chr>            <chr>                              <MULTIPOLYGON [°]>\n 1 FJ     Fiji             Oceania       (((-180 -16.55522, -179.9174 -16.50178…\n 2 TZ     Tanzania         Africa        (((33.90371 -0.95, 31.86617 -1.02736, …\n 3 EH     Western Sahara   Africa        (((-8.66559 27.65643, -8.817828 27.656…\n 4 CA     Canada           North America (((-132.71 54.04001, -133.18 54.16998,…\n 5 US     United States    North America (((-171.7317 63.78252, -171.7911 63.40…\n 6 KZ     Kazakhstan       Asia          (((87.35997 49.21498, 86.82936 49.8266…\n 7 UZ     Uzbekistan       Asia          (((55.96819 41.30864, 57.09639 41.3223…\n 8 PG     Papua New Guinea Oceania       (((141.0002 -2.600151, 141.0171 -5.859…\n 9 ID     Indonesia        Asia          (((104.37 -1.084843, 104.0108 -1.05921…\n10 AR     Argentina        South America (((-68.63401 -52.63637, -68.63335 -54.…\n# … with 167 more rows\n\n\n\n\n6.0.2 2. 열이름 사용해서 지리공간 벡터 데이터 열 가져오기\n\nworld[, c('name_long', 'lifeExp')]\n\nSimple feature collection with 177 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -89.9 xmax: 180 ymax: 83.64513\nGeodetic CRS:  WGS 84\n# A tibble: 177 × 3\n   name_long        lifeExp                                                 geom\n   <chr>              <dbl>                                   <MULTIPOLYGON [°]>\n 1 Fiji                70.0 (((-180 -16.55522, -179.9174 -16.50178, -179.7933 -…\n 2 Tanzania            64.2 (((33.90371 -0.95, 31.86617 -1.02736, 30.76986 -1.0…\n 3 Western Sahara      NA   (((-8.66559 27.65643, -8.817828 27.65643, -8.794884…\n 4 Canada              82.0 (((-132.71 54.04001, -133.18 54.16998, -133.2397 53…\n 5 United States       78.8 (((-171.7317 63.78252, -171.7911 63.40585, -171.553…\n 6 Kazakhstan          71.6 (((87.35997 49.21498, 86.82936 49.82667, 85.54127 4…\n 7 Uzbekistan          71.0 (((55.96819 41.30864, 57.09639 41.32231, 56.93222 4…\n 8 Papua New Guinea    65.2 (((141.0002 -2.600151, 141.0171 -5.859022, 141.0339…\n 9 Indonesia           68.9 (((104.37 -1.084843, 104.0108 -1.059212, 103.4376 -…\n10 Argentina           76.3 (((-68.63401 -52.63637, -68.63335 -54.8695, -67.562…\n# … with 167 more rows\n\n\n\n\n6.1 3. 논리 벡터 사용해서 데이터 가져오기\n\nsel_area <-  world$area_km2 < 10000\nsummary(sel_area)\n\n   Mode   FALSE    TRUE \nlogical     170       7 \n\nsmall_countries <-  world[sel_area, ]\nsmall_countries\n\nSimple feature collection with 7 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -67.24243 ymin: -16.59785 xmax: 167.8449 ymax: 50.12805\nGeodetic CRS:  WGS 84\n# A tibble: 7 × 11\n  iso_a2 name_long conti…¹ regio…² subre…³ type  area_…⁴     pop lifeExp gdpPe…⁵\n  <chr>  <chr>     <chr>   <chr>   <chr>   <chr>   <dbl>   <dbl>   <dbl>   <dbl>\n1 PR     Puerto R… North … Americ… Caribb… Depe…   9225. 3534874    79.4  35066.\n2 PS     Palestine Asia    Asia    Wester… Disp…   5037. 4294682    73.1   4320.\n3 VU     Vanuatu   Oceania Oceania Melane… Sove…   7490.  258850    71.7   2892.\n4 LU     Luxembou… Europe  Europe  Wester… Sove…   2417.  556319    82.2  93655.\n5 <NA>   Northern… Asia    Asia    Wester… Sove…   3786.      NA    NA       NA \n6 CY     Cyprus    Asia    Asia    Wester… Sove…   6207. 1152309    80.2  29786.\n7 TT     Trinidad… North … Americ… Caribb… Sove…   7738. 1354493    70.4  31182.\n# … with 1 more variable: geom <MULTIPOLYGON [°]>, and abbreviated variable\n#   names ¹​continent, ²​region_un, ³​subregion, ⁴​area_km2, ⁵​gdpPercap\n\nsmall_countries <- world[world$area_km2 < 10000, ]\nsmall_countries\n\nSimple feature collection with 7 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -67.24243 ymin: -16.59785 xmax: 167.8449 ymax: 50.12805\nGeodetic CRS:  WGS 84\n# A tibble: 7 × 11\n  iso_a2 name_long conti…¹ regio…² subre…³ type  area_…⁴     pop lifeExp gdpPe…⁵\n  <chr>  <chr>     <chr>   <chr>   <chr>   <chr>   <dbl>   <dbl>   <dbl>   <dbl>\n1 PR     Puerto R… North … Americ… Caribb… Depe…   9225. 3534874    79.4  35066.\n2 PS     Palestine Asia    Asia    Wester… Disp…   5037. 4294682    73.1   4320.\n3 VU     Vanuatu   Oceania Oceania Melane… Sove…   7490.  258850    71.7   2892.\n4 LU     Luxembou… Europe  Europe  Wester… Sove…   2417.  556319    82.2  93655.\n5 <NA>   Northern… Asia    Asia    Wester… Sove…   3786.      NA    NA       NA \n6 CY     Cyprus    Asia    Asia    Wester… Sove…   6207. 1152309    80.2  29786.\n7 TT     Trinidad… North … Americ… Caribb… Sove…   7738. 1354493    70.4  31182.\n# … with 1 more variable: geom <MULTIPOLYGON [°]>, and abbreviated variable\n#   names ¹​continent, ²​region_un, ³​subregion, ⁴​area_km2, ⁵​gdpPercap\n\nsmall_countries <- subset(world, area_km2 < 10000)\nsmall_countries\n\nSimple feature collection with 7 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -67.24243 ymin: -16.59785 xmax: 167.8449 ymax: 50.12805\nGeodetic CRS:  WGS 84\n# A tibble: 7 × 11\n  iso_a2 name_long conti…¹ regio…² subre…³ type  area_…⁴     pop lifeExp gdpPe…⁵\n  <chr>  <chr>     <chr>   <chr>   <chr>   <chr>   <dbl>   <dbl>   <dbl>   <dbl>\n1 PR     Puerto R… North … Americ… Caribb… Depe…   9225. 3534874    79.4  35066.\n2 PS     Palestine Asia    Asia    Wester… Disp…   5037. 4294682    73.1   4320.\n3 VU     Vanuatu   Oceania Oceania Melane… Sove…   7490.  258850    71.7   2892.\n4 LU     Luxembou… Europe  Europe  Wester… Sove…   2417.  556319    82.2  93655.\n5 <NA>   Northern… Asia    Asia    Wester… Sove…   3786.      NA    NA       NA \n6 CY     Cyprus    Asia    Asia    Wester… Sove…   6207. 1152309    80.2  29786.\n7 TT     Trinidad… North … Americ… Caribb… Sove…   7738. 1354493    70.4  31182.\n# … with 1 more variable: geom <MULTIPOLYGON [°]>, and abbreviated variable\n#   names ¹​continent, ²​region_un, ³​subregion, ⁴​area_km2, ⁵​gdpPercap"
  },
  {
    "objectID": "posts/Spatial_Info_Analysis Ch2/공간정보분석 CH3.html#dplyr-패키지로-벡터-데이터-속성정보-차원-가져오기",
    "href": "posts/Spatial_Info_Analysis Ch2/공간정보분석 CH3.html#dplyr-패키지로-벡터-데이터-속성정보-차원-가져오기",
    "title": "Spatial_Info_Analysis CH2",
    "section": "7 dplyr 패키지로 벡터 데이터 속성정보 차원 가져오기",
    "text": "7 dplyr 패키지로 벡터 데이터 속성정보 차원 가져오기\n\n가독성 좋고 속도 빠름\n체인 (%>%) 사용가능\nselect(), slice(), filter(), pull() 등의 함수 사용가능\n\n\n### select로 강력하게 지정가능\n\n# 선택지정\nworld1 <- select(world, name_long, pop)\nnames(world1)\n\n[1] \"name_long\" \"pop\"       \"geom\"     \n\n# 범위지정\nworld2 <- select(world, name_long:pop)\nnames(world2)\n\n[1] \"name_long\" \"continent\" \"region_un\" \"subregion\" \"type\"      \"area_km2\" \n[7] \"pop\"       \"geom\"     \n\n# 컬럼 번호로 선택지정\nworld3 <- select(world,2,7)\nnames(world3)\n\n[1] \"name_long\" \"area_km2\"  \"geom\"     \n\n# 특정 컬럼 제외\nworld4 <- select(world, -subregion, -area_km2)\nnames(world4)\n\n[1] \"iso_a2\"    \"name_long\" \"continent\" \"region_un\" \"type\"      \"pop\"      \n[7] \"lifeExp\"   \"gdpPercap\" \"geom\"     \n\n# 컬럼 이름 재설정\nworld5 <- select(world, name_long, population = pop)\nnames(world5)\n\n[1] \"name_long\"  \"population\" \"geom\"      \n\n# 특정 단어가 포함된 열 추출\nworld6 <- select(world, contains('Ex')) \nnames(world6)\n\n[1] \"lifeExp\" \"geom\"   \n\n# 특정 단어로 시작하는 열 추출\nworld7  <- select(world, starts_with('life')) \nnames(world7)\n\n[1] \"lifeExp\" \"geom\"   \n\n# 특정 단어로 끝나는 열 추출\nworld8 <-  select(world, ends_with('Exp')) \nnames(world8)\n\n[1] \"lifeExp\" \"geom\"   \n\n# 특정 단어의 소문자 or 대문자 포함된 열 추출\nworld9 <-  select(world, matches(\"[p]\")) \nnames(world9)\n\n[1] \"type\"      \"pop\"       \"lifeExp\"   \"gdpPercap\" \"geom\"     \n\n### num_range() 함수\nx1 <- c(1:5) \nx2 <- c(6:10) \nx3 <- c(11:15) \nx4 <- c(16:20) \nx5 <- c(21:25)\ndf <- data.frame(x1, x2, x3, x4, x5) \ndf\n\n  x1 x2 x3 x4 x5\n1  1  6 11 16 21\n2  2  7 12 17 22\n3  3  8 13 18 23\n4  4  9 14 19 24\n5  5 10 15 20 25\n\ndf2 <- select(df, num_range(\"x\", 1:3))\ndf2\n\n  x1 x2 x3\n1  1  6 11\n2  2  7 12\n3  3  8 13\n4  4  9 14\n5  5 10 15\n\n\n\n### filter() 함수 subset과 비슷\n\n# 조건 걸기\nworld10  <-  filter(world, lifeExp > 82)\nworld10\n\nSimple feature collection with 9 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -24.32618 ymin: -43.6346 xmax: 153.5695 ymax: 69.10625\nGeodetic CRS:  WGS 84\n# A tibble: 9 × 11\n  iso_a2 name_long  conti…¹ regio…² subre…³ type  area_…⁴    pop lifeExp gdpPe…⁵\n* <chr>  <chr>      <chr>   <chr>   <chr>   <chr>   <dbl>  <dbl>   <dbl>   <dbl>\n1 IL     Israel     Asia    Asia    Wester… Coun…  2.30e4 8.22e6    82.2  31702.\n2 SE     Sweden     Europe  Europe  Northe… Sove…  4.51e5 9.70e6    82.3  44168.\n3 CH     Switzerla… Europe  Europe  Wester… Sove…  4.62e4 8.19e6    83.2  57218.\n4 LU     Luxembourg Europe  Europe  Wester… Sove…  2.42e3 5.56e5    82.2  93655.\n5 ES     Spain      Europe  Europe  Southe… Sove…  5.02e5 4.65e7    83.2  31195.\n6 AU     Australia  Oceania Oceania Austra… Coun…  7.69e6 2.35e7    82.3  43547.\n7 IT     Italy      Europe  Europe  Southe… Sove…  3.15e5 6.08e7    83.1  33946.\n8 IS     Iceland    Europe  Europe  Northe… Sove…  1.08e5 3.27e5    82.9  41701.\n9 JP     Japan      Asia    Asia    Easter… Sove…  4.05e5 1.27e8    83.6  37337.\n# … with 1 more variable: geom <MULTIPOLYGON [°]>, and abbreviated variable\n#   names ¹​continent, ²​region_un, ³​subregion, ⁴​area_km2, ⁵​gdpPercap\n\n### 체인(%>%) 활용, 가독성 좋음\n\nworld11 <-  world %>%\n  filter(continent == \"Asia\") %>%\n  select(name_long, continent) %>%\n  slice(1:5)\nworld11\n\nSimple feature collection with 5 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 34.26543 ymin: -10.35999 xmax: 141.0339 ymax: 55.38525\nGeodetic CRS:  WGS 84\n# A tibble: 5 × 3\n  name_long   continent                                                     geom\n  <chr>       <chr>                                           <MULTIPOLYGON [°]>\n1 Kazakhstan  Asia      (((87.35997 49.21498, 86.82936 49.82667, 85.54127 49.69…\n2 Uzbekistan  Asia      (((55.96819 41.30864, 57.09639 41.32231, 56.93222 41.82…\n3 Indonesia   Asia      (((104.37 -1.084843, 104.0108 -1.059212, 103.4376 -0.71…\n4 Timor-Leste Asia      (((124.9687 -8.89279, 125.07 -9.089987, 125.0885 -9.393…\n5 Israel      Asia      (((35.71992 32.70919, 35.7008 32.71601, 35.8364 32.8681…\n\n\n\n### aggregate() 함수 사용해서 그룹별로 집계\n\n# x(속성정보:인구) ~ group(그룹:대륙), sum집계\nworld_agg1  <-  aggregate(pop ~ continent, FUN = sum, data = world, na.rm = TRUE)\n\nworld_agg1\n\n      continent        pop\n1        Africa 1154946633\n2          Asia 4311408059\n3        Europe  669036256\n4 North America  565028684\n5       Oceania   37757833\n6 South America  412060811\n\nstr(world_agg1)\n\n'data.frame':   6 obs. of  2 variables:\n $ continent: chr  \"Africa\" \"Asia\" \"Europe\" \"North America\" ...\n $ pop      : num  1.15e+09 4.31e+09 6.69e+08 5.65e+08 3.78e+07 ...\n\nclass(world_agg1)\n\n[1] \"data.frame\"\n\n### 2번째 방법\nworld_agg2  <-  aggregate(world[\"pop\"], by = list(world$continent),\n                       FUN = sum, na.rm = TRUE)\nworld_agg2\n\nSimple feature collection with 8 features and 2 fields\nAttribute-geometry relationship: 0 constant, 1 aggregate, 1 identity\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: -180 ymin: -89.9 xmax: 180 ymax: 83.64513\nGeodetic CRS:  WGS 84\n                  Group.1        pop                       geometry\n1                  Africa 1154946633 MULTIPOLYGON (((11.91496 -5...\n2              Antarctica          0 MULTIPOLYGON (((-61.13898 -...\n3                    Asia 4311408059 MULTIPOLYGON (((120.295 -10...\n4                  Europe  669036256 MULTIPOLYGON (((-53.77852 2...\n5           North America  565028684 MULTIPOLYGON (((-80.9473 8....\n6                 Oceania   37757833 MULTIPOLYGON (((171.9487 -4...\n7 Seven seas (open ocean)          0 POLYGON ((68.935 -48.625, 6...\n8           South America  412060811 MULTIPOLYGON (((-57.75 -51....\n\nclass(world_agg2)\n\n[1] \"sf\"         \"data.frame\"\n\n# sf 객체는 집계 결과가 sf 객체로 반환\nworld['pop']\n\nSimple feature collection with 177 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -89.9 xmax: 180 ymax: 83.64513\nGeodetic CRS:  WGS 84\n# A tibble: 177 × 2\n         pop                                                                geom\n       <dbl>                                                  <MULTIPOLYGON [°]>\n 1    885806 (((-180 -16.55522, -179.9174 -16.50178, -179.7933 -16.02088, -180 …\n 2  52234869 (((33.90371 -0.95, 31.86617 -1.02736, 30.76986 -1.01455, 30.4191 -…\n 3        NA (((-8.66559 27.65643, -8.817828 27.65643, -8.794884 27.1207, -9.41…\n 4  35535348 (((-132.71 54.04001, -133.18 54.16998, -133.2397 53.85108, -133.05…\n 5 318622525 (((-171.7317 63.78252, -171.7911 63.40585, -171.5531 63.31779, -17…\n 6  17288285 (((87.35997 49.21498, 86.82936 49.82667, 85.54127 49.69286, 85.115…\n 7  30757700 (((55.96819 41.30864, 57.09639 41.32231, 56.93222 41.82603, 57.786…\n 8   7755785 (((141.0002 -2.600151, 141.0171 -5.859022, 141.0339 -9.117893, 142…\n 9 255131116 (((104.37 -1.084843, 104.0108 -1.059212, 103.4376 -0.7119459, 103.…\n10  42981515 (((-68.63401 -52.63637, -68.63335 -54.8695, -67.56244 -54.87001, -…\n# … with 167 more rows\n\nclass(world['pop'])\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n# 숫자형 벡터는 aggregate 적용하면 d.f로 반환\nworld$pop %>% head()\n\n[1]    885806  52234869        NA  35535348 318622525  17288285\n\nclass(world$pop)\n\n[1] \"numeric\"\n\n\n\n### group_by + summarize : aggregate보다 가독성 좋음\n\n# 대륙별 인구의 합계\nworld_agg3  <-  world %>%\n  group_by(continent) %>%\n  summarize(pop = sum(pop, na.rm = TRUE))\nworld_agg3\n\nSimple feature collection with 8 features and 2 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: -180 ymin: -89.9 xmax: 180 ymax: 83.64513\nGeodetic CRS:  WGS 84\n# A tibble: 8 × 3\n  continent                      pop                                        geom\n  <chr>                        <dbl>                              <GEOMETRY [°]>\n1 Africa                  1154946633 MULTIPOLYGON (((36.86623 22, 36.69069 22.2…\n2 Antarctica                       0 MULTIPOLYGON (((-180 -89.9, 180 -89.9, 180…\n3 Asia                    4311408059 MULTIPOLYGON (((36.14976 35.82153, 35.9050…\n4 Europe                   669036256 MULTIPOLYGON (((26.29 35.29999, 25.74502 3…\n5 North America            565028684 MULTIPOLYGON (((-82.26815 23.18861, -82.51…\n6 Oceania                   37757833 MULTIPOLYGON (((166.7932 -15.66881, 167.00…\n7 Seven seas (open ocean)          0 POLYGON ((68.935 -48.625, 68.8675 -48.83, …\n8 South America            412060811 MULTIPOLYGON (((-66.95992 -54.89681, -66.4…\n\n# 대륙별 인구의 합계 + 대륙별 국가 수\nworld_agg4  <-  world %>%\n  group_by(continent) %>%\n  summarize(pop = sum(pop, na.rm = TRUE), n_countries = n())\nworld_agg4\n\nSimple feature collection with 8 features and 3 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: -180 ymin: -89.9 xmax: 180 ymax: 83.64513\nGeodetic CRS:  WGS 84\n# A tibble: 8 × 4\n  continent                      pop n_countries                            geom\n  <chr>                        <dbl>       <int>                  <GEOMETRY [°]>\n1 Africa                  1154946633          51 MULTIPOLYGON (((36.86623 22, 3…\n2 Antarctica                       0           1 MULTIPOLYGON (((-180 -89.9, 18…\n3 Asia                    4311408059          47 MULTIPOLYGON (((36.14976 35.82…\n4 Europe                   669036256          39 MULTIPOLYGON (((26.29 35.29999…\n5 North America            565028684          18 MULTIPOLYGON (((-82.26815 23.1…\n6 Oceania                   37757833           7 MULTIPOLYGON (((166.7932 -15.6…\n7 Seven seas (open ocean)          0           1 POLYGON ((68.935 -48.625, 68.8…\n8 South America            412060811          13 MULTIPOLYGON (((-66.95992 -54.…\n\n# 대륙별 인구의 합계 + 대륙별 국가 수 + 인구 탑3 대륙 + 인구 내림차순\nworld %>% \n  select(pop, continent) %>% \n  group_by(continent) %>% \n  summarize(pop = sum(pop, na.rm = TRUE), n_countries = n()) %>% \n  top_n(n = 3, wt = pop) %>%\n  arrange(desc(pop)) %>%\n  st_drop_geometry() # geom 컬럼 제거\n\n# A tibble: 3 × 3\n  continent        pop n_countries\n* <chr>          <dbl>       <int>\n1 Asia      4311408059          47\n2 Africa    1154946633          51\n3 Europe     669036256          39"
  },
  {
    "objectID": "posts/Spatial_Info_Analysis Ch2/공간정보분석 CH3.html#data.table로-벡터-데이터의-속성정보-그룹별-집계",
    "href": "posts/Spatial_Info_Analysis Ch2/공간정보분석 CH3.html#data.table로-벡터-데이터의-속성정보-그룹별-집계",
    "title": "Spatial_Info_Analysis CH2",
    "section": "8 data.table로 벡터 데이터의 속성정보 그룹별 집계",
    "text": "8 data.table로 벡터 데이터의 속성정보 그룹별 집계\n\ndata.table 데이터 구조를 조작, 관리, 처리할 때 사용. 간결성\n\n\n#install.packages(\"data.table\")\nlibrary(data.table)\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\n\nThe following object is masked from 'package:raster':\n\n    shift\n\nworld_dt <-  data.table(world)\nworld_dt\n\n     iso_a2           name_long     continent region_un        subregion\n  1:     FJ                Fiji       Oceania   Oceania        Melanesia\n  2:     TZ            Tanzania        Africa    Africa   Eastern Africa\n  3:     EH      Western Sahara        Africa    Africa  Northern Africa\n  4:     CA              Canada North America  Americas Northern America\n  5:     US       United States North America  Americas Northern America\n ---                                                                    \n173:     RS              Serbia        Europe    Europe  Southern Europe\n174:     ME          Montenegro        Europe    Europe  Southern Europe\n175:     XK              Kosovo        Europe    Europe  Southern Europe\n176:     TT Trinidad and Tobago North America  Americas        Caribbean\n177:     SS         South Sudan        Africa    Africa   Eastern Africa\n                  type    area_km2       pop  lifeExp gdpPercap     geom\n  1: Sovereign country    19289.97    885806 69.96000  8222.254  <XY[3]>\n  2: Sovereign country   932745.79  52234869 64.16300  2402.099  <XY[1]>\n  3:     Indeterminate    96270.60        NA       NA        NA  <XY[1]>\n  4: Sovereign country 10036042.98  35535348 81.95305 43079.143 <XY[30]>\n  5:           Country  9510743.74 318622525 78.84146 51921.985 <XY[10]>\n ---                                                                    \n173: Sovereign country    76388.61   7130576 75.33659 13112.909  <XY[1]>\n174: Sovereign country    13443.68    621810 76.71200 14796.635  <XY[1]>\n175: Sovereign country    11230.26   1821800 71.09756  8698.292  <XY[1]>\n176: Sovereign country     7737.81   1354493 70.42600 31181.821  <XY[1]>\n177: Sovereign country   624909.10  11530971 55.81700  1935.879  <XY[1]>\n\n# 간결하게 top3 대륙 추출\nworld_dt[, .(pop = sum(pop, na.rm = TRUE), n = .N), by = list(continent)][order(-pop)][1:3] \n\n   continent        pop  n\n1:      Asia 4311408059 47\n2:    Africa 1154946633 51\n3:    Europe  669036256 39"
  },
  {
    "objectID": "posts/Spatial_Info_Analysis Ch2/공간정보분석 CH3.html#두개의-지리-벡터-데이터-테이블-join-하기",
    "href": "posts/Spatial_Info_Analysis Ch2/공간정보분석 CH3.html#두개의-지리-벡터-데이터-테이블-join-하기",
    "title": "Spatial_Info_Analysis CH2",
    "section": "9 두개의 지리 벡터 데이터 테이블 join 하기",
    "text": "9 두개의 지리 벡터 데이터 테이블 join 하기\n\n### name_long 컬럼 기준으로 join\nworld_coffee <-  left_join(world, coffee_data) # by='name_long'\n\nJoining with `by = join_by(name_long)`\n\nworld_coffee\n\nSimple feature collection with 177 features and 12 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -89.9 xmax: 180 ymax: 83.64513\nGeodetic CRS:  WGS 84\n# A tibble: 177 × 13\n   iso_a2 name_l…¹ conti…² regio…³ subre…⁴ type  area_…⁵     pop lifeExp gdpPe…⁶\n   <chr>  <chr>    <chr>   <chr>   <chr>   <chr>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 FJ     Fiji     Oceania Oceania Melane… Sove…  1.93e4  8.86e5    70.0   8222.\n 2 TZ     Tanzania Africa  Africa  Easter… Sove…  9.33e5  5.22e7    64.2   2402.\n 3 EH     Western… Africa  Africa  Northe… Inde…  9.63e4 NA         NA       NA \n 4 CA     Canada   North … Americ… Northe… Sove…  1.00e7  3.55e7    82.0  43079.\n 5 US     United … North … Americ… Northe… Coun…  9.51e6  3.19e8    78.8  51922.\n 6 KZ     Kazakhs… Asia    Asia    Centra… Sove…  2.73e6  1.73e7    71.6  23587.\n 7 UZ     Uzbekis… Asia    Asia    Centra… Sove…  4.61e5  3.08e7    71.0   5371.\n 8 PG     Papua N… Oceania Oceania Melane… Sove…  4.65e5  7.76e6    65.2   3709.\n 9 ID     Indones… Asia    Asia    South-… Sove…  1.82e6  2.55e8    68.9  10003.\n10 AR     Argenti… South … Americ… South … Sove…  2.78e6  4.30e7    76.3  18798.\n# … with 167 more rows, 3 more variables: geom <MULTIPOLYGON [°]>,\n#   coffee_production_2016 <int>, coffee_production_2017 <int>, and abbreviated\n#   variable names ¹​name_long, ²​continent, ³​region_un, ⁴​subregion, ⁵​area_km2,\n#   ⁶​gdpPercap\n\nclass(world_coffee)\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nnames(world_coffee)\n\n [1] \"iso_a2\"                 \"name_long\"              \"continent\"             \n [4] \"region_un\"              \"subregion\"              \"type\"                  \n [7] \"area_km2\"               \"pop\"                    \"lifeExp\"               \n[10] \"gdpPercap\"              \"geom\"                   \"coffee_production_2016\"\n[13] \"coffee_production_2017\"\n\nplot(world_coffee[\"coffee_production_2017\"])\n\n\n\n### 두 데이터 셋에 같은 이름의 컬럼이 없는 경우\n# coffee_data의 name_long변수 이름을 nm으로 변경\ncoffee_renamed  <-  rename(coffee_data, nm = name_long)\n\n# by 사용하여 결합 변수를 지정하여 다른이름변수를 기준으로 조인하기\nworld_coffee2 <- left_join(world, coffee_renamed, by = c(name_long = \"nm\"))\n\nworld_coffee2\n\nSimple feature collection with 177 features and 12 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -89.9 xmax: 180 ymax: 83.64513\nGeodetic CRS:  WGS 84\n# A tibble: 177 × 13\n   iso_a2 name_l…¹ conti…² regio…³ subre…⁴ type  area_…⁵     pop lifeExp gdpPe…⁶\n   <chr>  <chr>    <chr>   <chr>   <chr>   <chr>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 FJ     Fiji     Oceania Oceania Melane… Sove…  1.93e4  8.86e5    70.0   8222.\n 2 TZ     Tanzania Africa  Africa  Easter… Sove…  9.33e5  5.22e7    64.2   2402.\n 3 EH     Western… Africa  Africa  Northe… Inde…  9.63e4 NA         NA       NA \n 4 CA     Canada   North … Americ… Northe… Sove…  1.00e7  3.55e7    82.0  43079.\n 5 US     United … North … Americ… Northe… Coun…  9.51e6  3.19e8    78.8  51922.\n 6 KZ     Kazakhs… Asia    Asia    Centra… Sove…  2.73e6  1.73e7    71.6  23587.\n 7 UZ     Uzbekis… Asia    Asia    Centra… Sove…  4.61e5  3.08e7    71.0   5371.\n 8 PG     Papua N… Oceania Oceania Melane… Sove…  4.65e5  7.76e6    65.2   3709.\n 9 ID     Indones… Asia    Asia    South-… Sove…  1.82e6  2.55e8    68.9  10003.\n10 AR     Argenti… South … Americ… South … Sove…  2.78e6  4.30e7    76.3  18798.\n# … with 167 more rows, 3 more variables: geom <MULTIPOLYGON [°]>,\n#   coffee_production_2016 <int>, coffee_production_2017 <int>, and abbreviated\n#   variable names ¹​name_long, ²​continent, ³​region_un, ⁴​subregion, ⁵​area_km2,\n#   ⁶​gdpPercap\n\n### inner_join (left_join과 다른점 : 공통된 값이 있는 행만 살림)\nworld_coffee_inner <- inner_join(world, coffee_data)\n\nJoining with `by = join_by(name_long)`\n\nworld_coffee_inner\n\nSimple feature collection with 45 features and 12 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -117.1278 ymin: -33.76838 xmax: 156.02 ymax: 35.49401\nGeodetic CRS:  WGS 84\n# A tibble: 45 × 13\n   iso_a2 name_long conti…¹ regio…² subre…³ type  area_…⁴    pop lifeExp gdpPe…⁵\n   <chr>  <chr>     <chr>   <chr>   <chr>   <chr>   <dbl>  <dbl>   <dbl>   <dbl>\n 1 TZ     Tanzania  Africa  Africa  Easter… Sove…  9.33e5 5.22e7    64.2   2402.\n 2 PG     Papua Ne… Oceania Oceania Melane… Sove…  4.65e5 7.76e6    65.2   3709.\n 3 ID     Indonesia Asia    Asia    South-… Sove…  1.82e6 2.55e8    68.9  10003.\n 4 KE     Kenya     Africa  Africa  Easter… Sove…  5.91e5 4.60e7    66.2   2753.\n 5 DO     Dominica… North … Americ… Caribb… Sove…  4.82e4 1.04e7    73.5  12663.\n 6 TL     Timor-Le… Asia    Asia    South-… Sove…  1.47e4 1.21e6    68.3   6263.\n 7 MX     Mexico    North … Americ… Centra… Sove…  1.97e6 1.24e8    76.8  16623.\n 8 BR     Brazil    South … Americ… South … Sove…  8.51e6 2.04e8    75.0  15374.\n 9 BO     Bolivia   South … Americ… South … Sove…  1.09e6 1.06e7    68.4   6325.\n10 PE     Peru      South … Americ… South … Sove…  1.31e6 3.10e7    74.5  11548.\n# … with 35 more rows, 3 more variables: geom <MULTIPOLYGON [°]>,\n#   coffee_production_2016 <int>, coffee_production_2017 <int>, and abbreviated\n#   variable names ¹​continent, ²​region_un, ³​subregion, ⁴​area_km2, ⁵​gdpPercap\n\n### 비교\ndim(world_coffee_inner)\n\n[1] 45 13\n\ndim(world_coffee)\n\n[1] 177  13\n\nnrow(coffee_data)\n\n[1] 47\n\n### setdiff(x,y) : x에서 y에는 없는 데이터 추출\nsetdiff(coffee_data$name_long, world$name_long)\n\n[1] \"Congo, Dem. Rep. of\" \"Others\"             \n\n### 문자열 추출 (Dem으로 시작, Congo로 끝나는 단어 추출)\nstr_subset(world$name_long, 'Dem*.+Congo')\n\n[1] \"Democratic Republic of the Congo\"\n\n### grepl : 문자열에서 특정 문자열 지정\n# 이름 동일하게 변경\ncoffee_data$name_long[grepl(\"Congo,\", coffee_data$name_long)]  <-  \n  str_subset(world$name_long, \"Dem*.+Congo\")\n\nstr_subset(coffee_data$name_long, 'Dem*.+Congo')\n\n[1] \"Democratic Republic of the Congo\"\n\n# 46개의 국가 모두 반환\nworld_coffee_match <- inner_join(world, coffee_data)\n\nJoining with `by = join_by(name_long)`\n\ndim(world_coffee_match)\n\n[1] 46 13\n\n### left_join(x,y) : x기준으로 y 병합\ncoffee_world = left_join(world, coffee_data)\n\nJoining with `by = join_by(name_long)`\n\ndim(coffee_world)\n\n[1] 177  13\n\ncoffee_world2 = left_join(coffee_data, world)\n\nJoining with `by = join_by(name_long)`\n\ndim(coffee_world2)\n\n[1] 47 13"
  },
  {
    "objectID": "posts/Spatial_Info_Analysis Ch2/공간정보분석 CH3.html#지리-벡터-데이터의-새로운-속성-만들기지리정보-제거하기",
    "href": "posts/Spatial_Info_Analysis Ch2/공간정보분석 CH3.html#지리-벡터-데이터의-새로운-속성-만들기지리정보-제거하기",
    "title": "Spatial_Info_Analysis CH2",
    "section": "10 지리 벡터 데이터의 새로운 속성 만들기/지리정보 제거하기",
    "text": "10 지리 벡터 데이터의 새로운 속성 만들기/지리정보 제거하기\n\n### 원본 데이터 덮어쓰기 방지\nworld_new <- world\n\n### 새로운 속성 pop_dens 생성 (Km2 면적당 인구밀도)\nworld_new$pop_dens <- world_new$pop / world_new$area_km2\n\n### 속성값으로 보기 vs 지도상으로 보기(geom 정보가 있기때문)\nplot(world_new$pop_dens)\n\n\n\nplot(world_new['pop_dens'])\n\n\n\n### dplyr로 새로운 속성 만들기\n# mutate\nworld_new <- world_new %>% mutate(pop_dens2 = pop / area_km2)\n\n# transmute : 기존 열 모두 제거, 새로 만든 열 + geom 열만 반환\nworld_new <- world %>% transmute(pop_dens3 = pop/area_km2)\n\nworld_new %>% head()\n\nSimple feature collection with 6 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -18.28799 xmax: 180 ymax: 83.23324\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 2\n  pop_dens3                                                                 geom\n      <dbl>                                                   <MULTIPOLYGON [°]>\n1     45.9  (((-180 -16.55522, -179.9174 -16.50178, -179.7933 -16.02088, -180 -…\n2     56.0  (((33.90371 -0.95, 31.86617 -1.02736, 30.76986 -1.01455, 30.4191 -1…\n3     NA    (((-8.66559 27.65643, -8.817828 27.65643, -8.794884 27.1207, -9.413…\n4      3.54 (((-132.71 54.04001, -133.18 54.16998, -133.2397 53.85108, -133.054…\n5     33.5  (((-171.7317 63.78252, -171.7911 63.40585, -171.5531 63.31779, -170…\n6      6.33 (((87.35997 49.21498, 86.82936 49.82667, 85.54127 49.69286, 85.1155…\n\n### tidyr로 기존 속성 합치거나 분리하기\n\n### unite()\n# con_reg : continent + region_un 합치고 ':' 로 분리, 합쳐진 열들 remove\nworld_unite <-  world %>%\n  unite(\"con_reg\", continent:region_un, sep = \":\", remove = TRUE)\n\nworld_unite\n\nSimple feature collection with 177 features and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -89.9 xmax: 180 ymax: 83.64513\nGeodetic CRS:  WGS 84\n# A tibble: 177 × 10\n   iso_a2 name_long        con_reg subre…¹ type  area_…²     pop lifeExp gdpPe…³\n   <chr>  <chr>            <chr>   <chr>   <chr>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 FJ     Fiji             Oceani… Melane… Sove…  1.93e4  8.86e5    70.0   8222.\n 2 TZ     Tanzania         Africa… Easter… Sove…  9.33e5  5.22e7    64.2   2402.\n 3 EH     Western Sahara   Africa… Northe… Inde…  9.63e4 NA         NA       NA \n 4 CA     Canada           North … Northe… Sove…  1.00e7  3.55e7    82.0  43079.\n 5 US     United States    North … Northe… Coun…  9.51e6  3.19e8    78.8  51922.\n 6 KZ     Kazakhstan       Asia:A… Centra… Sove…  2.73e6  1.73e7    71.6  23587.\n 7 UZ     Uzbekistan       Asia:A… Centra… Sove…  4.61e5  3.08e7    71.0   5371.\n 8 PG     Papua New Guinea Oceani… Melane… Sove…  4.65e5  7.76e6    65.2   3709.\n 9 ID     Indonesia        Asia:A… South-… Sove…  1.82e6  2.55e8    68.9  10003.\n10 AR     Argentina        South … South … Sove…  2.78e6  4.30e7    76.3  18798.\n# … with 167 more rows, 1 more variable: geom <MULTIPOLYGON [°]>, and\n#   abbreviated variable names ¹​subregion, ²​area_km2, ³​gdpPercap\n\n# 합쳐진 열들 제거X\nworld_unite2 <-  world %>%\n  unite(\"con_reg\", continent:region_un, sep = \":\", remove = FALSE)\n\nworld_unite2\n\nSimple feature collection with 177 features and 11 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -89.9 xmax: 180 ymax: 83.64513\nGeodetic CRS:  WGS 84\n# A tibble: 177 × 12\n   iso_a2 name_l…¹ con_reg conti…² regio…³ subre…⁴ type  area_…⁵     pop lifeExp\n   <chr>  <chr>    <chr>   <chr>   <chr>   <chr>   <chr>   <dbl>   <dbl>   <dbl>\n 1 FJ     Fiji     Oceani… Oceania Oceania Melane… Sove…  1.93e4  8.86e5    70.0\n 2 TZ     Tanzania Africa… Africa  Africa  Easter… Sove…  9.33e5  5.22e7    64.2\n 3 EH     Western… Africa… Africa  Africa  Northe… Inde…  9.63e4 NA         NA  \n 4 CA     Canada   North … North … Americ… Northe… Sove…  1.00e7  3.55e7    82.0\n 5 US     United … North … North … Americ… Northe… Coun…  9.51e6  3.19e8    78.8\n 6 KZ     Kazakhs… Asia:A… Asia    Asia    Centra… Sove…  2.73e6  1.73e7    71.6\n 7 UZ     Uzbekis… Asia:A… Asia    Asia    Centra… Sove…  4.61e5  3.08e7    71.0\n 8 PG     Papua N… Oceani… Oceania Oceania Melane… Sove…  4.65e5  7.76e6    65.2\n 9 ID     Indones… Asia:A… Asia    Asia    South-… Sove…  1.82e6  2.55e8    68.9\n10 AR     Argenti… South … South … Americ… South … Sove…  2.78e6  4.30e7    76.3\n# … with 167 more rows, 2 more variables: gdpPercap <dbl>,\n#   geom <MULTIPOLYGON [°]>, and abbreviated variable names ¹​name_long,\n#   ²​continent, ³​region_un, ⁴​subregion, ⁵​area_km2\n\n### separate()\n# con_reg를 continent, region_un로 분리\nworld_separate <-  world_unite %>% \n  separate(con_reg, c(\"continent\", \"region_un\"), sep = ':')\n\nworld_separate\n\nSimple feature collection with 177 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -89.9 xmax: 180 ymax: 83.64513\nGeodetic CRS:  WGS 84\n# A tibble: 177 × 11\n   iso_a2 name_l…¹ conti…² regio…³ subre…⁴ type  area_…⁵     pop lifeExp gdpPe…⁶\n   <chr>  <chr>    <chr>   <chr>   <chr>   <chr>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 FJ     Fiji     Oceania Oceania Melane… Sove…  1.93e4  8.86e5    70.0   8222.\n 2 TZ     Tanzania Africa  Africa  Easter… Sove…  9.33e5  5.22e7    64.2   2402.\n 3 EH     Western… Africa  Africa  Northe… Inde…  9.63e4 NA         NA       NA \n 4 CA     Canada   North … Americ… Northe… Sove…  1.00e7  3.55e7    82.0  43079.\n 5 US     United … North … Americ… Northe… Coun…  9.51e6  3.19e8    78.8  51922.\n 6 KZ     Kazakhs… Asia    Asia    Centra… Sove…  2.73e6  1.73e7    71.6  23587.\n 7 UZ     Uzbekis… Asia    Asia    Centra… Sove…  4.61e5  3.08e7    71.0   5371.\n 8 PG     Papua N… Oceania Oceania Melane… Sove…  4.65e5  7.76e6    65.2   3709.\n 9 ID     Indones… Asia    Asia    South-… Sove…  1.82e6  2.55e8    68.9  10003.\n10 AR     Argenti… South … Americ… South … Sove…  2.78e6  4.30e7    76.3  18798.\n# … with 167 more rows, 1 more variable: geom <MULTIPOLYGON [°]>, and\n#   abbreviated variable names ¹​name_long, ²​continent, ³​region_un, ⁴​subregion,\n#   ⁵​area_km2, ⁶​gdpPercap\n\n### 데이터 속성 이름 바꾸기\n# rename(x:new = y:old)\nworld %>% rename(name = name_long)\n\nSimple feature collection with 177 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -89.9 xmax: 180 ymax: 83.64513\nGeodetic CRS:  WGS 84\n# A tibble: 177 × 11\n   iso_a2 name     conti…¹ regio…² subre…³ type  area_…⁴     pop lifeExp gdpPe…⁵\n   <chr>  <chr>    <chr>   <chr>   <chr>   <chr>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 FJ     Fiji     Oceania Oceania Melane… Sove…  1.93e4  8.86e5    70.0   8222.\n 2 TZ     Tanzania Africa  Africa  Easter… Sove…  9.33e5  5.22e7    64.2   2402.\n 3 EH     Western… Africa  Africa  Northe… Inde…  9.63e4 NA         NA       NA \n 4 CA     Canada   North … Americ… Northe… Sove…  1.00e7  3.55e7    82.0  43079.\n 5 US     United … North … Americ… Northe… Coun…  9.51e6  3.19e8    78.8  51922.\n 6 KZ     Kazakhs… Asia    Asia    Centra… Sove…  2.73e6  1.73e7    71.6  23587.\n 7 UZ     Uzbekis… Asia    Asia    Centra… Sove…  4.61e5  3.08e7    71.0   5371.\n 8 PG     Papua N… Oceania Oceania Melane… Sove…  4.65e5  7.76e6    65.2   3709.\n 9 ID     Indones… Asia    Asia    South-… Sove…  1.82e6  2.55e8    68.9  10003.\n10 AR     Argenti… South … Americ… South … Sove…  2.78e6  4.30e7    76.3  18798.\n# … with 167 more rows, 1 more variable: geom <MULTIPOLYGON [°]>, and\n#   abbreviated variable names ¹​continent, ²​region_un, ³​subregion, ⁴​area_km2,\n#   ⁵​gdpPercap\n\nworld\n\nSimple feature collection with 177 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -89.9 xmax: 180 ymax: 83.64513\nGeodetic CRS:  WGS 84\n# A tibble: 177 × 11\n   iso_a2 name_l…¹ conti…² regio…³ subre…⁴ type  area_…⁵     pop lifeExp gdpPe…⁶\n * <chr>  <chr>    <chr>   <chr>   <chr>   <chr>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 FJ     Fiji     Oceania Oceania Melane… Sove…  1.93e4  8.86e5    70.0   8222.\n 2 TZ     Tanzania Africa  Africa  Easter… Sove…  9.33e5  5.22e7    64.2   2402.\n 3 EH     Western… Africa  Africa  Northe… Inde…  9.63e4 NA         NA       NA \n 4 CA     Canada   North … Americ… Northe… Sove…  1.00e7  3.55e7    82.0  43079.\n 5 US     United … North … Americ… Northe… Coun…  9.51e6  3.19e8    78.8  51922.\n 6 KZ     Kazakhs… Asia    Asia    Centra… Sove…  2.73e6  1.73e7    71.6  23587.\n 7 UZ     Uzbekis… Asia    Asia    Centra… Sove…  4.61e5  3.08e7    71.0   5371.\n 8 PG     Papua N… Oceania Oceania Melane… Sove…  4.65e5  7.76e6    65.2   3709.\n 9 ID     Indones… Asia    Asia    South-… Sove…  1.82e6  2.55e8    68.9  10003.\n10 AR     Argenti… South … Americ… South … Sove…  2.78e6  4.30e7    76.3  18798.\n# … with 167 more rows, 1 more variable: geom <MULTIPOLYGON [°]>, and\n#   abbreviated variable names ¹​name_long, ²​continent, ³​region_un, ⁴​subregion,\n#   ⁵​area_km2, ⁶​gdpPercap\n\n# 여러개의 이름 변경\nnew_names <- c(\"i\", \"n\", \"c\", \"r\", \"s\", \"t\", \"a\", \"p\", \"l\", \"gP\", \"geom\")\nworld %>% setNames(new_names)\n\nSimple feature collection with 177 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -89.9 xmax: 180 ymax: 83.64513\nGeodetic CRS:  WGS 84\n# A tibble: 177 × 11\n   i     n                c        r     s     t          a       p     l     gP\n * <chr> <chr>            <chr>    <chr> <chr> <chr>  <dbl>   <dbl> <dbl>  <dbl>\n 1 FJ    Fiji             Oceania  Ocea… Mela… Sove… 1.93e4  8.86e5  70.0  8222.\n 2 TZ    Tanzania         Africa   Afri… East… Sove… 9.33e5  5.22e7  64.2  2402.\n 3 EH    Western Sahara   Africa   Afri… Nort… Inde… 9.63e4 NA       NA      NA \n 4 CA    Canada           North A… Amer… Nort… Sove… 1.00e7  3.55e7  82.0 43079.\n 5 US    United States    North A… Amer… Nort… Coun… 9.51e6  3.19e8  78.8 51922.\n 6 KZ    Kazakhstan       Asia     Asia  Cent… Sove… 2.73e6  1.73e7  71.6 23587.\n 7 UZ    Uzbekistan       Asia     Asia  Cent… Sove… 4.61e5  3.08e7  71.0  5371.\n 8 PG    Papua New Guinea Oceania  Ocea… Mela… Sove… 4.65e5  7.76e6  65.2  3709.\n 9 ID    Indonesia        Asia     Asia  Sout… Sove… 1.82e6  2.55e8  68.9 10003.\n10 AR    Argentina        South A… Amer… Sout… Sove… 2.78e6  4.30e7  76.3 18798.\n# … with 167 more rows, and 1 more variable: geom <MULTIPOLYGON [°]>"
  },
  {
    "objectID": "posts/Spatial_Info_Analysis Ch2/공간정보분석 CH3.html#래스터-객체-조작",
    "href": "posts/Spatial_Info_Analysis Ch2/공간정보분석 CH3.html#래스터-객체-조작",
    "title": "Spatial_Info_Analysis CH2",
    "section": "11 래스터 객체 조작",
    "text": "11 래스터 객체 조작\n\nnumeric, integer, logical, factor 데이처 유형 지원 (character지원X)\n문자형으로 래스터 객체 만들기 : 요인형 or 논리형으로 변환 필요\n\n\n### raster 패키지를 이용한 integer 속성의 래스터객체\nelev <- raster(nrows = 6,  \n               ncols = 6,  \n               xmn = -1.5, \n               xmx = 1.5, \n               ymn = -1.5,  \n               ymx = 1.5, \n               vals = 1:36) \n\nelev\n\nclass      : RasterLayer \ndimensions : 6, 6, 36  (nrow, ncol, ncell)\nresolution : 0.5, 0.5  (x, y)\nextent     : -1.5, 1.5, -1.5, 1.5  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : memory\nnames      : layer \nvalues     : 1, 36  (min, max)\n\nplot(elev, main = 'raster datasets with numeric valeus')\n\n\n\n### 모래 굵기 수준 래스터객체 만들기\ngrain_order <- c(\"clay\", \"silt\", \"sand\")                      # 모래 이름\ngrain_char <- sample(grain_order, 36, replace = TRUE)         # 개수지정(중복허용)\ngrain_fact <- factor(grain_char, levels = grain_order)        # 팩터 변환\ngrain <- raster(nrows = 6, ncols = 6, res = 0.5, \n               xmn = -1.5, xmx = 1.5, ymn = -1.5, ymx = 1.5,\n               vals = grain_fact)\n\nplot(grain)\n\n\n\n###새로운 facter levels 추가\nlevels(grain)[[1]] <- cbind(levels(grain)[[1]], wetness = c(\"wet\", \"moist\", \"dry\"))\nlevels(grain)\n\n[[1]]\n  ID VALUE wetness\n1  1  clay     wet\n2  2  silt   moist\n3  3  sand     dry\n\ngrain[c(1, 11, 35)]\n\n[1] 1 1 1\n\nfactorValues(grain, grain[c(1, 11, 35)])\n\n  VALUE wetness\n1  clay     wet\n2  clay     wet\n3  clay     wet"
  },
  {
    "objectID": "posts/Spatial_Info_Analysis Ch2/공간정보분석 CH3.html#래스터-부분-설정",
    "href": "posts/Spatial_Info_Analysis Ch2/공간정보분석 CH3.html#래스터-부분-설정",
    "title": "Spatial_Info_Analysis CH2",
    "section": "12 래스터 부분 설정",
    "text": "12 래스터 부분 설정\n\nR에서 래스터 부분 설정 4가지\n\n행,열 인덱싱\n셀 ID\n좌표\n다른 공간 객체\n\n래스터 객체 elev의 왼쪽상단( the top left pixel) 의 값을 출력\n모든 값을 추출하거나 전체 행을 추출하려면 values, getValues를 사용\n\n\n### row 1, column 1\nelev[1, 1]\n\n[1] 1\n\n### cell ID 1\nelev[1]\n\n[1] 1"
  },
  {
    "objectID": "posts/Data_Mining_Geopandas/0_install_geopandas.html",
    "href": "posts/Data_Mining_Geopandas/0_install_geopandas.html",
    "title": "Data_Mining CH3",
    "section": "",
    "text": "Install Geopandas"
  },
  {
    "objectID": "posts/Data_Mining_Geopandas/0_install_geopandas.html#자료-출처",
    "href": "posts/Data_Mining_Geopandas/0_install_geopandas.html#자료-출처",
    "title": "Data_Mining CH3",
    "section": "1 자료 출처",
    "text": "1 자료 출처\nhttps://domdom.tistory.com/599"
  },
  {
    "objectID": "posts/Data_Mining_Geopandas/0_install_geopandas.html#기본-환경",
    "href": "posts/Data_Mining_Geopandas/0_install_geopandas.html#기본-환경",
    "title": "Data_Mining CH3",
    "section": "2 기본 환경",
    "text": "2 기본 환경\n\n현재 실행 중인 파이썬 인터프리터의 경로\n\nimport sys\nsys.executable\n'C:\\\\Users\\\\seong taek\\\\anaconda3\\\\python.exe'\n\n파이썬 인터프리터의 버전 정보\n\nsys.version\n'3.9.16 (main, Mar  1 2023, 18:30:21) [MSC v.1916 64 bit (AMD64)]'"
  },
  {
    "objectID": "posts/Data_Mining_Geopandas/0_install_geopandas.html#패키지-다운로드",
    "href": "posts/Data_Mining_Geopandas/0_install_geopandas.html#패키지-다운로드",
    "title": "Data_Mining CH3",
    "section": "3 패키지 다운로드",
    "text": "3 패키지 다운로드\n더 아래의 pip 설치 구문에 있는 패키지별 버전을 참조하여 아래의 사이트에서 각 패키지 설치 파일(.whl)을 다운로드 받습니다.\nhttps://www.lfd.uci.edu/~gohlke/pythonlibs/#shapely\nhttps://www.lfd.uci.edu/~gohlke/pythonlibs/#gdal\nhttps://www.lfd.uci.edu/~gohlke/pythonlibs/#fiona\nhttps://pypi.org/project/geopandas/#files"
  },
  {
    "objectID": "posts/Data_Mining_Geopandas/0_install_geopandas.html#설치-윈도우",
    "href": "posts/Data_Mining_Geopandas/0_install_geopandas.html#설치-윈도우",
    "title": "Data_Mining CH3",
    "section": "4 설치 (윈도우)",
    "text": "4 설치 (윈도우)\n아나콘다 프롬프트를 관리자 권한으로 실행합니다.\nVSCode로 진행하시는 분은 VSCode 실행을 관리자권한으로 해야 합니다.\n현재 경로를 확인하고 이를 감안하여 적정 경로에 설치 파일을 이동해놓습니다.\n저는 강의자료가 있는 곳에 geopandas를 폴더를 만들고 이 폴더 안에 설치 파일들을 옮겨놓았습니다\n아래의 설치/업데이트 구문을 한줄씩 실행합니다.\n먼저, pip와 numpy를 업그레이드 합니다.\n#!python -m pip install -- upgrade pip\n#!pip install -- upgrade numpy\nERROR: Could not find a version that satisfies the requirement upgrade (from versions: none)\nERROR: No matching distribution found for upgrade\nERROR: Could not find a version that satisfies the requirement upgrade (from versions: none)\nERROR: No matching distribution found for upgrade\n그 다음 아래 패키지를 순차적으로 설치합니다\n저는 윈도우 10을 써서 cp310으로 다운받았습니다\n#!pip install pyproj \n#!pip install geopandas/Shapely-1.8.2-cp310-cp310-win_amd64.whl\n#!pip install geopandas/GDAL-3.4.3-cp310-cp310-win_amd64.whl\n#!pip install geopandas/Fiona-1.8.21-cp310-cp310-win_amd64.whl\n#!pip install geopandas/geopandas-0.12.2-py3-none-any.whl \nRequirement already satisfied: pyproj in c:\\users\\seong taek\\anaconda3\\lib\\site-packages (3.5.0)\nRequirement already satisfied: certifi in c:\\users\\seong taek\\anaconda3\\lib\\site-packages (from pyproj) (2022.12.7)\n\n\nERROR: Shapely-1.8.2-cp310-cp310-win_amd64.whl is not a supported wheel on this platform.\nERROR: GDAL-3.4.3-cp310-cp310-win_amd64.whl is not a supported wheel on this platform.\nERROR: Fiona-1.8.21-cp310-cp310-win_amd64.whl is not a supported wheel on this platform.\n\n\nProcessing c:\\users\\seong taek\\3-1 data_mining\\geopandas\\geopandas-0.12.2-py3-none-any.whl\nRequirement already satisfied: pyproj>=2.6.1.post1 in c:\\users\\seong taek\\anaconda3\\lib\\site-packages (from geopandas==0.12.2) (3.5.0)\nRequirement already satisfied: pandas>=1.0.0 in c:\\users\\seong taek\\anaconda3\\lib\\site-packages (from geopandas==0.12.2) (1.5.3)\nRequirement already satisfied: fiona>=1.8 in c:\\users\\seong taek\\anaconda3\\lib\\site-packages (from geopandas==0.12.2) (1.9.3)\nRequirement already satisfied: packaging in c:\\users\\seong taek\\anaconda3\\lib\\site-packages (from geopandas==0.12.2) (22.0)\nRequirement already satisfied: shapely>=1.7 in c:\\users\\seong taek\\anaconda3\\lib\\site-packages (from geopandas==0.12.2) (2.0.1)\nRequirement already satisfied: attrs>=19.2.0 in c:\\users\\seong taek\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas==0.12.2) (22.1.0)\nRequirement already satisfied: importlib-metadata in c:\\users\\seong taek\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas==0.12.2) (4.11.3)\nRequirement already satisfied: certifi in c:\\users\\seong taek\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas==0.12.2) (2022.12.7)\nRequirement already satisfied: cligj>=0.5 in c:\\users\\seong taek\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas==0.12.2) (0.7.2)\nRequirement already satisfied: munch>=2.3.2 in c:\\users\\seong taek\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas==0.12.2) (2.5.0)\nRequirement already satisfied: click-plugins>=1.0 in c:\\users\\seong taek\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas==0.12.2) (1.1.1)\nRequirement already satisfied: click~=8.0 in c:\\users\\seong taek\\anaconda3\\lib\\site-packages (from fiona>=1.8->geopandas==0.12.2) (8.0.4)\nRequirement already satisfied: pytz>=2020.1 in c:\\users\\seong taek\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->geopandas==0.12.2) (2022.7)\nRequirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\seong taek\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->geopandas==0.12.2) (2.8.2)\nRequirement already satisfied: numpy>=1.20.3 in c:\\users\\seong taek\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->geopandas==0.12.2) (1.23.5)\nRequirement already satisfied: colorama in c:\\users\\seong taek\\anaconda3\\lib\\site-packages (from click~=8.0->fiona>=1.8->geopandas==0.12.2) (0.4.6)\nRequirement already satisfied: six in c:\\users\\seong taek\\anaconda3\\lib\\site-packages (from munch>=2.3.2->fiona>=1.8->geopandas==0.12.2) (1.16.0)\nRequirement already satisfied: zipp>=0.5 in c:\\users\\seong taek\\anaconda3\\lib\\site-packages (from importlib-metadata->fiona>=1.8->geopandas==0.12.2) (3.11.0)\ngeopandas is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel."
  },
  {
    "objectID": "posts/Data_Mining_Geopandas/0_install_geopandas.html#설치-맥북",
    "href": "posts/Data_Mining_Geopandas/0_install_geopandas.html#설치-맥북",
    "title": "Data_Mining CH3",
    "section": "5 설치 (맥북)",
    "text": "5 설치 (맥북)\n\n#!pip install pyproj\n#!pip install geopy\n#!pip install geopandas\n\nimport 되면 성공 !\nimport geopandas as gpd"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Lab",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nData_Mining CH3\n\n\n\n\n\n\n\ncode\n\n\ndata_mining\n\n\njupyter\n\n\n\n\n\n\n\n\n\n\n\nApr 27, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nSpatial_Info_Analysis CH2\n\n\n\n\n\n\n\ncode\n\n\nSpatial_Info_Analysis\n\n\nrstudio\n\n\n\n\n\n\n\n\n\n\n\nApr 22, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nSpatial_Info_Analysis CH1\n\n\n\n\n\n\n\ncode\n\n\nSpatial_Info_Analysis\n\n\nrstudio\n\n\n\n\n\n\n\n\n\n\n\nApr 20, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nOpendata_Analysis CH3\n\n\n\n\n\n\n\ncode\n\n\nopendata_analysis\n\n\nrstudio\n\n\n\n\n\n\n\n\n\n\n\nApr 16, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nOpendata_Analysis CH4\n\n\n\n\n\n\n\ncode\n\n\nopendata_analysis\n\n\nrstudio\n\n\n\n\n\n\n\n\n\n\n\nApr 16, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nOpendata_Analysis CH5\n\n\n\n\n\n\n\ncode\n\n\nopendata_analysis\n\n\nrstudio\n\n\n\n\n\n\n\n\n\n\n\nApr 16, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nOpendata_Analysis CH2\n\n\n\n\n\n\n\ncode\n\n\nopendata_analysis\n\n\nrstudio\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nOpendata_Analysis CH1\n\n\n\n\n\n\n\ncode\n\n\nopendata_analysis\n\n\nrstudio\n\n\n\n\n\n\n\n\n\n\n\nApr 14, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nData_Visualization CH7\n\n\n\n\n\n\n\ncode\n\n\ndata_visualization\n\n\nrstudio\n\n\n\n\n\n\n\n\n\n\n\nApr 13, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nData_Visualization CH6\n\n\n\n\n\n\n\ncode\n\n\ndata_visualization\n\n\nrstudio\n\n\n\n\n\n\n\n\n\n\n\nApr 12, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nData_Visualization CH5\n\n\n\n\n\n\n\ncode\n\n\ndata_visualization\n\n\nrstudio\n\n\n\n\n\n\n\n\n\n\n\nApr 6, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nData_Visualization CH4\n\n\n\n\n\n\n\ncode\n\n\ndata_visualization\n\n\nrstudio\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nData_Mining CH1\n\n\n\n\n\n\n\ncode\n\n\ndata_mining\n\n\njupyter\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nData_Mining CH2\n\n\n\n\n\n\n\ncode\n\n\ndata_mining\n\n\njupyter\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nData_Visualization CH3\n\n\n\n\n\n\n\ncode\n\n\ndata_visualization\n\n\nrstudio\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nData_Visualization CH2\n\n\n\n\n\n\n\ncode\n\n\ndata_visualization\n\n\nrstudio\n\n\n\n\n\n\n\n\n\n\n\nMar 30, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nData_Visualization CH1\n\n\n\n\n\n\n\ncode\n\n\ndata_visualization\n\n\nrstudio\n\n\n\n\n\n\n\n\n\n\n\nMar 29, 2023\n\n\nSeongtaek\n\n\n\n\n\n\n  \n\n\n\n\nadvanced_python CH1\n\n\n\n\n\n\n\ncode\n\n\nadvanced_python\n\n\njupyter\n\n\n\n\n\n\n\n\n\n\n\nMar 28, 2023\n\n\nSeongtaek\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Lab.html",
    "href": "Lab.html",
    "title": "Lab",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nData_Mining_Penguin\n\n\n\n\n\n\n\ncode\n\n\ndata_mining\n\n\njupyter\n\n\n\n\n\n\n\n\n\n\n\nApr 27, 2023\n\n\nSeongtaek\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts_Lab/Data_Mining_penguin/4월 25일 펭귄.html",
    "href": "posts_Lab/Data_Mining_penguin/4월 25일 펭귄.html",
    "title": "Data_Mining_Penguin",
    "section": "",
    "text": "Penguin Data"
  },
  {
    "objectID": "posts_Lab/Data_Mining_penguin/4월 25일 펭귄.html#load-data",
    "href": "posts_Lab/Data_Mining_penguin/4월 25일 펭귄.html#load-data",
    "title": "Data_Mining_Penguin",
    "section": "1 Load data",
    "text": "1 Load data\n\n예제로 사용할 펭귄 데이터를 불러옵니다.\nseaborn에 내장되어 있습니다.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npenguins = sns.load_dataset(\"penguins\")\npenguins.head()\n\n\n\n\n\n\n\n\n\nspecies\n\n\nisland\n\n\nbill_length_mm\n\n\nbill_depth_mm\n\n\nflipper_length_mm\n\n\nbody_mass_g\n\n\nsex\n\n\n\n\n\n\n0\n\n\nAdelie\n\n\nTorgersen\n\n\n39.1\n\n\n18.7\n\n\n181.0\n\n\n3750.0\n\n\nMale\n\n\n\n\n1\n\n\nAdelie\n\n\nTorgersen\n\n\n39.5\n\n\n17.4\n\n\n186.0\n\n\n3800.0\n\n\nFemale\n\n\n\n\n2\n\n\nAdelie\n\n\nTorgersen\n\n\n40.3\n\n\n18.0\n\n\n195.0\n\n\n3250.0\n\n\nFemale\n\n\n\n\n3\n\n\nAdelie\n\n\nTorgersen\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n\n\n4\n\n\nAdelie\n\n\nTorgersen\n\n\n36.7\n\n\n19.3\n\n\n193.0\n\n\n3450.0\n\n\nFemale"
  },
  {
    "objectID": "posts_Lab/Data_Mining_penguin/4월 25일 펭귄.html#figure-and-axes",
    "href": "posts_Lab/Data_Mining_penguin/4월 25일 펭귄.html#figure-and-axes",
    "title": "Data_Mining_Penguin",
    "section": "2 Figure and Axes",
    "text": "2 Figure and Axes\n\nmatplotlib으로 도화지figure를 깔고 축공간axes를 만듭니다.\n1 x 2 축공간을 구성합니다.\n\n### 도화지 생성\nfig, axes = plt.subplots(ncols=2, figsize=(8,4))\n\nfig.tight_layout()\n\n\n\npng"
  },
  {
    "objectID": "posts_Lab/Data_Mining_penguin/4월 25일 펭귄.html#plot-with-matplotlib",
    "href": "posts_Lab/Data_Mining_penguin/4월 25일 펭귄.html#plot-with-matplotlib",
    "title": "Data_Mining_Penguin",
    "section": "3 plot with matplotlib",
    "text": "3 plot with matplotlib\n\nmatplotlib 기능을 이용해서 산점도를 그립니다.\n\nx축은 부리 길이 bill length\ny축은 부리 위 아래 두께 bill depth\n색상은 종species로 합니다.\nAdelie, Chinstrap, Gentoo이 있습니다.\n\n두 축공간 중 왼쪽에만 그립니다.\n컬러를 다르게 주기 위해 f-string 포맷을 사용했습니다. f-string 포맷에 대한 설명은 https://blockdmask.tistory.com/429를 참고하세요\n\n### 도화지 생성\nfig, axes = plt.subplots(ncols=2,figsize=(8,4))\n\n### 모든 펭귄 종류\nspecies_u = penguins[\"species\"].unique()\n\n### 첫 번째 subplot 그리기\nfor i, s in enumerate(species_u):\n    axes[0].scatter(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s],\n                    penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s],\n                    c=f\"C{i}\", label=s, alpha=0.3)\n\n### 범례 추가\naxes[0].legend(species_u, title=\"species\", fontsize=8)\n\n### x,y 레이블 지정\naxes[0].set_xlabel(\"Bill Length (mm)\")\naxes[0].set_ylabel(\"Bill Depth (mm)\")\n\n### plt.show()\nfig.tight_layout()\n\n\n\npng\n\n\n\n조금 더 간단히 그리는 방법\n\nmatplotlib는 기본적으로 Categorical 변수를 color로 바로 사용하지 못함\n\n\n### 펭귄 종류를 고유의 숫자코드로 변환\npenguins[\"species_codes\"] = pd.Categorical(penguins[\"species\"]).codes\n\n### 도화지 생성\nfig, axes = plt.subplots(ncols=2,figsize=(8,4))\n\n### 첫 번재 subplot 그리기 \na = axes[0].scatter(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", c=\"species_codes\", alpha=0.3) # 크기:s\na\n\n### 범례 추가\naxes[0].legend(*a.legend_elements(), title=\"Species\", fontsize=8) # 범례위치 : loc='lower right', 'upper center', etc..\n\n### x,y 레이블 지정\naxes[0].set_xlabel(\"Bill Length (mm)\")\naxes[0].set_ylabel(\"Bill Depth (mm)\")\n<__array_function__ internals>:180: UserWarning: Warning: converting a masked element to nan.\nC:\\Users\\seong taek\\anaconda3\\lib\\site-packages\\matplotlib\\colors.py:1311: UserWarning: Warning: converting a masked element to nan.\n  data = np.asarray(value)\nC:\\Users\\seong taek\\anaconda3\\lib\\site-packages\\matplotlib\\ticker.py:521: UserWarning: Warning: converting a masked element to nan.\n  if self._useLocale else fmt % arg)\n\n\n\n\n\nText(0, 0.5, 'Bill Depth (mm)')\n\n\n\npng"
  },
  {
    "objectID": "posts_Lab/Data_Mining_penguin/4월 25일 펭귄.html#plot-with-seaborn",
    "href": "posts_Lab/Data_Mining_penguin/4월 25일 펭귄.html#plot-with-seaborn",
    "title": "Data_Mining_Penguin",
    "section": "4 Plot with seaborn",
    "text": "4 Plot with seaborn\n\n두 번째 plot 그리기\n\n### 도화지 생성\nfig, axes = plt.subplots(ncols=2,figsize=(8,4))\n\n### 모든 펭귄 종류\nspecies_u = penguins[\"species\"].unique()\n\n### 첫 번째 subplot 그리기\nfor i, s in enumerate(species_u):\n    axes[0].scatter(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s],\n                    penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s],\n                    c=f\"C{i}\", label=s, alpha=0.3)\n\n### 범례 추가    \naxes[0].legend(species_u, title=\"species\")\n\n### x,y 레이블 지정\naxes[0].set_xlabel(\"Bill Length (mm)\")\naxes[0].set_ylabel(\"Bill Depth (mm)\")\n\n\n### 두 번째 subplot 그리기\nsns.scatterplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\", data=penguins, alpha=0.3, ax=axes[1])\naxes[1].set_xlabel(\"Bill Length (mm)\")\naxes[1].set_ylabel(\"Bill Depth (mm)\")\n\nfig.tight_layout()\n\n\n\npng\n\n\n\n단 세 줄로 거의 동일한 그림이 나왔습니다.\n\nscatter plot의 점 크기만 살짝 작습니다.\nlabel의 투명도만 살짝 다릅니다.\n\nseaborn 명령 scatterplot()을 그대로 사용했습니다.\nx축과 y축 label도 바꾸었습니다.\n\nax=axes[1] 인자에서 볼 수 있듯, 존재하는 axes에 그림만 얹었습니다.\nmatplotlib 틀 + seaborn 그림 이므로, matplotlib 명령이 모두 통합니다."
  },
  {
    "objectID": "posts_Lab/Data_Mining_penguin/4월 25일 펭귄.html#matplotlib-seaborn-seaborn-matplotlib",
    "href": "posts_Lab/Data_Mining_penguin/4월 25일 펭귄.html#matplotlib-seaborn-seaborn-matplotlib",
    "title": "Data_Mining_Penguin",
    "section": "5 matplotlib + seaborn & seaborn + matplotlib",
    "text": "5 matplotlib + seaborn & seaborn + matplotlib\n\nmatplotlib과 seaborn이 자유롭게 섞일 수 있습니다.\n\nmatplotlib 산점도 위에 seaborn 추세선을 얹을 수 있고,\nseaborn 산점도 위에 matplotlib 중심점을 얹을 수 있습니다.\n\n파이썬 코드는 다음과 같습니다.\n\n### 도화지 생성\nfig, axes = plt.subplots(ncols=2, figsize=(8, 4))\n\n### 모든 펭귄 종류\nspecies_u = penguins[\"species\"].unique()\n\n### 첫 번째 subplot 그리기 + 추세선\nfor i, s in enumerate(species_u):\n    axes[0].scatter(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s],\n                   penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s],\n                   c=f\"C{i}\", label=s, alpha=0.3\n                  )                  \n    sns.regplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", data=penguins.loc[penguins[\"species\"]==s], \n                scatter=False, ax=axes[0])\n    \naxes[0].legend(species_u, title=\"species\")\naxes[0].set_xlabel(\"Bill Length (mm)\")\naxes[0].set_ylabel(\"Bill Depth (mm)\")\n\n### 두 번째 subplot 그리기\nsns.scatterplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\", data=penguins, alpha=0.3, ax=axes[1])\naxes[1].set_xlabel(\"Bill Length (mm)\")\naxes[1].set_ylabel(\"Bill Depth (mm)\")\n\n\n### 중심점 marker\nfor i, s in enumerate(species_u):\n    axes[1].scatter(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s].mean(),\n                   penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s].mean(),\n                   c=f\"C{i}\", alpha=1, marker=\"x\", s=100\n                  )\n\nfig.tight_layout()\n\n\n\npng"
  },
  {
    "objectID": "posts_Lab/Data_Mining_penguin/4월 25일 펭귄.html#seaborn-seaborn-matplotlib",
    "href": "posts_Lab/Data_Mining_penguin/4월 25일 펭귄.html#seaborn-seaborn-matplotlib",
    "title": "Data_Mining_Penguin",
    "section": "6 seaborn + seaborn + matplotlib",
    "text": "6 seaborn + seaborn + matplotlib\n\n안 될 이유가 없습니다.\nseaborn scatterplot + seaborn kdeplot + matplotlib text입니다\n\n### 도화지 생성\nfig, ax = plt.subplots(figsize=(6,5))\n\n### plot 0: scatter plot\nsns.scatterplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", color=\"k\", data=penguins, alpha=0.3, ax=ax, legend=False)\n\n### plot 1: kde plot (밀도 그래프)\nsns.kdeplot(x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\", data=penguins, alpha=0.5, ax=ax, legend=False)\n\n### text:\nspecies_u = penguins[\"species\"].unique()\nfor i, s in enumerate(species_u):\n    ax.text(penguins[\"bill_length_mm\"].loc[penguins[\"species\"]==s].mean(),\n            penguins[\"bill_depth_mm\"].loc[penguins[\"species\"]==s].mean(),\n            s = s, fontdict={\"fontsize\":14, \"fontweight\":\"bold\",\"color\":\"k\"}\n            )\n\nax.set_xlabel(\"Bill Length (mm)\")\nax.set_ylabel(\"Bill Depth (mm)\")\n\nfig.tight_layout()\n\n\n\npng"
  },
  {
    "objectID": "posts_Lab/Data_Mining_penguin/4월 25일 펭귄.html#quiz",
    "href": "posts_Lab/Data_Mining_penguin/4월 25일 펭귄.html#quiz",
    "title": "Data_Mining_Penguin",
    "section": "7 Quiz",
    "text": "7 Quiz\nBill length를 10단위로 나눈 후, Bill length에 따른 Bill depth의 boxplot을 그리시오\n### bill length를 10단위로 만든 후, 새로운 컬럼 추가\npenguins['bill_length_10'] = (penguins['bill_length_mm'] // 10) * 10\npenguins\n\n\n\n\n\n\n\n\n\nspecies\n\n\nisland\n\n\nbill_length_mm\n\n\nbill_depth_mm\n\n\nflipper_length_mm\n\n\nbody_mass_g\n\n\nsex\n\n\nspecies_codes\n\n\nbill_length_10\n\n\n\n\n\n\n0\n\n\nAdelie\n\n\nTorgersen\n\n\n39.1\n\n\n18.7\n\n\n181.0\n\n\n3750.0\n\n\nMale\n\n\n0\n\n\n30.0\n\n\n\n\n1\n\n\nAdelie\n\n\nTorgersen\n\n\n39.5\n\n\n17.4\n\n\n186.0\n\n\n3800.0\n\n\nFemale\n\n\n0\n\n\n30.0\n\n\n\n\n2\n\n\nAdelie\n\n\nTorgersen\n\n\n40.3\n\n\n18.0\n\n\n195.0\n\n\n3250.0\n\n\nFemale\n\n\n0\n\n\n40.0\n\n\n\n\n3\n\n\nAdelie\n\n\nTorgersen\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n0\n\n\nNaN\n\n\n\n\n4\n\n\nAdelie\n\n\nTorgersen\n\n\n36.7\n\n\n19.3\n\n\n193.0\n\n\n3450.0\n\n\nFemale\n\n\n0\n\n\n30.0\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n339\n\n\nGentoo\n\n\nBiscoe\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n2\n\n\nNaN\n\n\n\n\n340\n\n\nGentoo\n\n\nBiscoe\n\n\n46.8\n\n\n14.3\n\n\n215.0\n\n\n4850.0\n\n\nFemale\n\n\n2\n\n\n40.0\n\n\n\n\n341\n\n\nGentoo\n\n\nBiscoe\n\n\n50.4\n\n\n15.7\n\n\n222.0\n\n\n5750.0\n\n\nMale\n\n\n2\n\n\n50.0\n\n\n\n\n342\n\n\nGentoo\n\n\nBiscoe\n\n\n45.2\n\n\n14.8\n\n\n212.0\n\n\n5200.0\n\n\nFemale\n\n\n2\n\n\n40.0\n\n\n\n\n343\n\n\nGentoo\n\n\nBiscoe\n\n\n49.9\n\n\n16.1\n\n\n213.0\n\n\n5400.0\n\n\nMale\n\n\n2\n\n\n40.0\n\n\n\n\n\n\n344 rows × 9 columns\n\n\n### 박스 plot\nsns.boxenplot(x = 'bill_length_10', y = 'bill_depth_mm', data=penguins)\n\n### 점 표현\nsns.stripplot(x = 'bill_length_10', y = 'bill_depth_mm', data=penguins, color='black', size=4)\n\nsns.set_style('whitegrid')\nplt.show()\n\n\n\npng\n\n\n\n7.1 sns.set_style\n\ndarkgrid: 어두운 배경에 격자 라인이 그려지는 스타일\nwhitegrid: 밝은 배경에 격자 라인이 그려지는 스타일\ndark: 어두운 배경에 격자 라인이 없는 스타일\nwhite: 밝은 배경에 격자 라인이 없는 스타일\nticks: 격자 라인 대신 축의 눈금 표시가 있는 스타일\n\n\n\n7.2 pd.cut 이용\n### bill length를 구간별로 만든 후, 새로운 컬럼 추가\npenguins['bill_length_group'] = pd.cut(penguins['bill_length_mm'],\n                                      bins=[0,40,50,60],\n                                      labels=['0~40', '40~50', '50~60'])\npenguins\n\n\n\n\n\n\n\n\n\nspecies\n\n\nisland\n\n\nbill_length_mm\n\n\nbill_depth_mm\n\n\nflipper_length_mm\n\n\nbody_mass_g\n\n\nsex\n\n\nspecies_codes\n\n\nbill_length_10\n\n\nbill_length_group\n\n\n\n\n\n\n0\n\n\nAdelie\n\n\nTorgersen\n\n\n39.1\n\n\n18.7\n\n\n181.0\n\n\n3750.0\n\n\nMale\n\n\n0\n\n\n30.0\n\n\n0~40\n\n\n\n\n1\n\n\nAdelie\n\n\nTorgersen\n\n\n39.5\n\n\n17.4\n\n\n186.0\n\n\n3800.0\n\n\nFemale\n\n\n0\n\n\n30.0\n\n\n0~40\n\n\n\n\n2\n\n\nAdelie\n\n\nTorgersen\n\n\n40.3\n\n\n18.0\n\n\n195.0\n\n\n3250.0\n\n\nFemale\n\n\n0\n\n\n40.0\n\n\n40~50\n\n\n\n\n3\n\n\nAdelie\n\n\nTorgersen\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n0\n\n\nNaN\n\n\nNaN\n\n\n\n\n4\n\n\nAdelie\n\n\nTorgersen\n\n\n36.7\n\n\n19.3\n\n\n193.0\n\n\n3450.0\n\n\nFemale\n\n\n0\n\n\n30.0\n\n\n0~40\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n339\n\n\nGentoo\n\n\nBiscoe\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\nNaN\n\n\n2\n\n\nNaN\n\n\nNaN\n\n\n\n\n340\n\n\nGentoo\n\n\nBiscoe\n\n\n46.8\n\n\n14.3\n\n\n215.0\n\n\n4850.0\n\n\nFemale\n\n\n2\n\n\n40.0\n\n\n40~50\n\n\n\n\n341\n\n\nGentoo\n\n\nBiscoe\n\n\n50.4\n\n\n15.7\n\n\n222.0\n\n\n5750.0\n\n\nMale\n\n\n2\n\n\n50.0\n\n\n50~60\n\n\n\n\n342\n\n\nGentoo\n\n\nBiscoe\n\n\n45.2\n\n\n14.8\n\n\n212.0\n\n\n5200.0\n\n\nFemale\n\n\n2\n\n\n40.0\n\n\n40~50\n\n\n\n\n343\n\n\nGentoo\n\n\nBiscoe\n\n\n49.9\n\n\n16.1\n\n\n213.0\n\n\n5400.0\n\n\nMale\n\n\n2\n\n\n40.0\n\n\n40~50\n\n\n\n\n\n\n344 rows × 10 columns\n\n\nsns.boxenplot(x = 'bill_length_group', y = 'bill_depth_mm', data=penguins)\nsns.stripplot(x = 'bill_length_group', y = 'bill_depth_mm', data=penguins, color='black', size=4)\n\nsns.set_style('whitegrid')\nsns.despine()\nplt.show()\n\n\n\npng\n\n\nsns.scatterplot(x='bill_length_mm', y='bill_depth_mm', data=penguins, alpha=0.3)\n\nplt.show()\n\n\n\npng\n\n\n\n열 기준 : species\n색상 : species별\n한 행의 subplot 개수\nmap : x축, y축 지정\nsns.despine : 상단, 우측 축 제거\n\ng = sns.FacetGrid(penguins, col='species',hue='species',col_wrap=3)\ng.map(sns.scatterplot, 'bill_length_mm', 'bill_depth_mm')\n\nsns.set_style('whitegrid')\nsns.despine()\n\nplt.show()\n\n\n\npng"
  },
  {
    "objectID": "posts_Lab/Data_Mining_penguin/4월 25일 펭귄.html#section",
    "href": "posts_Lab/Data_Mining_penguin/4월 25일 펭귄.html#section",
    "title": "Data_Mining_Penguin",
    "section": "8 ",
    "text": "8"
  }
]